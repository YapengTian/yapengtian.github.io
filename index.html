<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <meta charset="utf-8">
  <title>Yapeng Tian | Home</title>
  <meta name="description" content="Yapeng Tian">
  <meta name="author" content="Yapeng Tian">


  <!-- <meta property="og:title" content="Martin Saveski" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="http://yapengtian.org/" />
  <meta property="og:site_name" content="Yapeng Tian" />
  <link rel="canonical" href="https://yapengtian.org/" /> -->

  <!-- Mobile Specific Metas
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- FONT
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <link rel="stylesheet" href=/libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=/libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=/libs/custom/my_css.css>

  <!-- JQuery
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <script src=/libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" | prepend: site.baseurl }}>

  <!-- Academicons
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <link rel="stylesheet" href=/libs/external/academicons-1.8.6/css/academicons.min.css>

  <!-- Skeleton tabs
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <link rel="stylesheet" href=/libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=/libs/external/skeleton_tabs/skeleton-tabs.js></script>



  <!-- Timeline
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <link rel="stylesheet" href=/libs/external/timeline.css>

  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"> -->


  <!-- Scripts
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <!--<link rel="stylesheet" href=/libs/external/github-prettify-theme.css>-->
  <script src=/libs/custom/my_js.js></script>

  <!-- Favicon
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <!-- <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png> -->

  <link rel="icon" href="https://upload.wikimedia.org/wikipedia/commons/7/7c/UT_Dallas_2_Color_Emblem_-_SVG_Brand_Identity_File.svg">
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"> -->
  <link rel="shortcut icon" href="https://upload.wikimedia.org/wikipedia/commons/7/7c/UT_Dallas_2_Color_Emblem_-_SVG_Brand_Identity_File.svg">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


</head>
<body>

  <!-- Primary Page Layout
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
  <div class="container">

    <section class="header">
      <div class="row">
        <div class="three columns">
          <a href="/"><img class="rounded" height="200px" width="200px" src='/assets/profile-pics/YapengTian.jpg'></a>
        </div>
        <div class="nine columns main-description">
            <h1>Yapeng Tian</h1>
            <p>Assistant Professor, The University of Texas at Dallas</p>
            <p>yapeng.tian@utdallas.edu</p>
            <p>ECSS 4.211</p>
            <p>
              <span onclick="window.open('https://twitter.com/YapengTian')" style="cursor: pointer">
                <i class="fa fa-twitter" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://www.linkedin.com/in/yapeng-tian-780795141/')" style="cursor: pointer">
                <i class="fa fa-linkedin-square" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://github.com/YapengTian')" style="cursor: pointer">
                <i class="fa fa-github" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://scholar.google.com/citations?user=lxCqdpoAAAAJ&hl=en')" style="cursor: pointer">
                <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
              </span>

              <span onclick="window.open('https://dblp.dagstuhl.de/pid/176/4020.html')" style="cursor: pointer">
                <i class="ai ai-dblp ai-lg" aria-hidden="true"></i>
              </span>
            </p>
        </div>
      </div>
    </section>

    <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href=/index.html#bio>Home</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#group>Students</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#publications>Publications</a></li>
          <!-- <li class="navbar-item"><a class="navbar-link" href=/index.html#projects>Projects</a></li> -->
          <li class="navbar-item"><a class="navbar-link" href=/index.html#teaching>Teaching</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#activity>Service</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#award>Awards</a></li>
          <li class="navbar-item"><a class="navbar-link" href=/index.html#resume>CV</a></li>
        </ul>
      </div>
    </nav>

    <!-- ========== BIO ========== -->
<span class="anchor" id="bio"></span>
<div class="docs-section">
<div class="docs-section">
  <h4>Bio</h4>
  <p style="text-align:justify; text-justify:inter-ideograph">
    I am an Assistant Professor in the Computer Science Department of UT Dallas and lead the <b>Computer Vision and Multimodal Computing (CVMC) Lab</b>.   Before coming to UTD, I finished my PhD at University of Rochester, advised by <a href="https://www.cs.rochester.edu/u/cxu22/" target="_blank">Chenliang Xu</a>, my master degree at Tsinghua University working with <a href="http://www.fiesta.tsinghua.edu.cn/pi/2/10" target="_blank">Wenming Yang</a>, and B.E degree at Xidian University.
    I was a visiting student at SIAT advised by <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en" target="_blank">Yu Qiao</a>. I did internships at Adobe Research with <a href="https://dingzeyu.li/" target="_blank">Dingzeyu Li</a> and Meta with <a href="https://alexanderrichard.github.io/" target="_blank">Alexander Richard</a>.
  
 I am interested in solving core <b>computer vision</b>, <b>computer audition</b>, and <b>machine learning</b> problems and applying the developed learning approaches to broad AI applications, such
 as multisensory perception, computational photography, AR/VR, accessibility, and healthcare. My work has been recognized with awards including the AAAI New Faculty Highlights, Cisco Faculty Research Award, and Amazon Research Award.
</p>

<!-- <p align="justify">
  <a style="color:red;margin-bottom:0;">
    <b>Call for Student (Summer 2025):</b><br>
  </a>
  I am pleased to announce an opportunity for one rising junior or senior at UTD to work on a summer research project through the Undergraduate Research Apprenticeship Program (URAP). The selected student will receive a $4,500 stipend over the course of the summer (approximately 300 hours of research work). Interested students should email me a brief statement of interest outlining their background, relevant coursework, and research goals. Please include your CV or resume and unofficial transcript. 
</p>   -->

<p align="justify">
  <a style="color:red;margin-bottom:0;">
    <b>ICCV 2025 Workshop Call for Papers:</b><br>
  </a>
  We are excited to announce the CV4A11y, KnowledgeMR, and MCL Workshops at ICCV 2025. We invite paper submissions.<br>
 üìç  <a href="https://cv4a11y.github.io/ICCV2025/index.html">CV4A11y: Workshop on Vision Foundation Models and Generative AI for Accessibility</a> <br>
 üìç  <a href="https://knowledgemr-workshop.github.io/">KnowledgeMR: Workshop on Knowledge-Intensive Multimodal Reasoning</a><br>
 üìç  <a href="https://mclworkshop25.github.io/mcl-iccv2025/ICCV2025/index.html">MCL: Workshop on Multimodal Continual Learning</a><br>
  </b>
</p>  


<!-- <p align="justify">
  <a style="color:red;margin-bottom:0;">
    <b>Summer research opportunity:</b><br>
  </a>
  I‚Äôm looking for two research interns for Summer 2025 to work on audio-visual scene perception and generation. If you‚Äôre interested, please send me an email with your CV and transcript.

  For students who previously reached out, if you‚Äôre still interested, kindly follow up with a new email. Apologies for the inconvenience.</b>
</p>  -->


<span class="anchor" id="highlight"></span>
<div class="docs-section">
<div class="docs-section">
  <h4>Research Highlights</h4>
    <ul class="list_no_dot">
<b>Audio-Visual Scene Understanding</b>:  To achieve truly intelligent systems, we must move beyond scene understanding that relies solely on individual senses like sight or sound. Our research pioneers a multimodal approach, integrating computer vision and audition to unlock the rich information available through combined audio-visual perception. We tackle fundamental challenges in this emerging field, developing unified, explainable, and robust multimodal models for video understanding.<br>
<br>
<b>Audio-Visual Scene Generation</b>: We're pushing beyond scene perception to content creation, developing audio-visual intelligent systems that can generate realistic sounds and videos from diverse multimodal inputs. Our work includes joint audio-visual generation, spatial audio generation, and video and text guided audio generation models.<br>    
<br>
<b>AI for Accessibility and Healthcare</b>: We develop computer vision and AI models to empower individuals with disabilities and improve healthcare. Our work spans: (a) Assistive Technologies: Creating systems to aid those with autism, blind or low vision, and hearing impairments. (b) Medical Imaging: Developing AI to enhance medical images for improved diagnosis and treatment.<br>  
<br>
<b>Image and Video Processing</b>: I enjoy building computer vision algorithms to improve image and video quality in both automated and creative ways. My research involves developing a range of image and video restoration models capable of generating photorealistic outputs. This work has also been applied to medical imaging, including MRI data. <br>








<!-- 
  <p align="justify">
    <a style="color:red;margin-bottom:0;">
      <b>Prospective students:</b><br>
    </a>
    My group has opening Ph.D. positions in Fall 2024. Please email me your CV if you are interested. For application, please apply to our CS <a href="https://www.utdallas.edu/fact-sheets/ecs/phd-computer-science/" target="_blank">Ph.D. program</a> and mention my name in your research statement.
  </p> -->

  <!-- <p align="justify">
      <b>Reading Group:</b><br>
    
    We are running a <a href="https://cvmc-reading-group.github.io/CVMC-Reading-Group.io/" target="_blank">reading group</a> to read and discuss papers in the areas of computer vision, machine learning, multi-modal learning, and robotics on every Thursday afternoon (3pm-5pm) in ECSS 4.910.  
  </p>  -->




    <!-- <p align="justify">
    <a style="color:red;margin-bottom:0;">
      <b>Undergraduate summer research opportunity:</b><br>
    </a>
  UTD Undergraduate students who are rising juniors or seniors interested in doing research with me are welcome to email me. Our lab will support one student to do research through the Undergraduate Research Apprenticeship Program in summer 2023. The selected student will receive $4500 over the course of the summer as a stipend.
    </b>
  </p>  -->


  <!-- <p align="justify">
    <a style="color:red;margin-bottom:0;">
      <b>Prospective students:</b><br>
    </a>
    My group has multiple opening Ph.D. positions in Spring and Fall 2023. Please email me your CV if you are interested. For application, please apply to our CS <a href="https://www.utdallas.edu/fact-sheets/ecs/phd-computer-science/" target="_blank">Ph.D. program</a> and mention my name in your research statement.
    <b>
    Current UTD students interested in doing research with me are welcome to email me or stop by my office during my office hours. 
    </b>
  </p>  -->

  <!--
  I am a Postdoc at the department of Management Science and Engineering at Stanford University, working with <a href="https://web.stanford.edu/~jugander/" target="_blank">Johan Ugander</a>. 
  My research develops tools for analyzing large-scale social data, aiming to provide a better understanding of social structure and behaviors online while also impacting the design of digital social systems.
  My work often falls at the intersections of Social Networks, Machine Learning, and Causal Inference.
  </p>

  <p>
  I completed my Ph.D. from MIT in 2020 under the supervision of Deb Roy. 
  Before coming to MIT, I spent one year in Paris and one year in Barcelona doing a M.Sc. in Data mining and Knowledge Management. 
  I got my B.Sc. from Staffordshire University with First Class honors in Computer Science. 
  Throughout my graduate studies, I spent several summers doing internships in industry, including Yahoo! Labs, Amazon, LinkedIn, and Facebook.
  </p>
  -->
</div>
</div>

<span class="anchor" id="news"></span>
<div class="docs-section">
<div class="docs-section">
  <h4>News</h4>
  <div  style="overflow-y: scroll; height:364px;">
<table border="1" style="border-width: 0px;" width="1050">
<tbody>
<tr>
<td style="border-style: none; border-width: medium;">

<ul class="STYLE238">
  <li> 11/2025: One paper accepted at AAAI 2026. </li>
  <li> 11/2025: Our autism gaze target detection paper, led by Shijian in collaboration with <a href="https://labs.utdallas.edu/socialcommunicationlab/staff/">Dr. Rollins‚Äôs lab</a> accepted at AAAI 2026 (AI for Social Impact Track). </li>
  <li> 10/2025: One paper accepted at TMLR. </li>
  <li> 10/2025: I will be serving as an Area Chair for ACL ARR.</li>
  <li> 09/2025: One paper accepted at NeurIPS 2025 (DB Track). </li>
  <li> 09/2025: One paper accepted at IJCV. </li>
  <li> 09/2025: One paper accepted at NeurIPS GenProCC Workshop. The project was led by Michael, a K‚Äì12 student advised by Shijian. </li>
  <li> 08/2025: I will be serving as an Area Chair for AAAI/CVPR/ICLR 2026.</li>
  <li> 08/2025: One paper accepted at EMNLP 2025 Findings. </li> 
  <li> 07/2025: Received an NSF grant.</li>
  <li> 07/2025: One paper accepted at COLM 2025. </li>
  <li> 07/2025: Our AV-DiT paper accepted at ACM MM 2025. </li>
  <li> 07/2025: Two papers conditionally accepted at UIST 2025. </li>
  <li> 07/2025: One paper accepted at BMVC 2025. </li>
  <li> 06/2025: Two papers accepted at ICCV 2025. </li>
  <li> 05/2025: One paper accepted at ACL 2025 (main conference). </li>
  <li> 04/2025: I will be serving as an Area Chair for NeurIPS 2025. </li>
  <li> 04/2025: We are excited to announce the CV4A11y, MCL, and KnowledgeMR workshops at ICCV 2025. More details coming soon. Stay tuned!</li>
  <li> 04/2025: Guest lecture at Texas A&M. </li>
  <li> 02/2025: Two papers accepted at CVPR 2025. </li>
  <li> 02/2025: Our Multimodal Large Language Model Pruning paper accepted at PAKDD 2025. </li>
  <li> 01/2025: Invited talk at UNT Artificial Intelligence (AI) Seminar. </li>
  <li> 12/2024: Our spatial audio generation paper accepted at ICASSP 2025. </li>
  <li> 12/2024: <span style='color: #FFC107;'> <i class='material-icons' style='font-size: 22px; vertical-align: text-bottom;'>emoji_events</i></span>DAVIS won the ACCV Best Paper Award, Honorable Mention! </li>
  <li> 11/2024: One paper accepted at WACV 2025 and one journal article accepted by IEEE TPAMI. </li>
  <li> 10/2024: Our Audio-Visual Dataset Distillation paper accepted at TMLR journal. </li>
  <li> 10/2024: I will be serving as an Area Chair for CVPR 2025.</li>
  <li> 10/2024: <span style='color: #FFC107;'> <i class='material-icons' style='font-size: 22px; vertical-align: text-bottom;'>emoji_events</i></span>ARSports won IEEE ISMAR'24 IDEATExR workshop Best Paper Award!</li>
  <li> 10/2024: Invited talk at UTSW.</li>
  <li> 09/2024: Our Continual Audio-Visual Sound Separation paper accepted at NeurIPS 2024.</li>
  <li> 09/2024: <span style='color: #FFC107;'> <i class='material-icons' style='font-size: 22px; vertical-align: text-bottom;'>emoji_events</i></span>CookAR received <a href="https://uist.acm.org/2024/">UIST'24 Belonging & Inclusion Best Paper Award</a>!</li>
  <li> 09/2024: Received an NIH R01 grant! This exciting project will focus on AI/AR-assisted Vision for people with low vision. We're excited to collaborate with <a href="https://www.yuhangz.com/">Prof. Yuhang Zhao</a> at UW-Madison (lead institute) and <a href="https://jonfroehlich.github.io/">Prof. Jon E. Froehlich</a>  at UW. Check out our preliminary work: <a href="https://github.com/makeabilitylab/CookAR">CookAR</a>.</li>
  <li> 09/2024: Our audio-visual question answering paper accepted at EMNLP 2024.
  <li> 09/2024: Two papers accepted at ACCV 2024. One on audio-visual sound separation and another on language-guided audio-visual editing.
  <li> 09/2024: Our Audio-Visual Autism Behavior Recognition paper acceted at IEEE TMM.</li>
  <li> 08/2024: I will be serving as an Area Chair for ICLR 2025 and a SPC for AAAI 2025.</li>
  <li> 08/2024: William received the Jonsson School of Engineering and Computer Science Award for his Undergraduate Research Project!</li>
  <li> 07/2024  Our CookAR paper accepted at UIST 2024.</li>
  <li> 07/2024: We will be organizing an Audio Imagination workshop at NeurIPS 2024. More details coming soon!</li>
  <li> 07/2024: Our mentored high school students received the Best Science Education Award at the <a href="https://stem.cast-texas.org/">2024 CAST-STEM Bridge Summer Camp</a>.  </li>
  <li> 07/2024: One paper accepted at BMVC 2024. 
  <li> 06/2024: Our EgoVSR paper accepted at IEEE TCSVT. </li>
  <li> 05/2024: One paper accepted at ACM TOMM. </li>
  <li> 05/2024: One paper accepted at ACM C&C. </li>
  <li> 05/2024: Seven papers accepted at CVPR Workshops. </li>
  <li> 04/2024: <a href="https://profiles.utdallas.edu/rollins">Dr. Rollins</a> and I received a UTD SPIRe grant.</li>
  <li> 04/2024: Received an <a href="https://www.amazon.science/research-awards/recipients/yapeng-tian">Amazon Research Award</a>.</li>
  <li> 03/2024: Congrats to Zeke Barnett, a K12 student in the lab! He will be joining CMU for his undergraduate study. </li>
  <li> 03/2024: One paper accepted at NAACL 2024. </li>
  <li> 03/2034: One journal article accepted at Medical Image Analysis. </li>
  <li> 03/2024: One journal article accepted at IEEE TMM. </li>
  <li> 03/2024: Received UTD Undergraduate Research Apprenticeship Program (URAP) award.</li>
  <li>02/2024:  We are organizing an <a href="https://sites.google.com/view/elvm/home">ELVM: Efficient Large Vision Models</a> workshop at CVPR 2024.  </li>
  <li>02/2024:  One paper accepted at CVPR 2024.</li>
  <li>02/2024:  One paper accepted at CHI 2024.</li>
  <li> 10/2023: Invited talk at UTD-DFWCSTA Battle of the Brains - Conference & Contest for K12 students.
  <li> 10/2023: Invited lightning talk at <a href="https://imaging.utdallas.edu/workshop/program/">Workshop on Imaging and Data Science</a>.</li>
  <li>10/2023:  One paper accepted at WACV 2024.</li>
  <li>10/2023:  Listed in 2022 World's Top 2% Scientists by Stanford University.</li>
  <li> 10/2023: Invited talk at Do Good with Data Webinar for K12 students.
  <li> 09/2023: Two papers accepted at NeurIPS 2023.</li>
  <li> 09/2023: One paper accepted at UIST 2023.</li>
  <li> 09/2023: Five papers accepted at ICCV AV4D workshop.</li>
  <li> 07/2023: Three papers accepted at ICCV 2023.</li>
  <li> 06/2023: One paper accepted at MICCAI 2023. We are organizing a <a href="https://cmrxrecon.github.io/">Cardiac MRI Reconstruction Challenge</a> in conjunction with MICCAI 2023.</li>
  <li> 06/2023: Invited talk at <a href="https://sightsound.org/">Sight and Sound Workshop</a> @ CVPR 2023.</li>
  <li> 06/2023: I will serve as a SPC for AAAI 2024.</li>
  <li> 06/2023: Received an Adobe Research Gift.</li>
  <li> 06/2023: One journal paper accepted at IEEE Transactions on Image Processing.</li>
  <li> 05/2023: Three papers accepted at CVPR Sight and Sound Workshop.</li>
  <li> 03/2023: I will serve as an Execution Area Chair for <a href="http://valser.org/">VALSE</a>.</li>
  <li> 03/2023: Received an inaugural Undergraduate Research Apprenticeship Program (URAP) award.</li>
  <li> 03/2023: Received a Cisco Faculty Research Award.</li>
  <li> 03/2023: I will be co-organizing a <a href="https://cmrxrecon.github.io/">Cardiac MRI Reconstruction Challenge</a> in conjunction with MICCAI 2023. </li>
  <li> 02/2023: Three papers accepted at CVPR 2023.</li>
  <li> 02/2023: Please check out our new <a href="https://liangsusan-git.github.io/project/avnerf/">AV-NeRF</a> paper. In this work, we pose and tackle a Real-World Audio-Visual Scene Synthesis problem.</li>
  <li> 02/2023: One journal paper accepted at IEEE Signal Processing Letters.</li>
  <li> 02/2023: One journal paper accepted at IEEE Transactions on Neural Networks and Learning Systems.</li>
  <li> 01/2023: Two papers accepted at ICLR 2023.</li>
  <li> 11/2022: Selected for the 2023 AAAI New Faculty Highlights Program. </li>
  <li> 10/2022: Invited talk at <a href="https://av4d.org/">AV4D Workshop</a> @ ECCV 2022.</li>
  <li> 09/2022: One paper accepted at NeurIPS 2022. Congratulations to Shentong!</li>
  <li> 09/2022: Two papers accepted at <a href="https://av4d.org/">ECCV@AV4D 2022 </a>.</li>
  <li> 08/2022: Please check out our new article "<a href="https://arxiv.org/pdf/2208.09579.pdf">Learning in Audio-visual Context: A Review, Analysis, and New Perspective</a>."</li>
  <li> 08/2022: I start as an assistant professor in CS at UTD. </li>
  <li> 07/2022: I will serve as a Senior Program Committee (SPC) Member for AAAI 2023. </li>
  <li> 07/2022: One paper accepted at ECCV 2022. </li>
  <li> 06/2022: One paper accepted at MICCAI 2022. </li>
  <li> 06/2022: Successfully defended my dissertation! Thanks to everyone who supported me and helped me along the way.   </li>
  <li> 04/2022: I will attend  CVPR'22 Doctoral Consortium.  </li>
  <li> 03/2022: Two works: audio-visual question answering and MRI SR are accepted by CVPR 2022. </li>
  <li> 12/2021: Two papers are accepted by AAAI 2022.</li>
<li> 10/2021: One paper on sounding object localization is accepted by BMVC 2021! </li>
<li> 07/2021: One paper on video matting is accepted by ICCV 2021! </li>
<li> 03/2021: Our two works: co-learn sounding object visual grounding and sound separation and audio-visual robustness are accepted by CVPR 2021!</li>
<li>02/2021: We will co-organize a CVPR 2021 Tutorial on Audio-visual Scene Understanding!</li>

<li>01/2021: Co-organized the WACV 2021 Tutorial on Audio-visual Scene Understanding. More details can be found in our <a href="https://echo0409.github.io/Audio-Visual-Scene-Understanding/">website</a>.</a></li>

<li>10/2020: I was in the top 10% of high-scoring reviewers for NeurIPS 2020! </li>

<li>07/2020: Our audio-visual video parsing work got accepted by ECCV 2020 as a Spotlight</a>.  </li>

<li>05/2020: Our three papers will be presented in the <a href="http://sightsound.org/">CVPR 2020 Sight and Sound workshop</a>.</li>

<li>02/2020: Two papers on video restoration got accepted by CVPR 2020! Congratulations to all co-authors! </li>

<li>01/2020: RDN is accepted by IEEE TPAMI! Congratulations to Yulun!</li>

<li>12/2019: Please check our deep audio prior paper. </li>

<li>08/2019: One paper is accepted by IEEE TIP. Congratulations to Xuechen!</li>

<li>07/2019: One paper is accepted by ICCV 2019. Congratulations to Wei! </li>

<li>05/2019: Our two works: audio-visual event localization and audio-visual video captioning will be presented in the CVPR 2019 Sight and Sound workshop.</li>

<li>02/2019: I will serve as an ICCV 2019 reviewer. </li>

<li>12/2018: Two papers are posted on ArXiv. Please watch the corresponding demos.  </li>

<li>07/2018: One paper is accepted by ECCV 2018! AVE dataset and codes have been released. </li>

<li> 02/2018: One paper is accepted by CVPR 2018. Congratulations to Yulun! </li>

<li>07/2017: I recieve '<strong>Outstanding Graduate of Tsinghua university</strong>' and '<strong>Outstanding Master Thesis Award</strong>'. </li>

<li>03/2017: I will join Prof. <a href="http://www.cs.rochester.edu/u/cxu22/">Chenliang Xu</a>'s lab to pursue a PhD degree at University of Rochester! </li>


</font>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<!-- ========== BIO ========== -->
<span class="anchor" id="group"></span>
<div class="docs-section">
<div class="docs-section">
  <h4>Students</h4>
    <p style="text-align:left"><b>Students at UTD:</b><br>
      <a href="https://scholar.google.com/citations?user=jBbU-_gAAAAJ&hl=en">Siva Sai Nagender Vasireddy</a> (PhD student; Fall 2022)<br>
      <a href="https://scholar.google.com/citations?user=7LBj70IAAAAJ&hl=en">Shijian Deng</a> (PhD student; Spring 2023)<br>
      <a href="https://sakshamsingh1.github.io/">Saksham Singh Kushwaha</a> (PhD student; Summer 2023)<br>
      <a href="https://scholar.google.com/citations?user=ayzNUqcAAAAJ&hl=en">Jia Li</a> (PhD student; Fall 2024)<br>
      <a href="https://sampson-lee.github.io/">Xinpeng Li</a> (PhD student; Fall 2024)<br>
     <!-- <a href="https://weiguopian.github.io/">Weiguo Pian</a> (PhD student co-advised with Dr. Yunhui Guo; Fall 2023)<br> -->
      
      <br>

      <!-- <a href="https://weiguopian.github.io/">Weiguo Pian</a> (PhD student;  co-advised with Prof. Yunhui Guo; Fall 2023)<br> -->
      
     
    
    

    </p>

    <p style="text-align:left"><b>Collaborated External Students:</b><br>
      <a href="//scholar.google.com/citations?user=bdEcg6cAAAAJ&hl=en">Tianyu Yang</a>  (PhD student at University of Notre Dame)<br>
      <a href="https://scholar.google.com/citations?user=6aYncPAAAAAJ&hl=en">Shentong Mo</a> (PhD student at Carnegie Mellon University)<br>
      <a href="https://scholar.google.com/citations?user=dBVSZWAAAAAJ&hl=zh-CN">Kai Wang</a> (PhD student at University of Toronto)<br>
    </p>

    <p style="text-align:left"><b>Alumni:</b><br>
    Vaishnavi Josyula (Undergraduate, School of Natural Sciences & Mathematics, Fall 2025)
    Ziru Huang (Visiting student; Tsinghua University; 2024)<br>
    <a href="https://nanyyyyyy.github.io/">Yiyang Nan</a> (Graduate student at Brown University; Spring 2023-2024; Next: researcher at Cohere for AI)<br>
    Matthew Wang (K12; Summer 2023-2024, Next Undergraduate at Cornell University)<br>
    <a href="https://wtd-1.github.io/web/"> William Doan</a> (Undergraduate; Fall 2023 - Summer 2024; Jonsson School of Engineering and Computer Science Award; Next: PhD student at UTD's CS Theory Group)<br>
    Zeke Barnett (K12; Parish Episcopal School at Dallas, Spring 2023 - Spring 2024; Next Undergraduate at CMU)<br>
    Anikait Bharadwaj (K12; Frisco ISD; Spring 2024)<br>
    Aditya Kulkarni (Undergraduate; Spring 2023; Next Meta)<br>
    Michael Yang (K12; Summer 2023)<br>
    Atmin Mehul Sheth (Undergraduate at UTD; 2023)<br>
    Yuxin Ye (Graduate student at Tsinghua University)<br>
    Yichen Chi (Graduate student at Tsinghua University)<br>
    Junhao Gu (PhD student at Tsinghua University)<br>
    Jiamiao Zhang (Graduate student at Tsinghua University)<br>
    <a href="https://fangsen9000.github.io/">Sen Fang</a> (Undergraduate at Victoria University, Next: PhD student at Rutgers University)<br>
    Sasha Kaplan (Undergraduate; Spring 2023)<br>
    Sisi Aarukapalli (Undergraduate; Summer 2023) <br>
    Harsh Singh (PhD student at UTD; Spring and Summer 2023; Next: CV MSC at MBZUAI)<br>
    Yulang Wu (Graduate student at UTD CS, Spring 2023; Next: Postdoc at University of California San Francisco)<br>
    <a href="https://ayameyao.github.io/">Guangyao Li</a> (PhD student at Renmin University of China, Fall 2020 - Spring 2023)<br>
    <!-- Prathyushaa Vajravelu Karthikeyan (Graduate student at UTD; Spring 2023)<br> -->
    Shijian Deng (Graduate student at University of Rochester; next: PhD student at UTD)<br>
    Hai Wang (Graduate student at Tsinghua University; next: PhD student at UCL)<br>
    <a href="https://sizhe-li.github.io/">Sizhe Li</a> (Undergraduate student at University of Rochester; next: Visiting student at MIT)<br>
    Yiyang Su (Undergraduate at University of Rochester; next: PhD student at Michigan State University)<br>
    Rohan Sharma (Graduate student at University of Rochester; next: PhD student at SUNY Buffalo) <br>
    Chenxiao Guan (Undergraduate at University of Rochester; next: Graduate student at CMU)<br>

    </p>

</div>
</div>

<!-- ========== PUBLICATIONS ========== -->
<span class="anchor"  id="publications"></span>
<div class="docs-section">
<div class="docs-section">
  <h4>Publications</h4>

  <p>Most recent publications on <a href="https://scholar.google.com/citations?user=lxCqdpoAAAAJ&hl=en" target="_blank">Google Scholar</a>.<br/>
  <sup>‚Ä°</sup> indicates equal contribution.
  </p>

  <ul class="tab-nav">
    <li><div class="button active" data-ref="#papers-all" font-size: 10px style="width:40px;padding:0 0;">All</div></li>
    <li><div  class="button" data-ref="#papers-arxiv" font-size: 10px style="width:90px;padding:0 0;">Preprint</div></li>
    <li><div class="button" data-ref="#papers-audiovisual" font-size: 10px  style="width:120px;padding:0 0;">Vision+Sound</div></li>
    <li><div class="button" data-ref="#papers-accessibility" font-size: 10px  style="width:120px;padding:0 0;">Accessibility</div></li>
    <li><div class="button" data-ref="#papers-videorestoration" font-size: 10px  style="width:160px;padding:0 0;">Video Restoration</div></li>
    <li><div class="button" data-ref="#papers-imagerestoration" font-size: 10px  style="width:160px;padding:0 0;">Image Restoration</div></li>
  </ul>

  <!-- <div class="tab-content">
    <div class="tab-pane " id="papers-all">
      
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_ommsi/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Towards Online Multi-Modal Social Interaction Understanding</b></p>
          <p>Xinpeng Li, Shijian Deng, Bolin Lai, Weiguo Pian, James M. Rehg, <b>Yapeng Tian</b></p>
          <p><i>Preprint'25. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2503.19851" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_iMMLM/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Modality-Inconsistent Continual Learning of Multimodal Large Language Models</b></p>
          <p>Weiguo Pian, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>Preprint'24. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.13050" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2022_avsurvey/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Learning in Audio-visual Context: A Review, Analysis, and New Perspective</b></p>
          <p>Yake Wei, Di Hu, <b>Yapeng Tian</b>, Xuelong Li</p>
          <p><i>Preprint'22. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2208.09579.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://gewu-lab.github.io/audio-visual-learning/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_segbias/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?</b></p>
          <p>Jia Li, Wenjie Zhao, Ziru Huang, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>AAAI'26: Annual AAAI Conference on Artificial Intelligence. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2502.00358" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2026_autismgaze/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Toward Gaze Target Detection of Young Autistic Children</b></p>
          <p>Shijian Deng, Erin E. Kosloski, Siva Sai Nagender Vasireddy, Jia Li, Randi Sierra Sherwood, Feroz Mohamed Hatha, Siddhi Patel, Pamela R Rollins, <b>Yapeng Tian</b></p>
          <p><i>AAAI'26 Oral: AAAI Conference on Artificial Intelligence (Social Impact Track). </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2511.11244" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_AVROBUSTBENCH/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>AVROBUSTBENCH: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time</b></p>
          <p>Sarthak Kumar Maharana, Saksham Singh Kushwaha, Baoming Zhang, Adrian Rodriguez, Songtao Wei, <b>Yapeng Tian</b>, Yunhui Guo</p>
          <p><i>NeurIPS'25: Conference on Neural Information Processing Systems (D&B Track). </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2506.00358" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/sarthaxxxxx/AVROBUSTBENCH" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_dpo/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach</b></p>
          <p>Shijian Deng, Wentian Zhao, Yu-Jhe Li, Kun Wan, Daniel Miranda, Ajinkya Kale, <b>Yapeng Tian</b></p>
          <p><i>COLM'25: Second Conference on Language Modeling. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2411.17760" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_survey/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Self-Improvement in Multimodal Large Language Models: A Survey</b></p>
          <p>Shijian Deng, Kai Wang, Tianyu Yang, Harsh Singh, <b>Yapeng Tian</b></p>
          <p><i>EMNLP'25 Findings: Conference on Empirical Methods in Natural Language Processing. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2510.02665" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_AVDiT/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation</b></p>
          <p>Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, <b>Yapeng Tian</b></p>
          <p><i>ACM MM'25: ACM International Conference on Multimedia. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.07686" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_davis/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling</b></p>
          <p>Chao Huang, Susan Liang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>IJCV'25: International Journal of Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2509.22063" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_vrsight/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People</b></p>
          <p>Daniel Killough, Justin Feng, Zheng Xue Ching, Daniel Wang, Rithvik Dyava, <b>Yapeng Tian</b>, Yuhang Zhao</p>
          <p><i>UIST'25:  ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://dl.acm.org/doi/full/10.1145/3746059.3747641" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_aroma/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>AROMA: Mixed-Initiative AI Assistance for Non-Visual Cooking by Grounding Multi-modal Information Between Reality and Videos</b></p>
          <p>Zheng Ning, Leyang Li, Daniel Killough, JooYoung Seo, Patrick Carrington, <b>Yapeng Tian</b>, Yuhang Zhao, Franklin Mingzhe Li, Toby Jia-Jun Li</p>
          <p><i>UIST'25:  ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2507.10963" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_tpblend/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models</b></p>
          <p>Xin Jin, Yichuan Zhong, <b>Yapeng Tian</b></p>
          <p><i>TMLR'25: Transactions on Machine Learning Research. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/pdf?id=q6M73uOBZE" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_PEAVL/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Prompt Image to Watch and Hear: Multimodal Prompting for Parameter-Efficient Audio-Visual Learning</b></p>
          <p>Kai Wang, Shentong Mo, <b>Yapeng Tian</b>, Dimitrios Hatzinakos</p>
          <p><i>BMVC'25: The British Machine Vision Conference (BMVC). </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_signllm/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Signllm: Sign language production large language models</b></p>
          <p>Sen Fang, Chen Chen, Lei Wang, Ce Zheng, Chunyu Sui, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'25:  IEEE/CVF International Conference on Computer Vision CV4A11y Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/ICCV2025W/CV4A11y/papers/Fang_SignLLM_Sign_Language_Production_Large_Language_Models_ICCVW_2025_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://signllm.github.io/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_cv4a11y/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Introduction to the First Workshop on Vision Foundation Models and Generative AI for Accessibility</b></p>
          <p><b>Yapeng Tian</b>, Yuhang Zhao, Jon E. Froehlich, Chu Li, Yuheng Wu</p>
          <p><i>ICCVW'25:  IEEE/CVF International Conference on Computer Vision CV4A11y Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/ICCV2025W/CV4A11y/papers/Tian_Introduction_to_the_First_Workshop_on_Vision_Foundation_Models_and_ICCVW_2025_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_ZFusion/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>ZFusion: Efficient Deep Compositional Zero-shot Learning for Blind Image Super-Resolution with Generative Diffusion Prior</b></p>
          <p>Alireza Esmaeilzehi, Hossein Zaredar, <b>Yapeng Tian</b>, Laleh Seyyed-Kalantari</p>
          <p><i>ICCV'25: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/2673.png?t=1759954583.3458676" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_prvql/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>PRVQL: Progressive Knowledge-guided Refinement for Robust Egocentric Visual Query Localization</b></p>
          <p>Bing Fan, Yunhe Feng, <b>Yapeng Tian</b>, Yuewei Lin, Yan Huang, Heng Fan</p>
          <p><i>ICCV'25: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2502.07707" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://github.com/fb-reps/PRVQL" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_VinTAGe/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation</b></p>
          <p>Saksham Singh Kushwaha, <b>Yapeng Tian</b></p>
          <p><i>CVPR'25: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.10768" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://sakshamsingh1.github.io/vintage/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_motion/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level</b></p>
          <p>Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, <b>Yapeng Tian</b>, Ajmal Saeed Mian, Mohit Bansal, Chen Chen</p>
          <p><i>CVPR'25: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2411.09921" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://groundmore.github.io/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_Diff-SAGe/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Diff-SAGe: End-to-End Spatial Audio Generation Using Diffusion Models</b></p>
          <p>Saksham Singh Kushwaha, Jianbo Ma, Mark R. P. Thomas,  <b>Yapeng Tian</b>, and Avery Bruni</p>
          <p><i>ICASSP'25: IEEE International Conference on Acoustics, Speech, and Signal Processing. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2410.11299" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_cliperase/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP</b></p>
          <p>Tianyu Yang, Lisen Dai, Zheyuan Liu, Xiangqi Wang, Meng Jiang, <b>Yapeng Tian</b>, Xiangliang Zhang</p>
          <p><i>ACL'25 Main: Annual Meeting of the Association for Computational Linguistics. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.23330" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://tianyu-yang-anna.github.io/ClipErase-ACL/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_magictalk/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>MagicTalk: Implicit and Explicit Correlation Learning for Diffusion-based Emotional Talking Face Generation</b></p>
          <p>Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, <b>Yapeng Tian</b>, Jiashi Feng, Xiaohu Guo</p>
          <p><i>CVM:  Computational Visual Media Journal. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11145205" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://magictalk.github.io/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_chiea/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Demonstration of VRSight: AI-Driven Real-Time Descriptions to Enhance VR Accessibility for Blind People</b></p>
          <p>Daniel Killough, Justin Feng, Rithvik Dyava, Zheng Xue Ching, Daniel Wang, <b>Yapeng Tian</b>, Yuhang Zhao</p>
          <p><i>CHI EA'25: Extended Abstracts of the CHI Conference </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://dl.acm.org/doi/abs/10.1145/3706599.3721194" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_srcld/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Leveraging AI to Assess Social Attention in Young Autistic Children</b></p>
          <p>Erin Kosloski, Shijian Deng, Siva S. N. Vasireddy, Randi S. Sherwood, Feroz M. Hatha, Jia Li, Siddhi Patel, <b>Yapeng Tian</b>, Pamela Rollins</p>
          <p><i>SRCLD'25: Symposium on Research in Child Language Disorders. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.researchgate.net/publication/392469852_Leveraging_Artificial_Intelligence_to_Assess_Social_Attention_in_Young_Autistic_Children" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_signdiff/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>SignDiff: Learning Diffusion Models for American Sign Language Production</b></p>
          <p>Sen Fang, Chunyu Sui, Yanghao Zhou, Xuedong Zhang, Hongbin Zhong, <b>Yapeng Tian</b>, Chen Chen</p>
          <p><i>FGW'25: International Conference on Automatic Face and Gesture Recognition Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.16082.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_eMMLM/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Language-Guided Adaptive Vision Token Pruning for Efficient Multimodal Large Language Models</b></p>
          <p>Omer Faruk Deniz, Tarik Arici, Fatemeh Sheikholeslami, Burak Gozluklu, Ameni Trabelsi, Suleiman Khan, <b>Yapeng Tian</b>, Latifur Khan</p>
          <p><i>PAKDD'25 Oral: The Pacific-Asia Conference on Knowledge Discovery and Data Mining. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://link.springer.com/chapter/10.1007/978-981-96-8186-0_9" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_gesture/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Joint Co-Speech Gesture and Expressive Talking Face Generation using Diffusion with Adapters</b></p>
          <p>Steven Hogue, Chenxu Zhang, <b>Yapeng Tian</b>, Xiaohu Guo</p>
          <p><i>WACV'25: IEEE/CVF Winter Conference on Applications of Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.14333" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2025_diffir/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>DiffIR: Efficient Diffusion Model for Image Restoration</b></p>
          <p>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, <b>Yapeng Tian</b>, Wenming Yang, Radu Timotfe, Luc Van Gool</p>
          <p><i>TPAMI'25: IEEE Transactions on Pattern Analysis and Machine Intelligence. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.13767" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/DiffIR" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_avdd/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Audio-Visual Dataset Distillation</b></p>
          <p>Saksham Singh Kushwaha, Siva Sai Nagender Vasireddy, Kai Wang, <b>Yapeng Tian</b></p>
          <p><i>TMLR'24: Transactions on Machine Learning Research </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/forum?id=IJlbuSrXmk" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_cavss/teaser1.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Continual Audio-Visual Sound Separation</b></p>
          <p>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>NeurIPS'24: The Annual Conference on Neural Information Processing Systems </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Pian_Learning_Continual_Audio-Visual_Sound_Separation_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_avasd/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition</b></p>
          <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
          <p><i>TMM'24: IEEE Transactions on Multimedia. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2406.02554" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://github.com/ShijianDeng/AV-ASD" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_ARSPORT/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span>Towards AI-Powered AR for Enhancing Sports Playability for People with Low Vision: An Exploration of ARSports <span style='color: red;'> (Best Paper Award)</span></b></p>
          <p>Jaewook Lee, Yang Li, Dylan Bunarto, Eujean Lee, Olivia Wang, Adrian Rodriguez, Yuhang Zhao, <b>Yapeng Tian</b>, Jon E. Froehlich</p>
          <p><i>ISMAR IDEATExR'24 : International Symposium on Mixed and Augmented Reality Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://makeabilitylab.cs.washington.edu/media/publications/Lee_TowardsAiPoweredArForEnhancingSportsPlayabilityForPeopleWithLowVisionAnExplorationOfArsports_IDEATExR2024.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_cookar/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span> CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool Interactions for People with Low Vision <span style='color: red;'> (Belonging & Inclusion Best Paper Award)</span></b></p>
          <p>Jaewook Lee, Andrew D. Tjahjadi, Jiho Kim, Junpu Yu, Minji Park, Jiawen Zhang, Jon E. Froehlich, <b>Yapeng Tian</b>, Yuhang Zhao</p>
          <p><i>UIST'24: ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2407.13515" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://github.com/makeabilitylab/CookAR" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_davis/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span> DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models<br><span style='color: red;'> (Best Paper Honorable Mention)</span></b></p>
          <p>Chao Huang, Susan Liang,  <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ACCV'24 Oral: Asian Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2308.00122" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_avedit/OAVE.svg" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Language-Guided Joint Audio-Visual Editing Via One-Shot Adaptation</b></p>
          <p>Susan Liang, Chao Huang,  <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ACCV'24: Asian Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.07463" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_SaSR/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering</b></p>
          <p>Tianyu Yang, Yiyang Nan, Lisen Dai, Zhenwen Liang, <b>Yapeng Tian</b>, Xiangliang Zhang</p>
          <p><i>EMNLP'24: Empirical Methods in Natural Language Processing (Findings) </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2411.04933" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_LFAV/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Towards Long Form Audio-visual Video Understanding</b></p>
          <p>Wenxuan Hou, Guangyao Li, <b>Yapeng Tian</b>, Di Hu</p>
          <p><i>TOMM'24: ACM Trans. on Multimedia Computing, Communications and App. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2306.09431" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://gewu-lab.github.io/LFAV/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_BOFL/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Benchmarking and Optimizing Federated Learning with Hardware-related Metrics</b></p>
          <p>Kai Pan, <b>Yapeng Tian</b>, Yinhe Han, Yiming Gan</p>
          <p><i>BMVC'24: British Machine Vision Conference </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_egovsr/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>EgoVSR: Towards High-Quality Egocentric Video Super-Resolution</b></p>
          <p>Yichen Chi, Junhao Gu, Jiamiao Zhang, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>TCSVT'24: IEEE Transactions on Circuits and Systems for Video Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2305.14708.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_MIMOSA/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>MIMOSA: Human-AI Co-Creation of Computational Spatial Audio Effects on Videos</b></p>
          <p>Zheng Ning, Zheng Zhang, Jerrick Ban, Kaiwen Jiang, Ruohong Gan, <b>Yapeng Tian</b>, Toby Jia-Jun Li</p>
          <p><i>C&C'24: ACM Conference on Creativity & Cognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://dl.acm.org/doi/pdf/10.1145/3635636.3656189" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_avmamba/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>AV-Mamba: Cross-Modality Selective State Space Models for Audio-Visual Question Answering</b></p>
          <p>Ziru Huang, Jia Li, Wenjie Zhao, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Huang_AV-Mamba_Cross-Modality_Selective_State_Space_Models_for_Audio-Visual_Question_Answering.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_cavss/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Learning Continual Audio-Visual Sound Separation Models</b></p>
          <p>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Pian_Learning_Continual_Audio-Visual_Sound_Separation_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_avasd/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Audio-Visual Autism Behavior Recognition with MMLMs</b></p>
          <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Audio-Visual_Autism_Behavior_Recognition_with_Multimodal_Large_Language_Models_Audio-Visual_Autism_Behavior_Recognition_with_Multimodal_Large_Language_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_avdd/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Dataset distillation for audio-visual datasets</b></p>
          <p>Saksham Singh Kushwaha, Siva Sai Nagender Vasireddy, Kai Wang, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Kushwaha_Dataset_distillation_for_audio-visual_datasets.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_tedgen/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures</b></p>
          <p>Steven Hogue, Chenxu Zhang, Hamza Daruger, <b>Yapeng Tian</b>, Xiaohu Guo</p>
          <p><i>CVPRW'24: CVPR HuMoGen Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2024W/HuMoGen/papers/Hogue_DiffTED_One-shot_Audio-driven_TED_Talk_Video_Generation_with_Diffusion-based_Co-speech_CVPRW_2024_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_stgcma/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Towards Efficient Audio-Visual Learners via Empowering Pre-trained Vision Transformers with Cross-Modal Adaptation</b></p>
          <p>Kai Wang, <b>Yapeng Tian</b>, Dimitrios Hatzinakos</p>
          <p><i>CVPRW'24: CVPR Multimodal Foundation Models Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2024W/MMFM/papers/Wang_Towards_Efficient_Audio-Visual_Learners_via_Empowering_Pre-trained_Vision_Transformers_with_CVPRW_2024_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/kaiw7/STG-CMA" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_MAAVT/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>MA-AVT: Modality Alignment for Parameter-Efficient Audio-Visual Transformers</b></p>
          <p>Tanvir Mahmud, Shentong Mo, <b>Yapeng Tian</b>, Diana Marculescu</p>
          <p><i>CVPRW'24: CVPR Efficient Deep Learning for Computer Vision Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.04930" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_TVSL/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>T-VSL: Text-Guided Visual Sound Source Localization in Mixtures</b></p>
          <p>Tanvir Mahmud, <b>Yapeng Tian</b>, Diana Marculescu</p>
          <p><i>CVPR'24: IEEE/CVF Conference on Computer Vision and Pattern Recognition </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2404.01751" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/enyac-group/T-VSL/tree/main" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_oscar/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>OSCaR: Object State Captioning and State Change Representation</b></p>
          <p>Nguyen Nguyen, Jing Bi, Ali Vosoughi, <b>Yapeng Tian</b>, Pooyan Fazli, Chenliang Xu</p>
          <p><i>NAACL'24: The North American Chapter of the Association for Computational Linguistics (Findings) </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2402.17128" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_SPICA/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers</b></p>
          <p>Zheng Ning, Brianna Wimer, Kaiwen Jiang, Keyi Chen, Jerrick Ban, <b>Yapeng Tian</b>, Yuhang Zhao, Toby Li</p>
          <p><i>CHI'24: The ACM Conference on Human Factors in Computing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2402.07300" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_STADNET/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>STADNet: Spatial-Temporal Attention-Guided Dual-Path Network for cardiac cine MRI super-resolution</b></p>
          <p>Jun Lyu, Shuo Wang, <b>Yapeng Tian</b>, Jing Zou, Shunjie Dong, Chengyan Wang, Angelica I Aviles-Rivero, Jing Qin</p>
          <p><i>MIA'24: Medical Image Analysis</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841524000677" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2024_vqa/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Unveiling cross modality bias in visual question answering: A causal view with possible worlds vqa</b></p>
          <p>Ali Vosoughi<sup>‚Ä°</sup>, Shijian Deng<sup>‚Ä°</sup>, Songyang Zhang, <b>Yapeng Tian</b>, Chenliang Xu, Jiebo Luo</p>
          <p><i>TMM'24: IEEE Transactions on Multimedia</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2305.19664" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_mavs/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b> LAVSS: Location-Guided Audio-Visual Spatial Audio Separation</b></p>
          <p>Yuxin Ye, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>WACV'24: Winter Conference on Applications of Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2310.20446.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://yyx666660.github.io/LAVSS/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_dcl/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Disentangled counterfactual learning for physical audiovisual commonsense reasoning</b></p>
          <p>Changsheng Lv, Shuai Zhang, <b>Yapeng Tian</b>, Mengshi Qi, Huadong Ma</p>
          <p><i>NeurIPS'23: The Annual Conference on Neural Information Processing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2310.19559" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Andy20178/DCL" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_avnerf/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>NeurIPS'23: The Annual Conference on Neural Information Processing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2302.02088.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://liangsusan-git.github.io/project/avnerf/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_PEANUT/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>PEANUT: A Human-AI Collaborative Tool for Annotating Audio-Visual Data</b></p>
          <p>Zheng Zhang<sup>‚Ä°</sup>, Zheng Ning<sup>‚Ä°</sup>, Chenliang Xu <b>Yapeng Tian</b>, Toby Jia-Jun Li</p>
          <p><i>UIST'23: ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2307.15167.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_rasd/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b> Towards Robust Active Speaker Detection</b></p>
          <p>Siva Sai Nagender Vasireddy, Chenxu Zhang, Xiaohu Guo, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p5.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_mavs/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b> Position-Aware Audio-Visual Separation for Spatial Audio</b></p>
          <p>Yuxin Ye, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p8.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_mimo/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b> Towards Better Egocentric Action Understanding in a Multi-Input Multi-Output View</b></p>
          <p>Wenxuan Hou, Ruoxuan Feng, Yixin Xu, <b>Yapeng Tian</b>, Di Hu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p13.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_nacf/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b> Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p1.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_invisiblesep/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b> Separating Invisible Sounds Toward Universal Audio-Visual Scene-Aware Sound Separation</b></p>
          <p>Yiyang Su, Ali Vosoughi, Shijian Deng, <b>Yapeng Tian</b>, Chenliang Xu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p3.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_avcil/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b> Audio-Visual Class-Incremental Learning</b></p>
          <p>Weiguo Pian<sup>‚Ä°</sup>, Shentong Mo<sup>‚Ä°</sup>, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.11073.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/weiguoPian/AV-CIL_ICCV2023" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_CIGN/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b> Class-Incremental Grouping Network for Continual Audio-Visual Learning</b></p>
          <p>Shentong Mo<sup>‚Ä°</sup>, Weiguo Pian<sup>‚Ä°</sup>, <b>Yapeng Tian</b></p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2309.05281.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/CIGN" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_diffir/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>DiffIR: Efficient Diffusion Model for Image Restoration</b></p>
          <p>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, <b>Yapeng Tian</b>, Wenming Yang, Luc Van Gool</p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2303.09472" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/DiffIR" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_MCMRI/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</b></p>
          <p>Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>MICCAI'23: Medical Image Computing and Computer-Assisted Intervention. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2307.02334" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/jmzhang79/Dual-ArbNet" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_mrda/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Meta-Learning based Degradation Representation for Blind Super-Resolution</b></p>
          <p>Bin Xia,  <b>Yapeng Tian</b>, Yulun Zhang, Yucheng Hang, Wenming Yang, Qingmin Liao</p>
          <p><i>TIP'23: IEEE Transactions on Image Processing. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2207.13963" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/MRDA" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_avsam/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2305.01836.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_diffava/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment</b></p>
          <p>Shentong Mo, Jing Shi, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2305.12903" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_avnerf/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2302.02088.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://liangsusan-git.github.io/project/avnerf/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_avg/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Audio-Visual Grouping Network for Sound Localization from Mixtures</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2303.17056.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/AVGN" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_egoav/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Egocentric Audio-Visual Object Localization</b></p>
          <p>Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, and Chenliang Xu</p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2303.13471.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/WikiChao/Ego-AV-Loc" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_ssl/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Structured Sparsity Learning for Efficient Video Super-Resolution</b></p>
          <p>Bin Xia, Jingwen He, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Luc Van Gool</p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2206.07687" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/SSL" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_iclrkd/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Knowledge Distillation based Degradation Estimation for Blind Super-Resolution</b></p>
          <p>Bin Xia, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Radu Timofte, Luc Van Gool</p>
          <p><i>ICLR'23: International Conference on Learning Representations.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2211.16928" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/KDSR" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_iclrbbcu/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Basic Binary Convolution Unit for Binarized Image Restoration Network</b></p>
          <p>Bin Xia, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Radu Timofte, Luc Van Gool</p>
          <p><i>ICLR'23: International Conference on Learning Representations.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2210.00405.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/BBCU" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_dan/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Stdan: deformable attention network for space-time video super-resolution</b></p>
          <p>Hai Wang, Xiaoyu Xiang, <b>Yapeng Tian</b>, Wenming Yang, Qingmin Liao</p>
          <p><i>TNNLS'23: IEEE Transactions on Neural Networks and Learning Systems.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/abstract/document/10045744" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/littlewhitesea/STDAN" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_spl/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>GDSSR: Toward Real-World Ultra-High-Resolution Image Super-Resolution</b></p>
          <p>Yichen Chi, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>SPL'23: IEEE Signal Processing Letters.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/abstract/document/10041757" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2023_aaainfh/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Towards Unified, Explainable, and Robust Multisensory Perception</b></p>
          <p>Yapeng Tian</p>
          <p><i>AAAI'23: AAAI Conference on Artificial Intelligence. (NFH program)</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="/assets/publications/2023_aaainfh/paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2022_mgn/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>NeurIPS'22: The Annual Conference on Neural Information Processing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/pdf?id=zfo2LqFEVY" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/MGN" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2022_std/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Learning Spatio-Temporal Downsampling for Effective Video Upscaling</b></p>
          <p>Xiaoyu Xiang, <b>Yapeng Tian</b>, Vijay Rengarajan, Lucas Young, Bo Zhu, Rakesh Ranjan</p>
          <p><i>ECCV'22: European Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136780159.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2022_thesis/thesis.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Audio-Visual Scene Understanding Towards Unified, Explainable, and Robust Multisensory Perception</b></p>
          <p><b>Yapeng Tian</b></p>
          <p><i>PhD Thesis </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.proquest.com/openview/99bef5e8207df0ebab29e6b1f2afbad8/1.pdf?pq-origsite=gscholar&cbl=18750&diss=y" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2022_dudocaf/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>DuDoCAF: Dual-Domain Cross-Attention Fusion with Recurrent Transformer for Fast Multi-contrast MR Imaging</b></p>
          <p>Jun Lyu, Bin Sui, Chengyan Wang, <b>Yapeng Tian</b>, Qi Dou, and Jing Qin</p>
          <p><i>MICCAI'22: Medical Image Computing and Computer Assisted Intervention. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="/assets/publications/2022_dudocaf/paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2022_avol/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Audio-Visual Object Localization in Egocentric Videos</b></p>
          <p>Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, and Chenliang Xu</p>
          <p><i>CVPRW'22: CVPR Workshops</i></p>
          
          <p style="color:red">Egocentric audio-visual learning.</p>
         
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2022_avqa/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</b></p>
          <p>Guangyao Li<sup>‚Ä°</sup>, Yake Wei<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, Chenliang Xu, Ji-Rong Wen, and Di Hu</p>
          <p><i>CVPR'22 Oral: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2203.14072.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=JH3t5gwe9Xw" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/GeWu-Lab/MUSIC-AVQA" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://gewu-lab.github.io/MUSIC-AVQA/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2022_mrisr/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Transformer-empowered Multi-contrast MRI Super-Resolution</b></p>
          <p>Guangyuan Li, Jun Lv, <b>Yapeng Tian</b>, Qi Dou, Chengyan Wang, Chenliang Xu, Jing Qin</p>
          <p><i>CVPR'22: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2203.13963.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/XAIMI-Lab/McMRSR" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2022_amsa/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution</b></p>
          <p>Bin Xia, <b>Yapeng Tian</b>, Yucheng Hang, Wenming Yang, Qingmin Liao, Jie Zhou</p>
          <p><i>AAAI'22: The AAAI Conference on Artificial Intelligence.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2201.04358.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/AMSA" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2022_enlca/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Efficient Non-Local Contrastive Attention for Image Super-Resolution</b></p>
          <p>Bin Xia<sup>‚Ä°</sup>, Yucheng Hang<sup>‚Ä°</sup>, <b>Yapeng Tian</b>, Wenming Yang, Qingmin Liao, Jie Zhou</p>
          <p><i>AAAI'22: The AAAI Conference on Artificial Intelligence. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2201.03794.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/ENLCA" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2021_stm/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Space-Time Memory Network for Sounding Object Localization in Videos</b></p>
          <p>Sizhe Li<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, and Chenliang Xu</p>
          <p><i>BMVC'21: The British Machine Vision Conference. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2111.05526.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/lester0866/Space-Time-Memory-Network-for-Sounding-Object-Localization-in-Videos" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://sites.google.com/view/bmvc2021stm" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2021_matting/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Video Matting via Consistency-Regularized Graph Neural Networks</b></p>
          <p>Tiantian Wang, Sifei Liu, <b>Yapeng Tian</b>, Kai Li, and Ming-Hsuan Yang</p>
          <p><i>ICCV'21: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://faculty.ucmerced.edu/mhyang/papers/iccv2021_video_matting.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/TiantianWang/VideoMatting-CRGNN" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2021_robustness/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Can audio-visual integration strengthen robustness under multimodal attacks?</b></p>
          <p><b>Yapeng Tian</b> and Chenliang Xu</p>
          <p><i>CVPR'21: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2104.02000.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/AV-Robustness-CVPR21" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2021_ccol/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation</b></p>
          <p><b>Yapeng Tian</b>, Di Hu, and Chenliang Xu</p>
          <p><i>CVPR'21: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2104.02026.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/CCOL-CVPR21" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2020_avvp/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing</b></p>
          <p><b>Yapeng Tian</b>, Dingzeyu Li, and Chenliang Xu</p>
          <p><i>ECCV'20 Spotlight: European Conference on Computer Vision.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2007.10558.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/AVVP-ECCV20" target="_blank">Code</a>
            

            
              <a class="button" href="https://github.com/YapengTian/AVVP-ECCV20" target="_blank">Data</a>
            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2020_zsm/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution</b></p>
          <p>Xiaoyu Xiang<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, Yulun Zhang, Yun Fu, Jan Allebach, and Chenliang Xu</p>
          <p><i>CVPR'20: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2002.11616.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=8mgD8JxBOus" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2020_tdan/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>TDAN: Temporally Deformable Alignment Network for Video Super-Resolution</b></p>
          <p><b>Yapeng Tian</b>, Yulun Zhang, Yun Fu, and Chenliang Xu</p>
          <p><i>CVPR'20: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
          
          <p style="color:red">This is the first work that uses deformable alignment to address video restoration.</p>
         
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Tian_TDAN_Temporally-Deformable_Alignment_Network_for_Video_Super-Resolution_CVPR_2020_paper.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=eZExENE50I0" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/YapengTian/TDAN-VSR-CVPR-2020" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2020_dap/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Deep Audio Prior</b></p>
          <p><b>Yapeng Tian</b>, Chenliang Xu, and Dingzeyu Li</p>
          <p><i>CVPRW'20: CVPR Workshops.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1912.10292v1.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/adobe/Deep-Audio-Prior" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://opensource.adobe.com/Deep-Audio-Prior/" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2020_rdnpami/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Residual Dense Network for Image Super-Resolution</b></p>
          <p>Yulun Zhang, <b>Yapeng Tian</b>, Yu Kong , Bineng Zhong, Yun Fu</p>
          <p><i>TPAMI'20: IEEE Transactions on Pattern Analysis and Machine Intelligence.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1812.10477.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/yulunzhang/RDN" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2019_csf/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>CFSNet: Toward a Controllable Feature Space for Image Restoration</b></p>
          <p>Wei Wang<sup>‚Ä°</sup>, Ruiming Guo<sup>‚Ä°</sup>, <b>Yapeng Tian</b>, and Wenming Yang</p>
          <p><i>ICCV'19: IEEE/CVF International Conference on Computer Vision.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1904.00634.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/qibao77/CFSNet" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2019_avc/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Interpretable and Controllable Audio-Visual Video Captioning</b></p>
          <p><b>Yapeng Tian</b>, Chenxiao Guan, Goodman Justin, Marc Moore, and Chenliang Xu</p>
          <p><i>CVPRW'19: CVPR Workshops.</i></p>
          
          <p style="color:red">Multisensory interpretability in terms of the audio-visual video captioning task.</p>
         
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Yapeng_Tian_Audio-Visual_Interpretable_and_Controllable_Video_Captioning_CVPRW_2019_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2019_lcsc/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>LCSCNet: Linear Compressing Based Skip-Connecting Network for ISR</b></p>
          <p>Wenming Yang, Xuechen Zhang, <b>Yapeng Tian</b>, Wei Wang, Jing-Hao Xue, Qingmin Liao</p>
          <p><i>TIP'19: IEEE Trans. Image Processing.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/1909.03573" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2019_review/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Deep Learning for Single Image Super-Resolution: A Brief Review</b></p>
          <p>Wenming Yang, Xuechen Zhang, <b>Yapeng Tian</b>, Wei Wang, JingHao Xue, Qingmin Liao</p>
          <p><i>TMM'19: IEEE Trans. Multimedia.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1808.03344.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2018_ave/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Audio-Visual Event Localization in Unconstrained Videos</b></p>
          <p><b>Yapeng Tian</b>, Jing Shi, Bochen Li, Zhiyao Duan, Chenliang Xu</p>
          <p><i>ECCV'18: European Conference on Computer Vision.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=m6r6BbD5MSc" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/YapengTian/AVE-ECCV18" target="_blank">Code</a>
            

            
              <a class="button" href="https://drive.google.com/file/d/1FjKwe79e0u96vdjIVwfRQ1V6SoDHe7kK" target="_blank">Data</a>
            

            
              <a class="button" href="https://sites.google.com/view/audiovisualresearch" target="_blank">Project</a>
            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2018_rdn/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Residual Dense Network for Image Super-Resolution</b></p>
          <p>Yulun Zhang, <b>Yapeng Tian</b>, Yu Kong , Bineng Zhong, Yun Fu</p>
          <p><i>CVPR'18 Spotlight: IEEE/CVF Conf. on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1802.08797.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/yulunzhang/RDN" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2017_ntire/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results</b></p>
          <p>Timofte et al.</p>
          <p><i>CVPRW'17: CVPR Workshops.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://people.ee.ethz.ch/~timofter/publications/Timofte-CVPRW-2017.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2016_ccs/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Consistent Coding Scheme for Single-Image Super-Resolution</b></p>
          <p>Wenming Yang, <b>Yapeng Tian</b>, Fei Zhou, Qingmin Liao, Hai Chen, Chenglin Zheng</p>
          <p><i>TMM'16: EEE Trans. Multimedia. (First student author)</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/1MpEM5qmSJi1GtY9TftNNV0cgSHqMT_KQ/view" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2016_anrse/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>Anchored Neighborhood Regression based SISR from Self-examples</b></p>
          <p><b>Yapeng Tian</b>, Fei Zhou, Wenming Yang, Xuesen Shang, Qingmin Liao</p>
          <p><i>ICIP'16: IEEE International Conference on Image Processing.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/1Ts_gYIp57llzK53Wyt7hwu4lZ9GQsbis/view" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/ICIP2016" target="_blank">Code</a>
            

            

            
           </div>
            </div>
          </div>


        </div>
      
        <div class="paper">
          <div class="row">
            <div class="four columns" style="margin-right:-35px;" >
              <div class="figure">
                <img style="position:relative;top:-8px" src="/assets/publications/2015_pfsr/teaser.png" width="220" height="110">
              </div>
            </div>
            <div class="eight columns" style="width:69%;">
          <p class="title"><b>SISR Using Clustering-Based Global Regression and Propagation Filtering</b></p>
          <p>Wenming Yang, <b>Yapeng Tian</b>, Fei Zhou, ..., Qingmin Liao</p>
          <p><i>ACPR'15 Oral: Asian Conference on Pattern Recognition. (First student author)</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/12tqTHt0aLn7-B1jEgEJBYY64uhwfEezA/view" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>
            </div>
          </div>


        </div>
      
    </div> -->


  <div class="tab-content" >
    <div class="tab-pane active" id="papers-all">
      
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_ommsi/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Towards Online Multi-Modal Social Interaction Understanding</b></p>
          <p>Xinpeng Li, Shijian Deng, Bolin Lai, Weiguo Pian, James M. Rehg, <b>Yapeng Tian</b></p>
          <p><i>Preprint'25. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2503.19851" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_iMMLM/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Modality-Inconsistent Continual Learning of Multimodal Large Language Models</b></p>
          <p>Weiguo Pian, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>Preprint'24. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.13050" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2022_avsurvey/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Learning in Audio-visual Context: A Review, Analysis, and New Perspective</b></p>
          <p>Yake Wei, Di Hu, <b>Yapeng Tian</b>, Xuelong Li</p>
          <p><i>Preprint'22. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2208.09579.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://gewu-lab.github.io/audio-visual-learning/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_segbias/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?</b></p>
          <p>Jia Li, Wenjie Zhao, Ziru Huang, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>AAAI'26: Annual AAAI Conference on Artificial Intelligence. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2502.00358" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2026_autismgaze/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Toward Gaze Target Detection of Young Autistic Children</b></p>
          <p>Shijian Deng, Erin E. Kosloski, Siva Sai Nagender Vasireddy, Jia Li, Randi Sierra Sherwood, Feroz Mohamed Hatha, Siddhi Patel, Pamela R Rollins, <b>Yapeng Tian</b></p>
          <p><i>AAAI'26 Oral: AAAI Conference on Artificial Intelligence (Social Impact Track). </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2511.11244" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_AVROBUSTBENCH/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>AVROBUSTBENCH: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time</b></p>
          <p>Sarthak Kumar Maharana, Saksham Singh Kushwaha, Baoming Zhang, Adrian Rodriguez, Songtao Wei, <b>Yapeng Tian</b>, Yunhui Guo</p>
          <p><i>NeurIPS'25: Conference on Neural Information Processing Systems (D&B Track). </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2506.00358" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/sarthaxxxxx/AVROBUSTBENCH" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_dpo/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach</b></p>
          <p>Shijian Deng, Wentian Zhao, Yu-Jhe Li, Kun Wan, Daniel Miranda, Ajinkya Kale, <b>Yapeng Tian</b></p>
          <p><i>COLM'25: Second Conference on Language Modeling. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2411.17760" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_survey/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Self-Improvement in Multimodal Large Language Models: A Survey</b></p>
          <p>Shijian Deng, Kai Wang, Tianyu Yang, Harsh Singh, <b>Yapeng Tian</b></p>
          <p><i>EMNLP'25 Findings: Conference on Empirical Methods in Natural Language Processing. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2510.02665" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_AVDiT/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation</b></p>
          <p>Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, <b>Yapeng Tian</b></p>
          <p><i>ACM MM'25: ACM International Conference on Multimedia. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.07686" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_davis/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling</b></p>
          <p>Chao Huang, Susan Liang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>IJCV'25: International Journal of Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2509.22063" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_vrsight/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People</b></p>
          <p>Daniel Killough, Justin Feng, Zheng Xue Ching, Daniel Wang, Rithvik Dyava, <b>Yapeng Tian</b>, Yuhang Zhao</p>
          <p><i>UIST'25:  ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://dl.acm.org/doi/full/10.1145/3746059.3747641" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_aroma/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>AROMA: Mixed-Initiative AI Assistance for Non-Visual Cooking by Grounding Multi-modal Information Between Reality and Videos</b></p>
          <p>Zheng Ning, Leyang Li, Daniel Killough, JooYoung Seo, Patrick Carrington, <b>Yapeng Tian</b>, Yuhang Zhao, Franklin Mingzhe Li, Toby Jia-Jun Li</p>
          <p><i>UIST'25:  ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2507.10963" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_tpblend/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models</b></p>
          <p>Xin Jin, Yichuan Zhong, <b>Yapeng Tian</b></p>
          <p><i>TMLR'25: Transactions on Machine Learning Research. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/pdf?id=q6M73uOBZE" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_PEAVL/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Prompt Image to Watch and Hear: Multimodal Prompting for Parameter-Efficient Audio-Visual Learning</b></p>
          <p>Kai Wang, Shentong Mo, <b>Yapeng Tian</b>, Dimitrios Hatzinakos</p>
          <p><i>BMVC'25: The British Machine Vision Conference (BMVC). </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_signllm/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Signllm: Sign language production large language models</b></p>
          <p>Sen Fang, Chen Chen, Lei Wang, Ce Zheng, Chunyu Sui, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'25:  IEEE/CVF International Conference on Computer Vision CV4A11y Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/ICCV2025W/CV4A11y/papers/Fang_SignLLM_Sign_Language_Production_Large_Language_Models_ICCVW_2025_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://signllm.github.io/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_cv4a11y/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Introduction to the First Workshop on Vision Foundation Models and Generative AI for Accessibility</b></p>
          <p><b>Yapeng Tian</b>, Yuhang Zhao, Jon E. Froehlich, Chu Li, Yuheng Wu</p>
          <p><i>ICCVW'25:  IEEE/CVF International Conference on Computer Vision CV4A11y Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/ICCV2025W/CV4A11y/papers/Tian_Introduction_to_the_First_Workshop_on_Vision_Foundation_Models_and_ICCVW_2025_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_ZFusion/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>ZFusion: Efficient Deep Compositional Zero-shot Learning for Blind Image Super-Resolution with Generative Diffusion Prior</b></p>
          <p>Alireza Esmaeilzehi, Hossein Zaredar, <b>Yapeng Tian</b>, Laleh Seyyed-Kalantari</p>
          <p><i>ICCV'25: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/2673.png?t=1759954583.3458676" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_prvql/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>PRVQL: Progressive Knowledge-guided Refinement for Robust Egocentric Visual Query Localization</b></p>
          <p>Bing Fan, Yunhe Feng, <b>Yapeng Tian</b>, Yuewei Lin, Yan Huang, Heng Fan</p>
          <p><i>ICCV'25: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2502.07707" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://github.com/fb-reps/PRVQL" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_VinTAGe/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation</b></p>
          <p>Saksham Singh Kushwaha, <b>Yapeng Tian</b></p>
          <p><i>CVPR'25: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.10768" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://sakshamsingh1.github.io/vintage/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_motion/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level</b></p>
          <p>Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, <b>Yapeng Tian</b>, Ajmal Saeed Mian, Mohit Bansal, Chen Chen</p>
          <p><i>CVPR'25: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2411.09921" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://groundmore.github.io/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_Diff-SAGe/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Diff-SAGe: End-to-End Spatial Audio Generation Using Diffusion Models</b></p>
          <p>Saksham Singh Kushwaha, Jianbo Ma, Mark R. P. Thomas,  <b>Yapeng Tian</b>, and Avery Bruni</p>
          <p><i>ICASSP'25: IEEE International Conference on Acoustics, Speech, and Signal Processing. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2410.11299" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_cliperase/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP</b></p>
          <p>Tianyu Yang, Lisen Dai, Zheyuan Liu, Xiangqi Wang, Meng Jiang, <b>Yapeng Tian</b>, Xiangliang Zhang</p>
          <p><i>ACL'25 Main: Annual Meeting of the Association for Computational Linguistics. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.23330" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://tianyu-yang-anna.github.io/ClipErase-ACL/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_magictalk/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>MagicTalk: Implicit and Explicit Correlation Learning for Diffusion-based Emotional Talking Face Generation</b></p>
          <p>Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, <b>Yapeng Tian</b>, Jiashi Feng, Xiaohu Guo</p>
          <p><i>CVM:  Computational Visual Media Journal. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11145205" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://magictalk.github.io/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_chiea/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Demonstration of VRSight: AI-Driven Real-Time Descriptions to Enhance VR Accessibility for Blind People</b></p>
          <p>Daniel Killough, Justin Feng, Rithvik Dyava, Zheng Xue Ching, Daniel Wang, <b>Yapeng Tian</b>, Yuhang Zhao</p>
          <p><i>CHI EA'25: Extended Abstracts of the CHI Conference </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://dl.acm.org/doi/abs/10.1145/3706599.3721194" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_srcld/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Leveraging AI to Assess Social Attention in Young Autistic Children</b></p>
          <p>Erin Kosloski, Shijian Deng, Siva S. N. Vasireddy, Randi S. Sherwood, Feroz M. Hatha, Jia Li, Siddhi Patel, <b>Yapeng Tian</b>, Pamela Rollins</p>
          <p><i>SRCLD'25: Symposium on Research in Child Language Disorders. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.researchgate.net/publication/392469852_Leveraging_Artificial_Intelligence_to_Assess_Social_Attention_in_Young_Autistic_Children" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_signdiff/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>SignDiff: Learning Diffusion Models for American Sign Language Production</b></p>
          <p>Sen Fang, Chunyu Sui, Yanghao Zhou, Xuedong Zhang, Hongbin Zhong, <b>Yapeng Tian</b>, Chen Chen</p>
          <p><i>FGW'25: International Conference on Automatic Face and Gesture Recognition Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.16082.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_eMMLM/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Language-Guided Adaptive Vision Token Pruning for Efficient Multimodal Large Language Models</b></p>
          <p>Omer Faruk Deniz, Tarik Arici, Fatemeh Sheikholeslami, Burak Gozluklu, Ameni Trabelsi, Suleiman Khan, <b>Yapeng Tian</b>, Latifur Khan</p>
          <p><i>PAKDD'25 Oral: The Pacific-Asia Conference on Knowledge Discovery and Data Mining. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://link.springer.com/chapter/10.1007/978-981-96-8186-0_9" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_gesture/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Joint Co-Speech Gesture and Expressive Talking Face Generation using Diffusion with Adapters</b></p>
          <p>Steven Hogue, Chenxu Zhang, <b>Yapeng Tian</b>, Xiaohu Guo</p>
          <p><i>WACV'25: IEEE/CVF Winter Conference on Applications of Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.14333" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2025_diffir/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>DiffIR: Efficient Diffusion Model for Image Restoration</b></p>
          <p>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, <b>Yapeng Tian</b>, Wenming Yang, Radu Timotfe, Luc Van Gool</p>
          <p><i>TPAMI'25: IEEE Transactions on Pattern Analysis and Machine Intelligence. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.13767" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/DiffIR" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_avdd/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Audio-Visual Dataset Distillation</b></p>
          <p>Saksham Singh Kushwaha, Siva Sai Nagender Vasireddy, Kai Wang, <b>Yapeng Tian</b></p>
          <p><i>TMLR'24: Transactions on Machine Learning Research </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/forum?id=IJlbuSrXmk" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_cavss/teaser1.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Continual Audio-Visual Sound Separation</b></p>
          <p>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>NeurIPS'24: The Annual Conference on Neural Information Processing Systems </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Pian_Learning_Continual_Audio-Visual_Sound_Separation_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_avasd/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition</b></p>
          <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
          <p><i>TMM'24: IEEE Transactions on Multimedia. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2406.02554" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://github.com/ShijianDeng/AV-ASD" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_ARSPORT/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span>Towards AI-Powered AR for Enhancing Sports Playability for People with Low Vision: An Exploration of ARSports <span style='color: red;'> (Best Paper Award)</span></b></p>
          <p>Jaewook Lee, Yang Li, Dylan Bunarto, Eujean Lee, Olivia Wang, Adrian Rodriguez, Yuhang Zhao, <b>Yapeng Tian</b>, Jon E. Froehlich</p>
          <p><i>ISMAR IDEATExR'24 : International Symposium on Mixed and Augmented Reality Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://makeabilitylab.cs.washington.edu/media/publications/Lee_TowardsAiPoweredArForEnhancingSportsPlayabilityForPeopleWithLowVisionAnExplorationOfArsports_IDEATExR2024.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_cookar/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span> CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool Interactions for People with Low Vision <span style='color: red;'> (Belonging & Inclusion Best Paper Award)</span></b></p>
          <p>Jaewook Lee, Andrew D. Tjahjadi, Jiho Kim, Junpu Yu, Minji Park, Jiawen Zhang, Jon E. Froehlich, <b>Yapeng Tian</b>, Yuhang Zhao</p>
          <p><i>UIST'24: ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2407.13515" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://github.com/makeabilitylab/CookAR" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_davis/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span> DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models<br><span style='color: red;'> (Best Paper Honorable Mention)</span></b></p>
          <p>Chao Huang, Susan Liang,  <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ACCV'24 Oral: Asian Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2308.00122" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_avedit/OAVE.svg" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Language-Guided Joint Audio-Visual Editing Via One-Shot Adaptation</b></p>
          <p>Susan Liang, Chao Huang,  <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ACCV'24: Asian Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.07463" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_SaSR/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering</b></p>
          <p>Tianyu Yang, Yiyang Nan, Lisen Dai, Zhenwen Liang, <b>Yapeng Tian</b>, Xiangliang Zhang</p>
          <p><i>EMNLP'24: Empirical Methods in Natural Language Processing (Findings) </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2411.04933" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_LFAV/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Towards Long Form Audio-visual Video Understanding</b></p>
          <p>Wenxuan Hou, Guangyao Li, <b>Yapeng Tian</b>, Di Hu</p>
          <p><i>TOMM'24: ACM Trans. on Multimedia Computing, Communications and App. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2306.09431" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://gewu-lab.github.io/LFAV/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_BOFL/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Benchmarking and Optimizing Federated Learning with Hardware-related Metrics</b></p>
          <p>Kai Pan, <b>Yapeng Tian</b>, Yinhe Han, Yiming Gan</p>
          <p><i>BMVC'24: British Machine Vision Conference </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_egovsr/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>EgoVSR: Towards High-Quality Egocentric Video Super-Resolution</b></p>
          <p>Yichen Chi, Junhao Gu, Jiamiao Zhang, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>TCSVT'24: IEEE Transactions on Circuits and Systems for Video Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2305.14708.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_MIMOSA/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>MIMOSA: Human-AI Co-Creation of Computational Spatial Audio Effects on Videos</b></p>
          <p>Zheng Ning, Zheng Zhang, Jerrick Ban, Kaiwen Jiang, Ruohong Gan, <b>Yapeng Tian</b>, Toby Jia-Jun Li</p>
          <p><i>C&C'24: ACM Conference on Creativity & Cognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://dl.acm.org/doi/pdf/10.1145/3635636.3656189" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_avmamba/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>AV-Mamba: Cross-Modality Selective State Space Models for Audio-Visual Question Answering</b></p>
          <p>Ziru Huang, Jia Li, Wenjie Zhao, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Huang_AV-Mamba_Cross-Modality_Selective_State_Space_Models_for_Audio-Visual_Question_Answering.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_cavss/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Learning Continual Audio-Visual Sound Separation Models</b></p>
          <p>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Pian_Learning_Continual_Audio-Visual_Sound_Separation_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_avasd/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Audio-Visual Autism Behavior Recognition with MMLMs</b></p>
          <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Audio-Visual_Autism_Behavior_Recognition_with_Multimodal_Large_Language_Models_Audio-Visual_Autism_Behavior_Recognition_with_Multimodal_Large_Language_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_avdd/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Dataset distillation for audio-visual datasets</b></p>
          <p>Saksham Singh Kushwaha, Siva Sai Nagender Vasireddy, Kai Wang, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Kushwaha_Dataset_distillation_for_audio-visual_datasets.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_tedgen/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures</b></p>
          <p>Steven Hogue, Chenxu Zhang, Hamza Daruger, <b>Yapeng Tian</b>, Xiaohu Guo</p>
          <p><i>CVPRW'24: CVPR HuMoGen Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2024W/HuMoGen/papers/Hogue_DiffTED_One-shot_Audio-driven_TED_Talk_Video_Generation_with_Diffusion-based_Co-speech_CVPRW_2024_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_stgcma/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Towards Efficient Audio-Visual Learners via Empowering Pre-trained Vision Transformers with Cross-Modal Adaptation</b></p>
          <p>Kai Wang, <b>Yapeng Tian</b>, Dimitrios Hatzinakos</p>
          <p><i>CVPRW'24: CVPR Multimodal Foundation Models Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2024W/MMFM/papers/Wang_Towards_Efficient_Audio-Visual_Learners_via_Empowering_Pre-trained_Vision_Transformers_with_CVPRW_2024_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/kaiw7/STG-CMA" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_MAAVT/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>MA-AVT: Modality Alignment for Parameter-Efficient Audio-Visual Transformers</b></p>
          <p>Tanvir Mahmud, Shentong Mo, <b>Yapeng Tian</b>, Diana Marculescu</p>
          <p><i>CVPRW'24: CVPR Efficient Deep Learning for Computer Vision Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.04930" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_TVSL/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>T-VSL: Text-Guided Visual Sound Source Localization in Mixtures</b></p>
          <p>Tanvir Mahmud, <b>Yapeng Tian</b>, Diana Marculescu</p>
          <p><i>CVPR'24: IEEE/CVF Conference on Computer Vision and Pattern Recognition </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2404.01751" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/enyac-group/T-VSL/tree/main" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_oscar/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>OSCaR: Object State Captioning and State Change Representation</b></p>
          <p>Nguyen Nguyen, Jing Bi, Ali Vosoughi, <b>Yapeng Tian</b>, Pooyan Fazli, Chenliang Xu</p>
          <p><i>NAACL'24: The North American Chapter of the Association for Computational Linguistics (Findings) </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2402.17128" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_SPICA/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers</b></p>
          <p>Zheng Ning, Brianna Wimer, Kaiwen Jiang, Keyi Chen, Jerrick Ban, <b>Yapeng Tian</b>, Yuhang Zhao, Toby Li</p>
          <p><i>CHI'24: The ACM Conference on Human Factors in Computing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2402.07300" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_STADNET/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>STADNet: Spatial-Temporal Attention-Guided Dual-Path Network for cardiac cine MRI super-resolution</b></p>
          <p>Jun Lyu, Shuo Wang, <b>Yapeng Tian</b>, Jing Zou, Shunjie Dong, Chengyan Wang, Angelica I Aviles-Rivero, Jing Qin</p>
          <p><i>MIA'24: Medical Image Analysis</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841524000677" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2024_vqa/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Unveiling cross modality bias in visual question answering: A causal view with possible worlds vqa</b></p>
          <p>Ali Vosoughi<sup>‚Ä°</sup>, Shijian Deng<sup>‚Ä°</sup>, Songyang Zhang, <b>Yapeng Tian</b>, Chenliang Xu, Jiebo Luo</p>
          <p><i>TMM'24: IEEE Transactions on Multimedia</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2305.19664" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_mavs/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b> LAVSS: Location-Guided Audio-Visual Spatial Audio Separation</b></p>
          <p>Yuxin Ye, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>WACV'24: Winter Conference on Applications of Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2310.20446.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://yyx666660.github.io/LAVSS/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_dcl/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Disentangled counterfactual learning for physical audiovisual commonsense reasoning</b></p>
          <p>Changsheng Lv, Shuai Zhang, <b>Yapeng Tian</b>, Mengshi Qi, Huadong Ma</p>
          <p><i>NeurIPS'23: The Annual Conference on Neural Information Processing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2310.19559" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Andy20178/DCL" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_avnerf/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>NeurIPS'23: The Annual Conference on Neural Information Processing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2302.02088.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://liangsusan-git.github.io/project/avnerf/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_PEANUT/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>PEANUT: A Human-AI Collaborative Tool for Annotating Audio-Visual Data</b></p>
          <p>Zheng Zhang<sup>‚Ä°</sup>, Zheng Ning<sup>‚Ä°</sup>, Chenliang Xu <b>Yapeng Tian</b>, Toby Jia-Jun Li</p>
          <p><i>UIST'23: ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2307.15167.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_rasd/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b> Towards Robust Active Speaker Detection</b></p>
          <p>Siva Sai Nagender Vasireddy, Chenxu Zhang, Xiaohu Guo, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p5.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_mavs/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b> Position-Aware Audio-Visual Separation for Spatial Audio</b></p>
          <p>Yuxin Ye, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p8.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_mimo/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b> Towards Better Egocentric Action Understanding in a Multi-Input Multi-Output View</b></p>
          <p>Wenxuan Hou, Ruoxuan Feng, Yixin Xu, <b>Yapeng Tian</b>, Di Hu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p13.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_nacf/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b> Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p1.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_invisiblesep/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b> Separating Invisible Sounds Toward Universal Audio-Visual Scene-Aware Sound Separation</b></p>
          <p>Yiyang Su, Ali Vosoughi, Shijian Deng, <b>Yapeng Tian</b>, Chenliang Xu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p3.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_avcil/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b> Audio-Visual Class-Incremental Learning</b></p>
          <p>Weiguo Pian<sup>‚Ä°</sup>, Shentong Mo<sup>‚Ä°</sup>, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.11073.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/weiguoPian/AV-CIL_ICCV2023" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_CIGN/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b> Class-Incremental Grouping Network for Continual Audio-Visual Learning</b></p>
          <p>Shentong Mo<sup>‚Ä°</sup>, Weiguo Pian<sup>‚Ä°</sup>, <b>Yapeng Tian</b></p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2309.05281.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/CIGN" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_diffir/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>DiffIR: Efficient Diffusion Model for Image Restoration</b></p>
          <p>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, <b>Yapeng Tian</b>, Wenming Yang, Luc Van Gool</p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2303.09472" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/DiffIR" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_MCMRI/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</b></p>
          <p>Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>MICCAI'23: Medical Image Computing and Computer-Assisted Intervention. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2307.02334" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/jmzhang79/Dual-ArbNet" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_mrda/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Meta-Learning based Degradation Representation for Blind Super-Resolution</b></p>
          <p>Bin Xia,  <b>Yapeng Tian</b>, Yulun Zhang, Yucheng Hang, Wenming Yang, Qingmin Liao</p>
          <p><i>TIP'23: IEEE Transactions on Image Processing. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2207.13963" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/MRDA" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_avsam/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2305.01836.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_diffava/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment</b></p>
          <p>Shentong Mo, Jing Shi, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2305.12903" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_avnerf/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2302.02088.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://liangsusan-git.github.io/project/avnerf/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_avg/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Audio-Visual Grouping Network for Sound Localization from Mixtures</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2303.17056.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/AVGN" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_egoav/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Egocentric Audio-Visual Object Localization</b></p>
          <p>Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, and Chenliang Xu</p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2303.13471.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/WikiChao/Ego-AV-Loc" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_ssl/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Structured Sparsity Learning for Efficient Video Super-Resolution</b></p>
          <p>Bin Xia, Jingwen He, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Luc Van Gool</p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2206.07687" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/SSL" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_iclrkd/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Knowledge Distillation based Degradation Estimation for Blind Super-Resolution</b></p>
          <p>Bin Xia, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Radu Timofte, Luc Van Gool</p>
          <p><i>ICLR'23: International Conference on Learning Representations.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2211.16928" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/KDSR" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_iclrbbcu/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Basic Binary Convolution Unit for Binarized Image Restoration Network</b></p>
          <p>Bin Xia, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Radu Timofte, Luc Van Gool</p>
          <p><i>ICLR'23: International Conference on Learning Representations.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2210.00405.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/BBCU" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_dan/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Stdan: deformable attention network for space-time video super-resolution</b></p>
          <p>Hai Wang, Xiaoyu Xiang, <b>Yapeng Tian</b>, Wenming Yang, Qingmin Liao</p>
          <p><i>TNNLS'23: IEEE Transactions on Neural Networks and Learning Systems.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/abstract/document/10045744" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/littlewhitesea/STDAN" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_spl/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>GDSSR: Toward Real-World Ultra-High-Resolution Image Super-Resolution</b></p>
          <p>Yichen Chi, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>SPL'23: IEEE Signal Processing Letters.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/abstract/document/10041757" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2023_aaainfh/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Towards Unified, Explainable, and Robust Multisensory Perception</b></p>
          <p>Yapeng Tian</p>
          <p><i>AAAI'23: AAAI Conference on Artificial Intelligence. (NFH program)</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="/assets/publications/2023_aaainfh/paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2022_mgn/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>NeurIPS'22: The Annual Conference on Neural Information Processing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/pdf?id=zfo2LqFEVY" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/MGN" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2022_std/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Learning Spatio-Temporal Downsampling for Effective Video Upscaling</b></p>
          <p>Xiaoyu Xiang, <b>Yapeng Tian</b>, Vijay Rengarajan, Lucas Young, Bo Zhu, Rakesh Ranjan</p>
          <p><i>ECCV'22: European Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136780159.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2022_thesis/thesis.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Audio-Visual Scene Understanding Towards Unified, Explainable, and Robust Multisensory Perception</b></p>
          <p><b>Yapeng Tian</b></p>
          <p><i>PhD Thesis </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.proquest.com/openview/99bef5e8207df0ebab29e6b1f2afbad8/1.pdf?pq-origsite=gscholar&cbl=18750&diss=y" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2022_dudocaf/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>DuDoCAF: Dual-Domain Cross-Attention Fusion with Recurrent Transformer for Fast Multi-contrast MR Imaging</b></p>
          <p>Jun Lyu, Bin Sui, Chengyan Wang, <b>Yapeng Tian</b>, Qi Dou, and Jing Qin</p>
          <p><i>MICCAI'22: Medical Image Computing and Computer Assisted Intervention. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="/assets/publications/2022_dudocaf/paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2022_avol/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Audio-Visual Object Localization in Egocentric Videos</b></p>
          <p>Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, and Chenliang Xu</p>
          <p><i>CVPRW'22: CVPR Workshops</i></p>
          
          <p style="color:red">Egocentric audio-visual learning.</p>
         
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2022_avqa/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</b></p>
          <p>Guangyao Li<sup>‚Ä°</sup>, Yake Wei<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, Chenliang Xu, Ji-Rong Wen, and Di Hu</p>
          <p><i>CVPR'22 Oral: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2203.14072.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=JH3t5gwe9Xw" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/GeWu-Lab/MUSIC-AVQA" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://gewu-lab.github.io/MUSIC-AVQA/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2022_mrisr/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Transformer-empowered Multi-contrast MRI Super-Resolution</b></p>
          <p>Guangyuan Li, Jun Lv, <b>Yapeng Tian</b>, Qi Dou, Chengyan Wang, Chenliang Xu, Jing Qin</p>
          <p><i>CVPR'22: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2203.13963.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/XAIMI-Lab/McMRSR" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2022_amsa/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution</b></p>
          <p>Bin Xia, <b>Yapeng Tian</b>, Yucheng Hang, Wenming Yang, Qingmin Liao, Jie Zhou</p>
          <p><i>AAAI'22: The AAAI Conference on Artificial Intelligence.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2201.04358.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/AMSA" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2022_enlca/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Efficient Non-Local Contrastive Attention for Image Super-Resolution</b></p>
          <p>Bin Xia<sup>‚Ä°</sup>, Yucheng Hang<sup>‚Ä°</sup>, <b>Yapeng Tian</b>, Wenming Yang, Qingmin Liao, Jie Zhou</p>
          <p><i>AAAI'22: The AAAI Conference on Artificial Intelligence. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2201.03794.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/ENLCA" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2021_stm/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Space-Time Memory Network for Sounding Object Localization in Videos</b></p>
          <p>Sizhe Li<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, and Chenliang Xu</p>
          <p><i>BMVC'21: The British Machine Vision Conference. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2111.05526.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/lester0866/Space-Time-Memory-Network-for-Sounding-Object-Localization-in-Videos" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://sites.google.com/view/bmvc2021stm" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2021_matting/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Video Matting via Consistency-Regularized Graph Neural Networks</b></p>
          <p>Tiantian Wang, Sifei Liu, <b>Yapeng Tian</b>, Kai Li, and Ming-Hsuan Yang</p>
          <p><i>ICCV'21: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://faculty.ucmerced.edu/mhyang/papers/iccv2021_video_matting.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/TiantianWang/VideoMatting-CRGNN" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2021_robustness/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Can audio-visual integration strengthen robustness under multimodal attacks?</b></p>
          <p><b>Yapeng Tian</b> and Chenliang Xu</p>
          <p><i>CVPR'21: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2104.02000.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/AV-Robustness-CVPR21" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2021_ccol/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation</b></p>
          <p><b>Yapeng Tian</b>, Di Hu, and Chenliang Xu</p>
          <p><i>CVPR'21: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2104.02026.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/CCOL-CVPR21" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2020_avvp/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing</b></p>
          <p><b>Yapeng Tian</b>, Dingzeyu Li, and Chenliang Xu</p>
          <p><i>ECCV'20 Spotlight: European Conference on Computer Vision.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2007.10558.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/AVVP-ECCV20" target="_blank">Code</a>
            

            
              <a class="button" href="https://github.com/YapengTian/AVVP-ECCV20" target="_blank">Data</a>
            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2020_zsm/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution</b></p>
          <p>Xiaoyu Xiang<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, Yulun Zhang, Yun Fu, Jan Allebach, and Chenliang Xu</p>
          <p><i>CVPR'20: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2002.11616.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=8mgD8JxBOus" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2020_tdan/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>TDAN: Temporally Deformable Alignment Network for Video Super-Resolution</b></p>
          <p><b>Yapeng Tian</b>, Yulun Zhang, Yun Fu, and Chenliang Xu</p>
          <p><i>CVPR'20: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
          
          <p style="color:red">This is the first work that uses deformable alignment to address video restoration.</p>
         
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Tian_TDAN_Temporally-Deformable_Alignment_Network_for_Video_Super-Resolution_CVPR_2020_paper.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=eZExENE50I0" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/YapengTian/TDAN-VSR-CVPR-2020" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2020_dap/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Deep Audio Prior</b></p>
          <p><b>Yapeng Tian</b>, Chenliang Xu, and Dingzeyu Li</p>
          <p><i>CVPRW'20: CVPR Workshops.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1912.10292v1.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/adobe/Deep-Audio-Prior" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://opensource.adobe.com/Deep-Audio-Prior/" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2020_rdnpami/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Residual Dense Network for Image Super-Resolution</b></p>
          <p>Yulun Zhang, <b>Yapeng Tian</b>, Yu Kong , Bineng Zhong, Yun Fu</p>
          <p><i>TPAMI'20: IEEE Transactions on Pattern Analysis and Machine Intelligence.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1812.10477.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/yulunzhang/RDN" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2019_csf/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>CFSNet: Toward a Controllable Feature Space for Image Restoration</b></p>
          <p>Wei Wang<sup>‚Ä°</sup>, Ruiming Guo<sup>‚Ä°</sup>, <b>Yapeng Tian</b>, and Wenming Yang</p>
          <p><i>ICCV'19: IEEE/CVF International Conference on Computer Vision.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1904.00634.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/qibao77/CFSNet" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2019_avc/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Interpretable and Controllable Audio-Visual Video Captioning</b></p>
          <p><b>Yapeng Tian</b>, Chenxiao Guan, Goodman Justin, Marc Moore, and Chenliang Xu</p>
          <p><i>CVPRW'19: CVPR Workshops.</i></p>
          
          <p style="color:red">Multisensory interpretability in terms of the audio-visual video captioning task.</p>
         
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Yapeng_Tian_Audio-Visual_Interpretable_and_Controllable_Video_Captioning_CVPRW_2019_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2019_lcsc/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>LCSCNet: Linear Compressing Based Skip-Connecting Network for ISR</b></p>
          <p>Wenming Yang, Xuechen Zhang, <b>Yapeng Tian</b>, Wei Wang, Jing-Hao Xue, Qingmin Liao</p>
          <p><i>TIP'19: IEEE Trans. Image Processing.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/1909.03573" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2019_review/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Deep Learning for Single Image Super-Resolution: A Brief Review</b></p>
          <p>Wenming Yang, Xuechen Zhang, <b>Yapeng Tian</b>, Wei Wang, JingHao Xue, Qingmin Liao</p>
          <p><i>TMM'19: IEEE Trans. Multimedia.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1808.03344.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2018_ave/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Audio-Visual Event Localization in Unconstrained Videos</b></p>
          <p><b>Yapeng Tian</b>, Jing Shi, Bochen Li, Zhiyao Duan, Chenliang Xu</p>
          <p><i>ECCV'18: European Conference on Computer Vision.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=m6r6BbD5MSc" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/YapengTian/AVE-ECCV18" target="_blank">Code</a>
            

            
              <a class="button" href="https://drive.google.com/file/d/1FjKwe79e0u96vdjIVwfRQ1V6SoDHe7kK" target="_blank">Data</a>
            

            
              <a class="button" href="https://sites.google.com/view/audiovisualresearch" target="_blank">Project</a>
            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2018_rdn/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Residual Dense Network for Image Super-Resolution</b></p>
          <p>Yulun Zhang, <b>Yapeng Tian</b>, Yu Kong , Bineng Zhong, Yun Fu</p>
          <p><i>CVPR'18 Spotlight: IEEE/CVF Conf. on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1802.08797.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/yulunzhang/RDN" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2017_ntire/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results</b></p>
          <p>Timofte et al.</p>
          <p><i>CVPRW'17: CVPR Workshops.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://people.ee.ethz.ch/~timofter/publications/Timofte-CVPRW-2017.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2016_ccs/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Consistent Coding Scheme for Single-Image Super-Resolution</b></p>
          <p>Wenming Yang, <b>Yapeng Tian</b>, Fei Zhou, Qingmin Liao, Hai Chen, Chenglin Zheng</p>
          <p><i>TMM'16: EEE Trans. Multimedia. (First student author)</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/1MpEM5qmSJi1GtY9TftNNV0cgSHqMT_KQ/view" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2016_anrse/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>Anchored Neighborhood Regression based SISR from Self-examples</b></p>
          <p><b>Yapeng Tian</b>, Fei Zhou, Wenming Yang, Xuesen Shang, Qingmin Liao</p>
          <p><i>ICIP'16: IEEE International Conference on Image Processing.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/1Ts_gYIp57llzK53Wyt7hwu4lZ9GQsbis/view" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/ICIP2016" target="_blank">Code</a>
            

            

            
           </div>


        </div>
          </div>
      
    
        <div style="min-height: 160px;">
          <div class="figure">
            <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" src="/assets/publications/2015_pfsr/teaser.png" width="240" height="120" /> 
          </div>
        <div class="paper">
          <p class="title"><b>SISR Using Clustering-Based Global Regression and Propagation Filtering</b></p>
          <p>Wenming Yang, <b>Yapeng Tian</b>, Fei Zhou, ..., Qingmin Liao</p>
          <p><i>ACPR'15 Oral: Asian Conference on Pattern Recognition. (First student author)</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/12tqTHt0aLn7-B1jEgEJBYY64uhwfEezA/view" target="_blank">Paper</a>
            

            

            

            

            

            

            
           </div>


        </div>
          </div>
      
    </div>


    <div class="tab-pane" id="papers-arxiv">
      
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_ommsi/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Towards Online Multi-Modal Social Interaction Understanding</b></p>
        <p>Xinpeng Li, Shijian Deng, Bolin Lai, Weiguo Pian, James M. Rehg, <b>Yapeng Tian</b></p>
        <p><i>Preprint'25. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2503.19851" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_iMMLM/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Modality-Inconsistent Continual Learning of Multimodal Large Language Models</b></p>
        <p>Weiguo Pian, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
        <p><i>Preprint'24. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2412.13050" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_avsurvey/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Learning in Audio-visual Context: A Review, Analysis, and New Perspective</b></p>
        <p>Yake Wei, Di Hu, <b>Yapeng Tian</b>, Xuelong Li</p>
        <p><i>Preprint'22. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2208.09579.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://gewu-lab.github.io/audio-visual-learning/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
    </div>


    <div class="tab-pane" id="papers-audiovisual">
      
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_ommsi/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Towards Online Multi-Modal Social Interaction Understanding</b></p>
        <p>Xinpeng Li, Shijian Deng, Bolin Lai, Weiguo Pian, James M. Rehg, <b>Yapeng Tian</b></p>
        <p><i>Preprint'25. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2503.19851" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_iMMLM/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Modality-Inconsistent Continual Learning of Multimodal Large Language Models</b></p>
        <p>Weiguo Pian, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
        <p><i>Preprint'24. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2412.13050" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_avsurvey/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Learning in Audio-visual Context: A Review, Analysis, and New Perspective</b></p>
        <p>Yake Wei, Di Hu, <b>Yapeng Tian</b>, Xuelong Li</p>
        <p><i>Preprint'22. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2208.09579.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://gewu-lab.github.io/audio-visual-learning/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_segbias/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?</b></p>
        <p>Jia Li, Wenjie Zhao, Ziru Huang, Yunhui Guo, <b>Yapeng Tian</b></p>
        <p><i>AAAI'26: Annual AAAI Conference on Artificial Intelligence. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2502.00358" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_AVROBUSTBENCH/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>AVROBUSTBENCH: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time</b></p>
        <p>Sarthak Kumar Maharana, Saksham Singh Kushwaha, Baoming Zhang, Adrian Rodriguez, Songtao Wei, <b>Yapeng Tian</b>, Yunhui Guo</p>
        <p><i>NeurIPS'25: Conference on Neural Information Processing Systems (D&B Track). </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2506.00358" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/sarthaxxxxx/AVROBUSTBENCH" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_AVDiT/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation</b></p>
        <p>Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, <b>Yapeng Tian</b></p>
        <p><i>ACM MM'25: ACM International Conference on Multimedia. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2406.07686" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_davis/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling</b></p>
        <p>Chao Huang, Susan Liang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
        <p><i>IJCV'25: International Journal of Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2509.22063" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_PEAVL/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Prompt Image to Watch and Hear: Multimodal Prompting for Parameter-Efficient Audio-Visual Learning</b></p>
        <p>Kai Wang, Shentong Mo, <b>Yapeng Tian</b>, Dimitrios Hatzinakos</p>
        <p><i>BMVC'25: The British Machine Vision Conference (BMVC). </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_VinTAGe/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation</b></p>
        <p>Saksham Singh Kushwaha, <b>Yapeng Tian</b></p>
        <p><i>CVPR'25: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2412.10768" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://sakshamsingh1.github.io/vintage/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_Diff-SAGe/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Diff-SAGe: End-to-End Spatial Audio Generation Using Diffusion Models</b></p>
        <p>Saksham Singh Kushwaha, Jianbo Ma, Mark R. P. Thomas,  <b>Yapeng Tian</b>, and Avery Bruni</p>
        <p><i>ICASSP'25: IEEE International Conference on Acoustics, Speech, and Signal Processing. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2410.11299" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_magictalk/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>MagicTalk: Implicit and Explicit Correlation Learning for Diffusion-based Emotional Talking Face Generation</b></p>
        <p>Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, <b>Yapeng Tian</b>, Jiashi Feng, Xiaohu Guo</p>
        <p><i>CVM:  Computational Visual Media Journal. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11145205" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://magictalk.github.io/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_gesture/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Joint Co-Speech Gesture and Expressive Talking Face Generation using Diffusion with Adapters</b></p>
        <p>Steven Hogue, Chenxu Zhang, <b>Yapeng Tian</b>, Xiaohu Guo</p>
        <p><i>WACV'25: IEEE/CVF Winter Conference on Applications of Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2412.14333" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_avdd/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Audio-Visual Dataset Distillation</b></p>
        <p>Saksham Singh Kushwaha, Siva Sai Nagender Vasireddy, Kai Wang, <b>Yapeng Tian</b></p>
        <p><i>TMLR'24: Transactions on Machine Learning Research </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://openreview.net/forum?id=IJlbuSrXmk" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_cavss/teaser1.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Continual Audio-Visual Sound Separation</b></p>
        <p>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
        <p><i>NeurIPS'24: The Annual Conference on Neural Information Processing Systems </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://sightsound.org/papers/2024/Pian_Learning_Continual_Audio-Visual_Sound_Separation_Models.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_avasd/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition</b></p>
        <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
        <p><i>TMM'24: IEEE Transactions on Multimedia. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2406.02554" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://github.com/ShijianDeng/AV-ASD" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_davis/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span> DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models<br><span style='color: red;'> (Best Paper Honorable Mention)</span></b></p>
        <p>Chao Huang, Susan Liang,  <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
        <p><i>ACCV'24 Oral: Asian Conference on Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2308.00122" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_avedit/OAVE.svg" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Language-Guided Joint Audio-Visual Editing Via One-Shot Adaptation</b></p>
        <p>Susan Liang, Chao Huang,  <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
        <p><i>ACCV'24: Asian Conference on Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2410.07463" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_SaSR/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering</b></p>
        <p>Tianyu Yang, Yiyang Nan, Lisen Dai, Zhenwen Liang, <b>Yapeng Tian</b>, Xiangliang Zhang</p>
        <p><i>EMNLP'24: Empirical Methods in Natural Language Processing (Findings) </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2411.04933" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_LFAV/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Towards Long Form Audio-visual Video Understanding</b></p>
        <p>Wenxuan Hou, Guangyao Li, <b>Yapeng Tian</b>, Di Hu</p>
        <p><i>TOMM'24: ACM Trans. on Multimedia Computing, Communications and App. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2306.09431" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://gewu-lab.github.io/LFAV/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_MIMOSA/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>MIMOSA: Human-AI Co-Creation of Computational Spatial Audio Effects on Videos</b></p>
        <p>Zheng Ning, Zheng Zhang, Jerrick Ban, Kaiwen Jiang, Ruohong Gan, <b>Yapeng Tian</b>, Toby Jia-Jun Li</p>
        <p><i>C&C'24: ACM Conference on Creativity & Cognition. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://dl.acm.org/doi/pdf/10.1145/3635636.3656189" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_avmamba/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>AV-Mamba: Cross-Modality Selective State Space Models for Audio-Visual Question Answering</b></p>
        <p>Ziru Huang, Jia Li, Wenjie Zhao, Yunhui Guo, <b>Yapeng Tian</b></p>
        <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://sightsound.org/papers/2024/Huang_AV-Mamba_Cross-Modality_Selective_State_Space_Models_for_Audio-Visual_Question_Answering.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_cavss/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Learning Continual Audio-Visual Sound Separation Models</b></p>
        <p>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
        <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://sightsound.org/papers/2024/Pian_Learning_Continual_Audio-Visual_Sound_Separation_Models.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_avasd/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Audio-Visual Autism Behavior Recognition with MMLMs</b></p>
        <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
        <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://sightsound.org/papers/2024/Audio-Visual_Autism_Behavior_Recognition_with_Multimodal_Large_Language_Models_Audio-Visual_Autism_Behavior_Recognition_with_Multimodal_Large_Language_Models.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_avdd/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Dataset distillation for audio-visual datasets</b></p>
        <p>Saksham Singh Kushwaha, Siva Sai Nagender Vasireddy, Kai Wang, <b>Yapeng Tian</b></p>
        <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://sightsound.org/papers/2024/Kushwaha_Dataset_distillation_for_audio-visual_datasets.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_tedgen/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures</b></p>
        <p>Steven Hogue, Chenxu Zhang, Hamza Daruger, <b>Yapeng Tian</b>, Xiaohu Guo</p>
        <p><i>CVPRW'24: CVPR HuMoGen Workshop </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://openaccess.thecvf.com/content/CVPR2024W/HuMoGen/papers/Hogue_DiffTED_One-shot_Audio-driven_TED_Talk_Video_Generation_with_Diffusion-based_Co-speech_CVPRW_2024_paper.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_stgcma/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Towards Efficient Audio-Visual Learners via Empowering Pre-trained Vision Transformers with Cross-Modal Adaptation</b></p>
        <p>Kai Wang, <b>Yapeng Tian</b>, Dimitrios Hatzinakos</p>
        <p><i>CVPRW'24: CVPR Multimodal Foundation Models Workshop </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://openaccess.thecvf.com/content/CVPR2024W/MMFM/papers/Wang_Towards_Efficient_Audio-Visual_Learners_via_Empowering_Pre-trained_Vision_Transformers_with_CVPRW_2024_paper.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/kaiw7/STG-CMA" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_MAAVT/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>MA-AVT: Modality Alignment for Parameter-Efficient Audio-Visual Transformers</b></p>
        <p>Tanvir Mahmud, Shentong Mo, <b>Yapeng Tian</b>, Diana Marculescu</p>
        <p><i>CVPRW'24: CVPR Efficient Deep Learning for Computer Vision Workshop </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2406.04930" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_TVSL/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>T-VSL: Text-Guided Visual Sound Source Localization in Mixtures</b></p>
        <p>Tanvir Mahmud, <b>Yapeng Tian</b>, Diana Marculescu</p>
        <p><i>CVPR'24: IEEE/CVF Conference on Computer Vision and Pattern Recognition </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2404.01751" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/enyac-group/T-VSL/tree/main" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_SPICA/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers</b></p>
        <p>Zheng Ning, Brianna Wimer, Kaiwen Jiang, Keyi Chen, Jerrick Ban, <b>Yapeng Tian</b>, Yuhang Zhao, Toby Li</p>
        <p><i>CHI'24: The ACM Conference on Human Factors in Computing Systems. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2402.07300" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_mavs/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b> LAVSS: Location-Guided Audio-Visual Spatial Audio Separation</b></p>
        <p>Yuxin Ye, Wenming Yang, <b>Yapeng Tian</b></p>
        <p><i>WACV'24: Winter Conference on Applications of Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2310.20446.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://yyx666660.github.io/LAVSS/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_dcl/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Disentangled counterfactual learning for physical audiovisual commonsense reasoning</b></p>
        <p>Changsheng Lv, Shuai Zhang, <b>Yapeng Tian</b>, Mengshi Qi, Huadong Ma</p>
        <p><i>NeurIPS'23: The Annual Conference on Neural Information Processing Systems. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2310.19559" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/Andy20178/DCL" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_avnerf/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</b></p>
        <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
        <p><i>NeurIPS'23: The Annual Conference on Neural Information Processing Systems. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2302.02088.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://liangsusan-git.github.io/project/avnerf/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_PEANUT/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>PEANUT: A Human-AI Collaborative Tool for Annotating Audio-Visual Data</b></p>
        <p>Zheng Zhang<sup>‚Ä°</sup>, Zheng Ning<sup>‚Ä°</sup>, Chenliang Xu <b>Yapeng Tian</b>, Toby Jia-Jun Li</p>
        <p><i>UIST'23: ACM Symposium on User Interface Software and Technology. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2307.15167.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_rasd/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b> Towards Robust Active Speaker Detection</b></p>
        <p>Siva Sai Nagender Vasireddy, Chenxu Zhang, Xiaohu Guo, <b>Yapeng Tian</b></p>
        <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://av4d.org/papers/iccv23/p5.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_mavs/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b> Position-Aware Audio-Visual Separation for Spatial Audio</b></p>
        <p>Yuxin Ye, Wenming Yang, <b>Yapeng Tian</b></p>
        <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://av4d.org/papers/iccv23/p8.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_mimo/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b> Towards Better Egocentric Action Understanding in a Multi-Input Multi-Output View</b></p>
        <p>Wenxuan Hou, Ruoxuan Feng, Yixin Xu, <b>Yapeng Tian</b>, Di Hu</p>
        <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://av4d.org/papers/iccv23/p13.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_nacf/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b> Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields</b></p>
        <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
        <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://av4d.org/papers/iccv23/p1.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_invisiblesep/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b> Separating Invisible Sounds Toward Universal Audio-Visual Scene-Aware Sound Separation</b></p>
        <p>Yiyang Su, Ali Vosoughi, Shijian Deng, <b>Yapeng Tian</b>, Chenliang Xu</p>
        <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://av4d.org/papers/iccv23/p3.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_avcil/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b> Audio-Visual Class-Incremental Learning</b></p>
        <p>Weiguo Pian<sup>‚Ä°</sup>, Shentong Mo<sup>‚Ä°</sup>, Yunhui Guo, <b>Yapeng Tian</b></p>
        <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2308.11073.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/weiguoPian/AV-CIL_ICCV2023" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_CIGN/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b> Class-Incremental Grouping Network for Continual Audio-Visual Learning</b></p>
        <p>Shentong Mo<sup>‚Ä°</sup>, Weiguo Pian<sup>‚Ä°</sup>, <b>Yapeng Tian</b></p>
        <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2309.05281.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/stoneMo/CIGN" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_avsam/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation</b></p>
        <p>Shentong Mo, <b>Yapeng Tian</b></p>
        <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2305.01836.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_diffava/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment</b></p>
        <p>Shentong Mo, Jing Shi, <b>Yapeng Tian</b></p>
        <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2305.12903" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_avnerf/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</b></p>
        <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
        <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2302.02088.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://liangsusan-git.github.io/project/avnerf/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_avg/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Audio-Visual Grouping Network for Sound Localization from Mixtures</b></p>
        <p>Shentong Mo, <b>Yapeng Tian</b></p>
        <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2303.17056.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/stoneMo/AVGN" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_egoav/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Egocentric Audio-Visual Object Localization</b></p>
        <p>Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, and Chenliang Xu</p>
        <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2303.13471.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/WikiChao/Ego-AV-Loc" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_aaainfh/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Towards Unified, Explainable, and Robust Multisensory Perception</b></p>
        <p>Yapeng Tian</p>
        <p><i>AAAI'23: AAAI Conference on Artificial Intelligence. (NFH program)</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="/assets/publications/2023_aaainfh/paper.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_mgn/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing</b></p>
        <p>Shentong Mo, <b>Yapeng Tian</b></p>
        <p><i>NeurIPS'22: The Annual Conference on Neural Information Processing Systems. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://openreview.net/pdf?id=zfo2LqFEVY" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/stoneMo/MGN" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_thesis/thesis.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Audio-Visual Scene Understanding Towards Unified, Explainable, and Robust Multisensory Perception</b></p>
        <p><b>Yapeng Tian</b></p>
        <p><i>PhD Thesis </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://www.proquest.com/openview/99bef5e8207df0ebab29e6b1f2afbad8/1.pdf?pq-origsite=gscholar&cbl=18750&diss=y" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_avol/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Audio-Visual Object Localization in Egocentric Videos</b></p>
        <p>Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, and Chenliang Xu</p>
        <p><i>CVPRW'22: CVPR Workshops</i></p>
        
        <p style="color:red">Egocentric audio-visual learning.</p>
       
         <div class="paper-buttons">
          
            <a class="button" href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_avqa/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</b></p>
        <p>Guangyao Li<sup>‚Ä°</sup>, Yake Wei<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, Chenliang Xu, Ji-Rong Wen, and Di Hu</p>
        <p><i>CVPR'22 Oral: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2203.14072.pdf" target="_blank">Paper</a>
          

          

          

          
            <a class="button" href="https://www.youtube.com/watch?v=JH3t5gwe9Xw" target="_blank">Video</a>
          

          
            <a class="button" href="https://github.com/GeWu-Lab/MUSIC-AVQA" target="_blank">Code</a>
          

          

          
            <a class="button" href="https://gewu-lab.github.io/MUSIC-AVQA/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2021_stm/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Space-Time Memory Network for Sounding Object Localization in Videos</b></p>
        <p>Sizhe Li<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, and Chenliang Xu</p>
        <p><i>BMVC'21: The British Machine Vision Conference. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2111.05526.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/lester0866/Space-Time-Memory-Network-for-Sounding-Object-Localization-in-Videos" target="_blank">Code</a>
          

          

          
            <a class="button" href="https://sites.google.com/view/bmvc2021stm" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2021_robustness/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Can audio-visual integration strengthen robustness under multimodal attacks?</b></p>
        <p><b>Yapeng Tian</b> and Chenliang Xu</p>
        <p><i>CVPR'21: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2104.02000.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/YapengTian/AV-Robustness-CVPR21" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2021_ccol/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation</b></p>
        <p><b>Yapeng Tian</b>, Di Hu, and Chenliang Xu</p>
        <p><i>CVPR'21: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2104.02026.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/YapengTian/CCOL-CVPR21" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2020_avvp/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing</b></p>
        <p><b>Yapeng Tian</b>, Dingzeyu Li, and Chenliang Xu</p>
        <p><i>ECCV'20 Spotlight: European Conference on Computer Vision.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2007.10558.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/YapengTian/AVVP-ECCV20" target="_blank">Code</a>
          

          
            <a class="button" href="https://github.com/YapengTian/AVVP-ECCV20" target="_blank">Data</a>
          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2020_dap/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Deep Audio Prior</b></p>
        <p><b>Yapeng Tian</b>, Chenliang Xu, and Dingzeyu Li</p>
        <p><i>CVPRW'20: CVPR Workshops.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/1912.10292v1.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/adobe/Deep-Audio-Prior" target="_blank">Code</a>
          

          

          
            <a class="button" href="https://opensource.adobe.com/Deep-Audio-Prior/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2019_avc/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Interpretable and Controllable Audio-Visual Video Captioning</b></p>
        <p><b>Yapeng Tian</b>, Chenxiao Guan, Goodman Justin, Marc Moore, and Chenliang Xu</p>
        <p><i>CVPRW'19: CVPR Workshops.</i></p>
        
        <p style="color:red">Multisensory interpretability in terms of the audio-visual video captioning task.</p>
       
         <div class="paper-buttons">
          
            <a class="button" href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Yapeng_Tian_Audio-Visual_Interpretable_and_Controllable_Video_Captioning_CVPRW_2019_paper.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2018_ave/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Audio-Visual Event Localization in Unconstrained Videos</b></p>
        <p><b>Yapeng Tian</b>, Jing Shi, Bochen Li, Zhiyao Duan, Chenliang Xu</p>
        <p><i>ECCV'18: European Conference on Computer Vision.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf" target="_blank">Paper</a>
          

          

          

          
            <a class="button" href="https://www.youtube.com/watch?v=m6r6BbD5MSc" target="_blank">Video</a>
          

          
            <a class="button" href="https://github.com/YapengTian/AVE-ECCV18" target="_blank">Code</a>
          

          
            <a class="button" href="https://drive.google.com/file/d/1FjKwe79e0u96vdjIVwfRQ1V6SoDHe7kK" target="_blank">Data</a>
          

          
            <a class="button" href="https://sites.google.com/view/audiovisualresearch" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
    </div>


        <div class="tab-pane" id="papers-accessibility">
      
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2026_autismgaze/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Toward Gaze Target Detection of Young Autistic Children</b></p>
        <p>Shijian Deng, Erin E. Kosloski, Siva Sai Nagender Vasireddy, Jia Li, Randi Sierra Sherwood, Feroz Mohamed Hatha, Siddhi Patel, Pamela R Rollins, <b>Yapeng Tian</b></p>
        <p><i>AAAI'26 Oral: AAAI Conference on Artificial Intelligence (Social Impact Track). </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2511.11244" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_vrsight/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People</b></p>
        <p>Daniel Killough, Justin Feng, Zheng Xue Ching, Daniel Wang, Rithvik Dyava, <b>Yapeng Tian</b>, Yuhang Zhao</p>
        <p><i>UIST'25:  ACM Symposium on User Interface Software and Technology. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://dl.acm.org/doi/full/10.1145/3746059.3747641" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_aroma/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>AROMA: Mixed-Initiative AI Assistance for Non-Visual Cooking by Grounding Multi-modal Information Between Reality and Videos</b></p>
        <p>Zheng Ning, Leyang Li, Daniel Killough, JooYoung Seo, Patrick Carrington, <b>Yapeng Tian</b>, Yuhang Zhao, Franklin Mingzhe Li, Toby Jia-Jun Li</p>
        <p><i>UIST'25:  ACM Symposium on User Interface Software and Technology. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2507.10963" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_signllm/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Signllm: Sign language production large language models</b></p>
        <p>Sen Fang, Chen Chen, Lei Wang, Ce Zheng, Chunyu Sui, <b>Yapeng Tian</b></p>
        <p><i>ICCVW'25:  IEEE/CVF International Conference on Computer Vision CV4A11y Workshop. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://openaccess.thecvf.com/content/ICCV2025W/CV4A11y/papers/Fang_SignLLM_Sign_Language_Production_Large_Language_Models_ICCVW_2025_paper.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://signllm.github.io/" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_cv4a11y/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Introduction to the First Workshop on Vision Foundation Models and Generative AI for Accessibility</b></p>
        <p><b>Yapeng Tian</b>, Yuhang Zhao, Jon E. Froehlich, Chu Li, Yuheng Wu</p>
        <p><i>ICCVW'25:  IEEE/CVF International Conference on Computer Vision CV4A11y Workshop. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://openaccess.thecvf.com/content/ICCV2025W/CV4A11y/papers/Tian_Introduction_to_the_First_Workshop_on_Vision_Foundation_Models_and_ICCVW_2025_paper.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_chiea/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Demonstration of VRSight: AI-Driven Real-Time Descriptions to Enhance VR Accessibility for Blind People</b></p>
        <p>Daniel Killough, Justin Feng, Rithvik Dyava, Zheng Xue Ching, Daniel Wang, <b>Yapeng Tian</b>, Yuhang Zhao</p>
        <p><i>CHI EA'25: Extended Abstracts of the CHI Conference </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://dl.acm.org/doi/abs/10.1145/3706599.3721194" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_srcld/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Leveraging AI to Assess Social Attention in Young Autistic Children</b></p>
        <p>Erin Kosloski, Shijian Deng, Siva S. N. Vasireddy, Randi S. Sherwood, Feroz M. Hatha, Jia Li, Siddhi Patel, <b>Yapeng Tian</b>, Pamela Rollins</p>
        <p><i>SRCLD'25: Symposium on Research in Child Language Disorders. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://www.researchgate.net/publication/392469852_Leveraging_Artificial_Intelligence_to_Assess_Social_Attention_in_Young_Autistic_Children" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_signdiff/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>SignDiff: Learning Diffusion Models for American Sign Language Production</b></p>
        <p>Sen Fang, Chunyu Sui, Yanghao Zhou, Xuedong Zhang, Hongbin Zhong, <b>Yapeng Tian</b>, Chen Chen</p>
        <p><i>FGW'25: International Conference on Automatic Face and Gesture Recognition Workshop. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2308.16082.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_avasd/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition</b></p>
        <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
        <p><i>TMM'24: IEEE Transactions on Multimedia. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2406.02554" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://github.com/ShijianDeng/AV-ASD" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_ARSPORT/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span>Towards AI-Powered AR for Enhancing Sports Playability for People with Low Vision: An Exploration of ARSports <span style='color: red;'> (Best Paper Award)</span></b></p>
        <p>Jaewook Lee, Yang Li, Dylan Bunarto, Eujean Lee, Olivia Wang, Adrian Rodriguez, Yuhang Zhao, <b>Yapeng Tian</b>, Jon E. Froehlich</p>
        <p><i>ISMAR IDEATExR'24 : International Symposium on Mixed and Augmented Reality Workshop. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://makeabilitylab.cs.washington.edu/media/publications/Lee_TowardsAiPoweredArForEnhancingSportsPlayabilityForPeopleWithLowVisionAnExplorationOfArsports_IDEATExR2024.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_cookar/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span> CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool Interactions for People with Low Vision <span style='color: red;'> (Belonging & Inclusion Best Paper Award)</span></b></p>
        <p>Jaewook Lee, Andrew D. Tjahjadi, Jiho Kim, Junpu Yu, Minji Park, Jiawen Zhang, Jon E. Froehlich, <b>Yapeng Tian</b>, Yuhang Zhao</p>
        <p><i>UIST'24: ACM Symposium on User Interface Software and Technology. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2407.13515" target="_blank">Paper</a>
          

          

          

          

          

          

          
            <a class="button" href="https://github.com/makeabilitylab/CookAR" target="_blank">Project</a>
          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_SPICA/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers</b></p>
        <p>Zheng Ning, Brianna Wimer, Kaiwen Jiang, Keyi Chen, Jerrick Ban, <b>Yapeng Tian</b>, Yuhang Zhao, Toby Li</p>
        <p><i>CHI'24: The ACM Conference on Human Factors in Computing Systems. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2402.07300" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
    </div>



    <div class="tab-pane" id="papers-videorestoration">
      
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_egovsr/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>EgoVSR: Towards High-Quality Egocentric Video Super-Resolution</b></p>
        <p>Yichen Chi, Junhao Gu, Jiamiao Zhang, Wenming Yang, <b>Yapeng Tian</b></p>
        <p><i>TCSVT'24: IEEE Transactions on Circuits and Systems for Video Technology. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2305.14708.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2024_STADNET/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>STADNet: Spatial-Temporal Attention-Guided Dual-Path Network for cardiac cine MRI super-resolution</b></p>
        <p>Jun Lyu, Shuo Wang, <b>Yapeng Tian</b>, Jing Zou, Shunjie Dong, Chengyan Wang, Angelica I Aviles-Rivero, Jing Qin</p>
        <p><i>MIA'24: Medical Image Analysis</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841524000677" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_ssl/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Structured Sparsity Learning for Efficient Video Super-Resolution</b></p>
        <p>Bin Xia, Jingwen He, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Luc Van Gool</p>
        <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2206.07687" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/Zj-BinXia/SSL" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_dan/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Stdan: deformable attention network for space-time video super-resolution</b></p>
        <p>Hai Wang, Xiaoyu Xiang, <b>Yapeng Tian</b>, Wenming Yang, Qingmin Liao</p>
        <p><i>TNNLS'23: IEEE Transactions on Neural Networks and Learning Systems.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://ieeexplore.ieee.org/abstract/document/10045744" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/littlewhitesea/STDAN" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_std/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Learning Spatio-Temporal Downsampling for Effective Video Upscaling</b></p>
        <p>Xiaoyu Xiang, <b>Yapeng Tian</b>, Vijay Rengarajan, Lucas Young, Bo Zhu, Rakesh Ranjan</p>
        <p><i>ECCV'22: European Conference on Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136780159.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2021_matting/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Video Matting via Consistency-Regularized Graph Neural Networks</b></p>
        <p>Tiantian Wang, Sifei Liu, <b>Yapeng Tian</b>, Kai Li, and Ming-Hsuan Yang</p>
        <p><i>ICCV'21: IEEE/CVF International Conference on Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://faculty.ucmerced.edu/mhyang/papers/iccv2021_video_matting.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/TiantianWang/VideoMatting-CRGNN" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2020_zsm/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution</b></p>
        <p>Xiaoyu Xiang<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, Yulun Zhang, Yun Fu, Jan Allebach, and Chenliang Xu</p>
        <p><i>CVPR'20: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2002.11616.pdf" target="_blank">Paper</a>
          

          

          

          
            <a class="button" href="https://www.youtube.com/watch?v=8mgD8JxBOus" target="_blank">Video</a>
          

          
            <a class="button" href="https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2020_tdan/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>TDAN: Temporally Deformable Alignment Network for Video Super-Resolution</b></p>
        <p><b>Yapeng Tian</b>, Yulun Zhang, Yun Fu, and Chenliang Xu</p>
        <p><i>CVPR'20: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
        
        <p style="color:red">This is the first work that uses deformable alignment to address video restoration.</p>
       
         <div class="paper-buttons">
          
            <a class="button" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Tian_TDAN_Temporally-Deformable_Alignment_Network_for_Video_Super-Resolution_CVPR_2020_paper.pdf" target="_blank">Paper</a>
          

          

          

          
            <a class="button" href="https://www.youtube.com/watch?v=eZExENE50I0" target="_blank">Video</a>
          

          
            <a class="button" href="https://github.com/YapengTian/TDAN-VSR-CVPR-2020" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
    </div>



    <div class="tab-pane" id="papers-imagerestoration">
      
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_ZFusion/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>ZFusion: Efficient Deep Compositional Zero-shot Learning for Blind Image Super-Resolution with Generative Diffusion Prior</b></p>
        <p>Alireza Esmaeilzehi, Hossein Zaredar, <b>Yapeng Tian</b>, Laleh Seyyed-Kalantari</p>
        <p><i>ICCV'25: IEEE/CVF International Conference on Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/2673.png?t=1759954583.3458676" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2025_diffir/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>DiffIR: Efficient Diffusion Model for Image Restoration</b></p>
        <p>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, <b>Yapeng Tian</b>, Wenming Yang, Radu Timotfe, Luc Van Gool</p>
        <p><i>TPAMI'25: IEEE Transactions on Pattern Analysis and Machine Intelligence. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2308.13767" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/Zj-BinXia/DiffIR" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_diffir/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>DiffIR: Efficient Diffusion Model for Image Restoration</b></p>
        <p>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, <b>Yapeng Tian</b>, Wenming Yang, Luc Van Gool</p>
        <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2303.09472" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/Zj-BinXia/DiffIR" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_MCMRI/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</b></p>
        <p>Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, <b>Yapeng Tian</b></p>
        <p><i>MICCAI'23: Medical Image Computing and Computer-Assisted Intervention. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2307.02334" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/jmzhang79/Dual-ArbNet" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_mrda/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Meta-Learning based Degradation Representation for Blind Super-Resolution</b></p>
        <p>Bin Xia,  <b>Yapeng Tian</b>, Yulun Zhang, Yucheng Hang, Wenming Yang, Qingmin Liao</p>
        <p><i>TIP'23: IEEE Transactions on Image Processing. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2207.13963" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/Zj-BinXia/MRDA" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_iclrkd/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Knowledge Distillation based Degradation Estimation for Blind Super-Resolution</b></p>
        <p>Bin Xia, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Radu Timofte, Luc Van Gool</p>
        <p><i>ICLR'23: International Conference on Learning Representations.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/2211.16928" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/Zj-BinXia/KDSR" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_iclrbbcu/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Basic Binary Convolution Unit for Binarized Image Restoration Network</b></p>
        <p>Bin Xia, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Radu Timofte, Luc Van Gool</p>
        <p><i>ICLR'23: International Conference on Learning Representations.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2210.00405.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/Zj-BinXia/BBCU" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2023_spl/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>GDSSR: Toward Real-World Ultra-High-Resolution Image Super-Resolution</b></p>
        <p>Yichen Chi, Wenming Yang, <b>Yapeng Tian</b></p>
        <p><i>SPL'23: IEEE Signal Processing Letters.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://ieeexplore.ieee.org/abstract/document/10041757" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_dudocaf/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>DuDoCAF: Dual-Domain Cross-Attention Fusion with Recurrent Transformer for Fast Multi-contrast MR Imaging</b></p>
        <p>Jun Lyu, Bin Sui, Chengyan Wang, <b>Yapeng Tian</b>, Qi Dou, and Jing Qin</p>
        <p><i>MICCAI'22: Medical Image Computing and Computer Assisted Intervention. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="/assets/publications/2022_dudocaf/paper.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_mrisr/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Transformer-empowered Multi-contrast MRI Super-Resolution</b></p>
        <p>Guangyuan Li, Jun Lv, <b>Yapeng Tian</b>, Qi Dou, Chengyan Wang, Chenliang Xu, Jing Qin</p>
        <p><i>CVPR'22: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2203.13963.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/XAIMI-Lab/McMRSR" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_amsa/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution</b></p>
        <p>Bin Xia, <b>Yapeng Tian</b>, Yucheng Hang, Wenming Yang, Qingmin Liao, Jie Zhou</p>
        <p><i>AAAI'22: The AAAI Conference on Artificial Intelligence.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2201.04358.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/Zj-BinXia/AMSA" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2022_enlca/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Efficient Non-Local Contrastive Attention for Image Super-Resolution</b></p>
        <p>Bin Xia<sup>‚Ä°</sup>, Yucheng Hang<sup>‚Ä°</sup>, <b>Yapeng Tian</b>, Wenming Yang, Qingmin Liao, Jie Zhou</p>
        <p><i>AAAI'22: The AAAI Conference on Artificial Intelligence. </i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/2201.03794.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/Zj-BinXia/ENLCA" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2020_rdnpami/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Residual Dense Network for Image Super-Resolution</b></p>
        <p>Yulun Zhang, <b>Yapeng Tian</b>, Yu Kong , Bineng Zhong, Yun Fu</p>
        <p><i>TPAMI'20: IEEE Transactions on Pattern Analysis and Machine Intelligence.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/1812.10477.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/yulunzhang/RDN" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2019_csf/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>CFSNet: Toward a Controllable Feature Space for Image Restoration</b></p>
        <p>Wei Wang<sup>‚Ä°</sup>, Ruiming Guo<sup>‚Ä°</sup>, <b>Yapeng Tian</b>, and Wenming Yang</p>
        <p><i>ICCV'19: IEEE/CVF International Conference on Computer Vision.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/1904.00634.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/qibao77/CFSNet" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2019_lcsc/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>LCSCNet: Linear Compressing Based Skip-Connecting Network for ISR</b></p>
        <p>Wenming Yang, Xuechen Zhang, <b>Yapeng Tian</b>, Wei Wang, Jing-Hao Xue, Qingmin Liao</p>
        <p><i>TIP'19: IEEE Trans. Image Processing.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/abs/1909.03573" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2019_review/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Deep Learning for Single Image Super-Resolution: A Brief Review</b></p>
        <p>Wenming Yang, Xuechen Zhang, <b>Yapeng Tian</b>, Wei Wang, JingHao Xue, Qingmin Liao</p>
        <p><i>TMM'19: IEEE Trans. Multimedia.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/1808.03344.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2018_rdn/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Residual Dense Network for Image Super-Resolution</b></p>
        <p>Yulun Zhang, <b>Yapeng Tian</b>, Yu Kong , Bineng Zhong, Yun Fu</p>
        <p><i>CVPR'18 Spotlight: IEEE/CVF Conf. on Computer Vision and Pattern Recognition.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://arxiv.org/pdf/1802.08797.pdf" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/yulunzhang/RDN" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2017_ntire/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results</b></p>
        <p>Timofte et al.</p>
        <p><i>CVPRW'17: CVPR Workshops.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://people.ee.ethz.ch/~timofter/publications/Timofte-CVPRW-2017.pdf" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2016_ccs/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Consistent Coding Scheme for Single-Image Super-Resolution</b></p>
        <p>Wenming Yang, <b>Yapeng Tian</b>, Fei Zhou, Qingmin Liao, Hai Chen, Chenglin Zheng</p>
        <p><i>TMM'16: EEE Trans. Multimedia. (First student author)</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://drive.google.com/file/d/1MpEM5qmSJi1GtY9TftNNV0cgSHqMT_KQ/view" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2016_anrse/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>Anchored Neighborhood Regression based SISR from Self-examples</b></p>
        <p><b>Yapeng Tian</b>, Fei Zhou, Wenming Yang, Xuesen Shang, Qingmin Liao</p>
        <p><i>ICIP'16: IEEE International Conference on Image Processing.</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://drive.google.com/file/d/1Ts_gYIp57llzK53Wyt7hwu4lZ9GQsbis/view" target="_blank">Paper</a>
          

          

          

          

          
            <a class="button" href="https://github.com/YapengTian/ICIP2016" target="_blank">Code</a>
          

          

          
         </div>


      </div>
        </div>
      
      <div >
        <div class="figure">
          <img style="float:left;margin-bottom:20px;margin-right:30px;vertical-align:middle;" class="hidden-xs hidden-sm hidden-md" src="/assets/publications/2015_pfsr/teaser.png" width="220" height="110" /> 
        </div>
      <div class="paper">
        <p class="title"><b>SISR Using Clustering-Based Global Regression and Propagation Filtering</b></p>
        <p>Wenming Yang, <b>Yapeng Tian</b>, Fei Zhou, ..., Qingmin Liao</p>
        <p><i>ACPR'15 Oral: Asian Conference on Pattern Recognition. (First student author)</i></p>
         
         <div class="paper-buttons">
          
            <a class="button" href="https://drive.google.com/file/d/12tqTHt0aLn7-B1jEgEJBYY64uhwfEezA/view" target="_blank">Paper</a>
          

          

          

          

          

          

          
         </div>


      </div>
        </div>
      
    </div>


    <!-- <div class="tab-pane active" id="papers-all">
      
      
        <div class="paper">
          <p class="title"><b>Towards Online Multi-Modal Social Interaction Understanding</b></p>
          <p>Xinpeng Li, Shijian Deng, Bolin Lai, Weiguo Pian, James M. Rehg, <b>Yapeng Tian</b></p>
          <p><i>Preprint'25. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2503.19851" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Modality-Inconsistent Continual Learning of Multimodal Large Language Models</b></p>
          <p>Weiguo Pian, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>Preprint'24. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.13050" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning in Audio-visual Context: A Review, Analysis, and New Perspective</b></p>
          <p>Yake Wei, Di Hu, <b>Yapeng Tian</b>, Xuelong Li</p>
          <p><i>Preprint'22. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2208.09579.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://gewu-lab.github.io/audio-visual-learning/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?</b></p>
          <p>Jia Li, Wenjie Zhao, Ziru Huang, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>AAAI'26: Annual AAAI Conference on Artificial Intelligence. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2502.00358" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Toward Gaze Target Detection of Young Autistic Children</b></p>
          <p>Shijian Deng, Erin E. Kosloski, Siva Sai Nagender Vasireddy, Jia Li, Randi Sierra Sherwood, Feroz Mohamed Hatha, Siddhi Patel, Pamela R Rollins, <b>Yapeng Tian</b></p>
          <p><i>AAAI'26 Oral: AAAI Conference on Artificial Intelligence (Social Impact Track). </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2511.11244" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AVROBUSTBENCH: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time</b></p>
          <p>Sarthak Kumar Maharana, Saksham Singh Kushwaha, Baoming Zhang, Adrian Rodriguez, Songtao Wei, <b>Yapeng Tian</b>, Yunhui Guo</p>
          <p><i>NeurIPS'25: Conference on Neural Information Processing Systems (D&B Track). </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2506.00358" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/sarthaxxxxx/AVROBUSTBENCH" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach</b></p>
          <p>Shijian Deng, Wentian Zhao, Yu-Jhe Li, Kun Wan, Daniel Miranda, Ajinkya Kale, <b>Yapeng Tian</b></p>
          <p><i>COLM'25: Second Conference on Language Modeling. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2411.17760" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Self-Improvement in Multimodal Large Language Models: A Survey</b></p>
          <p>Shijian Deng, Kai Wang, Tianyu Yang, Harsh Singh, <b>Yapeng Tian</b></p>
          <p><i>EMNLP'25 Findings: Conference on Empirical Methods in Natural Language Processing. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2510.02665" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation</b></p>
          <p>Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, <b>Yapeng Tian</b></p>
          <p><i>ACM MM'25: ACM International Conference on Multimedia. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.07686" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling</b></p>
          <p>Chao Huang, Susan Liang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>IJCV'25: International Journal of Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2509.22063" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People</b></p>
          <p>Daniel Killough, Justin Feng, Zheng Xue Ching, Daniel Wang, Rithvik Dyava, <b>Yapeng Tian</b>, Yuhang Zhao</p>
          <p><i>UIST'25:  ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://dl.acm.org/doi/full/10.1145/3746059.3747641" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AROMA: Mixed-Initiative AI Assistance for Non-Visual Cooking by Grounding Multi-modal Information Between Reality and Videos</b></p>
          <p>Zheng Ning, Leyang Li, Daniel Killough, JooYoung Seo, Patrick Carrington, <b>Yapeng Tian</b>, Yuhang Zhao, Franklin Mingzhe Li, Toby Jia-Jun Li</p>
          <p><i>UIST'25:  ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2507.10963" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models</b></p>
          <p>Xin Jin, Yichuan Zhong, <b>Yapeng Tian</b></p>
          <p><i>TMLR'25: Transactions on Machine Learning Research. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/pdf?id=q6M73uOBZE" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Prompt Image to Watch and Hear: Multimodal Prompting for Parameter-Efficient Audio-Visual Learning</b></p>
          <p>Kai Wang, Shentong Mo, <b>Yapeng Tian</b>, Dimitrios Hatzinakos</p>
          <p><i>BMVC'25: The British Machine Vision Conference (BMVC). </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Signllm: Sign language production large language models</b></p>
          <p>Sen Fang, Chen Chen, Lei Wang, Ce Zheng, Chunyu Sui, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'25:  IEEE/CVF International Conference on Computer Vision CV4A11y Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/ICCV2025W/CV4A11y/papers/Fang_SignLLM_Sign_Language_Production_Large_Language_Models_ICCVW_2025_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://signllm.github.io/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Introduction to the First Workshop on Vision Foundation Models and Generative AI for Accessibility</b></p>
          <p><b>Yapeng Tian</b>, Yuhang Zhao, Jon E. Froehlich, Chu Li, Yuheng Wu</p>
          <p><i>ICCVW'25:  IEEE/CVF International Conference on Computer Vision CV4A11y Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/ICCV2025W/CV4A11y/papers/Tian_Introduction_to_the_First_Workshop_on_Vision_Foundation_Models_and_ICCVW_2025_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>ZFusion: Efficient Deep Compositional Zero-shot Learning for Blind Image Super-Resolution with Generative Diffusion Prior</b></p>
          <p>Alireza Esmaeilzehi, Hossein Zaredar, <b>Yapeng Tian</b>, Laleh Seyyed-Kalantari</p>
          <p><i>ICCV'25: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/2673.png?t=1759954583.3458676" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>PRVQL: Progressive Knowledge-guided Refinement for Robust Egocentric Visual Query Localization</b></p>
          <p>Bing Fan, Yunhe Feng, <b>Yapeng Tian</b>, Yuewei Lin, Yan Huang, Heng Fan</p>
          <p><i>ICCV'25: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2502.07707" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://github.com/fb-reps/PRVQL" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation</b></p>
          <p>Saksham Singh Kushwaha, <b>Yapeng Tian</b></p>
          <p><i>CVPR'25: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.10768" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://sakshamsingh1.github.io/vintage/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level</b></p>
          <p>Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, <b>Yapeng Tian</b>, Ajmal Saeed Mian, Mohit Bansal, Chen Chen</p>
          <p><i>CVPR'25: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2411.09921" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://groundmore.github.io/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Diff-SAGe: End-to-End Spatial Audio Generation Using Diffusion Models</b></p>
          <p>Saksham Singh Kushwaha, Jianbo Ma, Mark R. P. Thomas,  <b>Yapeng Tian</b>, and Avery Bruni</p>
          <p><i>ICASSP'25: IEEE International Conference on Acoustics, Speech, and Signal Processing. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2410.11299" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP</b></p>
          <p>Tianyu Yang, Lisen Dai, Zheyuan Liu, Xiangqi Wang, Meng Jiang, <b>Yapeng Tian</b>, Xiangliang Zhang</p>
          <p><i>ACL'25 Main: Annual Meeting of the Association for Computational Linguistics. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.23330" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://tianyu-yang-anna.github.io/ClipErase-ACL/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>MagicTalk: Implicit and Explicit Correlation Learning for Diffusion-based Emotional Talking Face Generation</b></p>
          <p>Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, <b>Yapeng Tian</b>, Jiashi Feng, Xiaohu Guo</p>
          <p><i>CVM:  Computational Visual Media Journal. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11145205" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://magictalk.github.io/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Demonstration of VRSight: AI-Driven Real-Time Descriptions to Enhance VR Accessibility for Blind People</b></p>
          <p>Daniel Killough, Justin Feng, Rithvik Dyava, Zheng Xue Ching, Daniel Wang, <b>Yapeng Tian</b>, Yuhang Zhao</p>
          <p><i>CHI EA'25: Extended Abstracts of the CHI Conference </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://dl.acm.org/doi/abs/10.1145/3706599.3721194" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Leveraging AI to Assess Social Attention in Young Autistic Children</b></p>
          <p>Erin Kosloski, Shijian Deng, Siva S. N. Vasireddy, Randi S. Sherwood, Feroz M. Hatha, Jia Li, Siddhi Patel, <b>Yapeng Tian</b>, Pamela Rollins</p>
          <p><i>SRCLD'25: Symposium on Research in Child Language Disorders. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.researchgate.net/publication/392469852_Leveraging_Artificial_Intelligence_to_Assess_Social_Attention_in_Young_Autistic_Children" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>SignDiff: Learning Diffusion Models for American Sign Language Production</b></p>
          <p>Sen Fang, Chunyu Sui, Yanghao Zhou, Xuedong Zhang, Hongbin Zhong, <b>Yapeng Tian</b>, Chen Chen</p>
          <p><i>FGW'25: International Conference on Automatic Face and Gesture Recognition Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.16082.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Language-Guided Adaptive Vision Token Pruning for Efficient Multimodal Large Language Models</b></p>
          <p>Omer Faruk Deniz, Tarik Arici, Fatemeh Sheikholeslami, Burak Gozluklu, Ameni Trabelsi, Suleiman Khan, <b>Yapeng Tian</b>, Latifur Khan</p>
          <p><i>PAKDD'25 Oral: The Pacific-Asia Conference on Knowledge Discovery and Data Mining. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://link.springer.com/chapter/10.1007/978-981-96-8186-0_9" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Joint Co-Speech Gesture and Expressive Talking Face Generation using Diffusion with Adapters</b></p>
          <p>Steven Hogue, Chenxu Zhang, <b>Yapeng Tian</b>, Xiaohu Guo</p>
          <p><i>WACV'25: IEEE/CVF Winter Conference on Applications of Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.14333" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>DiffIR: Efficient Diffusion Model for Image Restoration</b></p>
          <p>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, <b>Yapeng Tian</b>, Wenming Yang, Radu Timotfe, Luc Van Gool</p>
          <p><i>TPAMI'25: IEEE Transactions on Pattern Analysis and Machine Intelligence. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.13767" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/DiffIR" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Dataset Distillation</b></p>
          <p>Saksham Singh Kushwaha, Siva Sai Nagender Vasireddy, Kai Wang, <b>Yapeng Tian</b></p>
          <p><i>TMLR'24: Transactions on Machine Learning Research </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/forum?id=IJlbuSrXmk" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Continual Audio-Visual Sound Separation</b></p>
          <p>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>NeurIPS'24: The Annual Conference on Neural Information Processing Systems </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Pian_Learning_Continual_Audio-Visual_Sound_Separation_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition</b></p>
          <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
          <p><i>TMM'24: IEEE Transactions on Multimedia. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2406.02554" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://github.com/ShijianDeng/AV-ASD" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span>Towards AI-Powered AR for Enhancing Sports Playability for People with Low Vision: An Exploration of ARSports <span style='color: red;'> (Best Paper Award)</span></b></p>
          <p>Jaewook Lee, Yang Li, Dylan Bunarto, Eujean Lee, Olivia Wang, Adrian Rodriguez, Yuhang Zhao, <b>Yapeng Tian</b>, Jon E. Froehlich</p>
          <p><i>ISMAR IDEATExR'24 : International Symposium on Mixed and Augmented Reality Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://makeabilitylab.cs.washington.edu/media/publications/Lee_TowardsAiPoweredArForEnhancingSportsPlayabilityForPeopleWithLowVisionAnExplorationOfArsports_IDEATExR2024.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span> CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool Interactions for People with Low Vision <span style='color: red;'> (Belonging & Inclusion Best Paper Award)</span></b></p>
          <p>Jaewook Lee, Andrew D. Tjahjadi, Jiho Kim, Junpu Yu, Minji Park, Jiawen Zhang, Jon E. Froehlich, <b>Yapeng Tian</b>, Yuhang Zhao</p>
          <p><i>UIST'24: ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2407.13515" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://github.com/makeabilitylab/CookAR" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span> DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models<br><span style='color: red;'> (Best Paper Honorable Mention)</span></b></p>
          <p>Chao Huang, Susan Liang,  <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ACCV'24 Oral: Asian Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2308.00122" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Language-Guided Joint Audio-Visual Editing Via One-Shot Adaptation</b></p>
          <p>Susan Liang, Chao Huang,  <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ACCV'24: Asian Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.07463" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering</b></p>
          <p>Tianyu Yang, Yiyang Nan, Lisen Dai, Zhenwen Liang, <b>Yapeng Tian</b>, Xiangliang Zhang</p>
          <p><i>EMNLP'24: Empirical Methods in Natural Language Processing (Findings) </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2411.04933" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Towards Long Form Audio-visual Video Understanding</b></p>
          <p>Wenxuan Hou, Guangyao Li, <b>Yapeng Tian</b>, Di Hu</p>
          <p><i>TOMM'24: ACM Trans. on Multimedia Computing, Communications and App. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2306.09431" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://gewu-lab.github.io/LFAV/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Benchmarking and Optimizing Federated Learning with Hardware-related Metrics</b></p>
          <p>Kai Pan, <b>Yapeng Tian</b>, Yinhe Han, Yiming Gan</p>
          <p><i>BMVC'24: British Machine Vision Conference </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>EgoVSR: Towards High-Quality Egocentric Video Super-Resolution</b></p>
          <p>Yichen Chi, Junhao Gu, Jiamiao Zhang, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>TCSVT'24: IEEE Transactions on Circuits and Systems for Video Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2305.14708.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>MIMOSA: Human-AI Co-Creation of Computational Spatial Audio Effects on Videos</b></p>
          <p>Zheng Ning, Zheng Zhang, Jerrick Ban, Kaiwen Jiang, Ruohong Gan, <b>Yapeng Tian</b>, Toby Jia-Jun Li</p>
          <p><i>C&C'24: ACM Conference on Creativity & Cognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://dl.acm.org/doi/pdf/10.1145/3635636.3656189" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AV-Mamba: Cross-Modality Selective State Space Models for Audio-Visual Question Answering</b></p>
          <p>Ziru Huang, Jia Li, Wenjie Zhao, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Huang_AV-Mamba_Cross-Modality_Selective_State_Space_Models_for_Audio-Visual_Question_Answering.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning Continual Audio-Visual Sound Separation Models</b></p>
          <p>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Pian_Learning_Continual_Audio-Visual_Sound_Separation_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Autism Behavior Recognition with MMLMs</b></p>
          <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Audio-Visual_Autism_Behavior_Recognition_with_Multimodal_Large_Language_Models_Audio-Visual_Autism_Behavior_Recognition_with_Multimodal_Large_Language_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Dataset distillation for audio-visual datasets</b></p>
          <p>Saksham Singh Kushwaha, Siva Sai Nagender Vasireddy, Kai Wang, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Kushwaha_Dataset_distillation_for_audio-visual_datasets.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures</b></p>
          <p>Steven Hogue, Chenxu Zhang, Hamza Daruger, <b>Yapeng Tian</b>, Xiaohu Guo</p>
          <p><i>CVPRW'24: CVPR HuMoGen Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2024W/HuMoGen/papers/Hogue_DiffTED_One-shot_Audio-driven_TED_Talk_Video_Generation_with_Diffusion-based_Co-speech_CVPRW_2024_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Towards Efficient Audio-Visual Learners via Empowering Pre-trained Vision Transformers with Cross-Modal Adaptation</b></p>
          <p>Kai Wang, <b>Yapeng Tian</b>, Dimitrios Hatzinakos</p>
          <p><i>CVPRW'24: CVPR Multimodal Foundation Models Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2024W/MMFM/papers/Wang_Towards_Efficient_Audio-Visual_Learners_via_Empowering_Pre-trained_Vision_Transformers_with_CVPRW_2024_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/kaiw7/STG-CMA" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>MA-AVT: Modality Alignment for Parameter-Efficient Audio-Visual Transformers</b></p>
          <p>Tanvir Mahmud, Shentong Mo, <b>Yapeng Tian</b>, Diana Marculescu</p>
          <p><i>CVPRW'24: CVPR Efficient Deep Learning for Computer Vision Workshop </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.04930" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>T-VSL: Text-Guided Visual Sound Source Localization in Mixtures</b></p>
          <p>Tanvir Mahmud, <b>Yapeng Tian</b>, Diana Marculescu</p>
          <p><i>CVPR'24: IEEE/CVF Conference on Computer Vision and Pattern Recognition </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2404.01751" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/enyac-group/T-VSL/tree/main" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>OSCaR: Object State Captioning and State Change Representation</b></p>
          <p>Nguyen Nguyen, Jing Bi, Ali Vosoughi, <b>Yapeng Tian</b>, Pooyan Fazli, Chenliang Xu</p>
          <p><i>NAACL'24: The North American Chapter of the Association for Computational Linguistics (Findings) </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2402.17128" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers</b></p>
          <p>Zheng Ning, Brianna Wimer, Kaiwen Jiang, Keyi Chen, Jerrick Ban, <b>Yapeng Tian</b>, Yuhang Zhao, Toby Li</p>
          <p><i>CHI'24: The ACM Conference on Human Factors in Computing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2402.07300" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>STADNet: Spatial-Temporal Attention-Guided Dual-Path Network for cardiac cine MRI super-resolution</b></p>
          <p>Jun Lyu, Shuo Wang, <b>Yapeng Tian</b>, Jing Zou, Shunjie Dong, Chengyan Wang, Angelica I Aviles-Rivero, Jing Qin</p>
          <p><i>MIA'24: Medical Image Analysis</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841524000677" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Unveiling cross modality bias in visual question answering: A causal view with possible worlds vqa</b></p>
          <p>Ali Vosoughi<sup>‚Ä°</sup>, Shijian Deng<sup>‚Ä°</sup>, Songyang Zhang, <b>Yapeng Tian</b>, Chenliang Xu, Jiebo Luo</p>
          <p><i>TMM'24: IEEE Transactions on Multimedia</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2305.19664" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> LAVSS: Location-Guided Audio-Visual Spatial Audio Separation</b></p>
          <p>Yuxin Ye, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>WACV'24: Winter Conference on Applications of Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2310.20446.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://yyx666660.github.io/LAVSS/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Disentangled counterfactual learning for physical audiovisual commonsense reasoning</b></p>
          <p>Changsheng Lv, Shuai Zhang, <b>Yapeng Tian</b>, Mengshi Qi, Huadong Ma</p>
          <p><i>NeurIPS'23: The Annual Conference on Neural Information Processing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2310.19559" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Andy20178/DCL" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>NeurIPS'23: The Annual Conference on Neural Information Processing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2302.02088.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://liangsusan-git.github.io/project/avnerf/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>PEANUT: A Human-AI Collaborative Tool for Annotating Audio-Visual Data</b></p>
          <p>Zheng Zhang<sup>‚Ä°</sup>, Zheng Ning<sup>‚Ä°</sup>, Chenliang Xu <b>Yapeng Tian</b>, Toby Jia-Jun Li</p>
          <p><i>UIST'23: ACM Symposium on User Interface Software and Technology. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2307.15167.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Towards Robust Active Speaker Detection</b></p>
          <p>Siva Sai Nagender Vasireddy, Chenxu Zhang, Xiaohu Guo, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p5.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Position-Aware Audio-Visual Separation for Spatial Audio</b></p>
          <p>Yuxin Ye, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p8.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Towards Better Egocentric Action Understanding in a Multi-Input Multi-Output View</b></p>
          <p>Wenxuan Hou, Ruoxuan Feng, Yixin Xu, <b>Yapeng Tian</b>, Di Hu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p13.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p1.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Separating Invisible Sounds Toward Universal Audio-Visual Scene-Aware Sound Separation</b></p>
          <p>Yiyang Su, Ali Vosoughi, Shijian Deng, <b>Yapeng Tian</b>, Chenliang Xu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p3.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Audio-Visual Class-Incremental Learning</b></p>
          <p>Weiguo Pian<sup>‚Ä°</sup>, Shentong Mo<sup>‚Ä°</sup>, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.11073.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/weiguoPian/AV-CIL_ICCV2023" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Class-Incremental Grouping Network for Continual Audio-Visual Learning</b></p>
          <p>Shentong Mo<sup>‚Ä°</sup>, Weiguo Pian<sup>‚Ä°</sup>, <b>Yapeng Tian</b></p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2309.05281.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/CIGN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>DiffIR: Efficient Diffusion Model for Image Restoration</b></p>
          <p>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, <b>Yapeng Tian</b>, Wenming Yang, Luc Van Gool</p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2303.09472" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/DiffIR" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</b></p>
          <p>Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>MICCAI'23: Medical Image Computing and Computer-Assisted Intervention. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2307.02334" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/jmzhang79/Dual-ArbNet" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Meta-Learning based Degradation Representation for Blind Super-Resolution</b></p>
          <p>Bin Xia,  <b>Yapeng Tian</b>, Yulun Zhang, Yucheng Hang, Wenming Yang, Qingmin Liao</p>
          <p><i>TIP'23: IEEE Transactions on Image Processing. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2207.13963" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/MRDA" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2305.01836.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment</b></p>
          <p>Shentong Mo, Jing Shi, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2305.12903" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2302.02088.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://liangsusan-git.github.io/project/avnerf/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Grouping Network for Sound Localization from Mixtures</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2303.17056.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/AVGN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Egocentric Audio-Visual Object Localization</b></p>
          <p>Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, and Chenliang Xu</p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2303.13471.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/WikiChao/Ego-AV-Loc" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Structured Sparsity Learning for Efficient Video Super-Resolution</b></p>
          <p>Bin Xia, Jingwen He, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Luc Van Gool</p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2206.07687" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/SSL" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Knowledge Distillation based Degradation Estimation for Blind Super-Resolution</b></p>
          <p>Bin Xia, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Radu Timofte, Luc Van Gool</p>
          <p><i>ICLR'23: International Conference on Learning Representations.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2211.16928" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/KDSR" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Basic Binary Convolution Unit for Binarized Image Restoration Network</b></p>
          <p>Bin Xia, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Radu Timofte, Luc Van Gool</p>
          <p><i>ICLR'23: International Conference on Learning Representations.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2210.00405.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/BBCU" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Stdan: deformable attention network for space-time video super-resolution</b></p>
          <p>Hai Wang, Xiaoyu Xiang, <b>Yapeng Tian</b>, Wenming Yang, Qingmin Liao</p>
          <p><i>TNNLS'23: IEEE Transactions on Neural Networks and Learning Systems.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/abstract/document/10045744" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/littlewhitesea/STDAN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>GDSSR: Toward Real-World Ultra-High-Resolution Image Super-Resolution</b></p>
          <p>Yichen Chi, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>SPL'23: IEEE Signal Processing Letters.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/abstract/document/10041757" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Towards Unified, Explainable, and Robust Multisensory Perception</b></p>
          <p>Yapeng Tian</p>
          <p><i>AAAI'23: AAAI Conference on Artificial Intelligence. (NFH program)</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="/assets/publications/2023_aaainfh/paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>NeurIPS'22: The Annual Conference on Neural Information Processing Systems. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/pdf?id=zfo2LqFEVY" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/MGN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning Spatio-Temporal Downsampling for Effective Video Upscaling</b></p>
          <p>Xiaoyu Xiang, <b>Yapeng Tian</b>, Vijay Rengarajan, Lucas Young, Bo Zhu, Rakesh Ranjan</p>
          <p><i>ECCV'22: European Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136780159.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Scene Understanding Towards Unified, Explainable, and Robust Multisensory Perception</b></p>
          <p><b>Yapeng Tian</b></p>
          <p><i>PhD Thesis </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://www.proquest.com/openview/99bef5e8207df0ebab29e6b1f2afbad8/1.pdf?pq-origsite=gscholar&cbl=18750&diss=y" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>DuDoCAF: Dual-Domain Cross-Attention Fusion with Recurrent Transformer for Fast Multi-contrast MR Imaging</b></p>
          <p>Jun Lyu, Bin Sui, Chengyan Wang, <b>Yapeng Tian</b>, Qi Dou, and Jing Qin</p>
          <p><i>MICCAI'22: Medical Image Computing and Computer Assisted Intervention. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="/assets/publications/2022_dudocaf/paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Object Localization in Egocentric Videos</b></p>
          <p>Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, and Chenliang Xu</p>
          <p><i>CVPRW'22: CVPR Workshops</i></p>
          
          <p style="color:red">Egocentric audio-visual learning.</p>
         
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</b></p>
          <p>Guangyao Li<sup>‚Ä°</sup>, Yake Wei<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, Chenliang Xu, Ji-Rong Wen, and Di Hu</p>
          <p><i>CVPR'22 Oral: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2203.14072.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=JH3t5gwe9Xw" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/GeWu-Lab/MUSIC-AVQA" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://gewu-lab.github.io/MUSIC-AVQA/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Transformer-empowered Multi-contrast MRI Super-Resolution</b></p>
          <p>Guangyuan Li, Jun Lv, <b>Yapeng Tian</b>, Qi Dou, Chengyan Wang, Chenliang Xu, Jing Qin</p>
          <p><i>CVPR'22: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2203.13963.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/XAIMI-Lab/McMRSR" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution</b></p>
          <p>Bin Xia, <b>Yapeng Tian</b>, Yucheng Hang, Wenming Yang, Qingmin Liao, Jie Zhou</p>
          <p><i>AAAI'22: The AAAI Conference on Artificial Intelligence.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2201.04358.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/AMSA" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Efficient Non-Local Contrastive Attention for Image Super-Resolution</b></p>
          <p>Bin Xia<sup>‚Ä°</sup>, Yucheng Hang<sup>‚Ä°</sup>, <b>Yapeng Tian</b>, Wenming Yang, Qingmin Liao, Jie Zhou</p>
          <p><i>AAAI'22: The AAAI Conference on Artificial Intelligence. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2201.03794.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/ENLCA" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Space-Time Memory Network for Sounding Object Localization in Videos</b></p>
          <p>Sizhe Li<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, and Chenliang Xu</p>
          <p><i>BMVC'21: The British Machine Vision Conference. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2111.05526.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/lester0866/Space-Time-Memory-Network-for-Sounding-Object-Localization-in-Videos" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://sites.google.com/view/bmvc2021stm" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Video Matting via Consistency-Regularized Graph Neural Networks</b></p>
          <p>Tiantian Wang, Sifei Liu, <b>Yapeng Tian</b>, Kai Li, and Ming-Hsuan Yang</p>
          <p><i>ICCV'21: IEEE/CVF International Conference on Computer Vision. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://faculty.ucmerced.edu/mhyang/papers/iccv2021_video_matting.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/TiantianWang/VideoMatting-CRGNN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Can audio-visual integration strengthen robustness under multimodal attacks?</b></p>
          <p><b>Yapeng Tian</b> and Chenliang Xu</p>
          <p><i>CVPR'21: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2104.02000.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/AV-Robustness-CVPR21" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation</b></p>
          <p><b>Yapeng Tian</b>, Di Hu, and Chenliang Xu</p>
          <p><i>CVPR'21: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2104.02026.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/CCOL-CVPR21" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing</b></p>
          <p><b>Yapeng Tian</b>, Dingzeyu Li, and Chenliang Xu</p>
          <p><i>ECCV'20 Spotlight: European Conference on Computer Vision.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2007.10558.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/AVVP-ECCV20" target="_blank">Code</a>
            

            
              <a class="button" href="https://github.com/YapengTian/AVVP-ECCV20" target="_blank">Data</a>
            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution</b></p>
          <p>Xiaoyu Xiang<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, Yulun Zhang, Yun Fu, Jan Allebach, and Chenliang Xu</p>
          <p><i>CVPR'20: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2002.11616.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=8mgD8JxBOus" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>TDAN: Temporally Deformable Alignment Network for Video Super-Resolution</b></p>
          <p><b>Yapeng Tian</b>, Yulun Zhang, Yun Fu, and Chenliang Xu</p>
          <p><i>CVPR'20: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
          
          <p style="color:red">This is the first work that uses deformable alignment to address video restoration.</p>
         
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Tian_TDAN_Temporally-Deformable_Alignment_Network_for_Video_Super-Resolution_CVPR_2020_paper.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=eZExENE50I0" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/YapengTian/TDAN-VSR-CVPR-2020" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Deep Audio Prior</b></p>
          <p><b>Yapeng Tian</b>, Chenliang Xu, and Dingzeyu Li</p>
          <p><i>CVPRW'20: CVPR Workshops.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1912.10292v1.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/adobe/Deep-Audio-Prior" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://opensource.adobe.com/Deep-Audio-Prior/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Residual Dense Network for Image Super-Resolution</b></p>
          <p>Yulun Zhang, <b>Yapeng Tian</b>, Yu Kong , Bineng Zhong, Yun Fu</p>
          <p><i>TPAMI'20: IEEE Transactions on Pattern Analysis and Machine Intelligence.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1812.10477.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/yulunzhang/RDN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>CFSNet: Toward a Controllable Feature Space for Image Restoration</b></p>
          <p>Wei Wang<sup>‚Ä°</sup>, Ruiming Guo<sup>‚Ä°</sup>, <b>Yapeng Tian</b>, and Wenming Yang</p>
          <p><i>ICCV'19: IEEE/CVF International Conference on Computer Vision.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1904.00634.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/qibao77/CFSNet" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Interpretable and Controllable Audio-Visual Video Captioning</b></p>
          <p><b>Yapeng Tian</b>, Chenxiao Guan, Goodman Justin, Marc Moore, and Chenliang Xu</p>
          <p><i>CVPRW'19: CVPR Workshops.</i></p>
          
          <p style="color:red">Multisensory interpretability in terms of the audio-visual video captioning task.</p>
         
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Yapeng_Tian_Audio-Visual_Interpretable_and_Controllable_Video_Captioning_CVPRW_2019_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>LCSCNet: Linear Compressing Based Skip-Connecting Network for ISR</b></p>
          <p>Wenming Yang, Xuechen Zhang, <b>Yapeng Tian</b>, Wei Wang, Jing-Hao Xue, Qingmin Liao</p>
          <p><i>TIP'19: IEEE Trans. Image Processing.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/1909.03573" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Deep Learning for Single Image Super-Resolution: A Brief Review</b></p>
          <p>Wenming Yang, Xuechen Zhang, <b>Yapeng Tian</b>, Wei Wang, JingHao Xue, Qingmin Liao</p>
          <p><i>TMM'19: IEEE Trans. Multimedia.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1808.03344.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Event Localization in Unconstrained Videos</b></p>
          <p><b>Yapeng Tian</b>, Jing Shi, Bochen Li, Zhiyao Duan, Chenliang Xu</p>
          <p><i>ECCV'18: European Conference on Computer Vision.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=m6r6BbD5MSc" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/YapengTian/AVE-ECCV18" target="_blank">Code</a>
            

            
              <a class="button" href="https://drive.google.com/file/d/1FjKwe79e0u96vdjIVwfRQ1V6SoDHe7kK" target="_blank">Data</a>
            

            
              <a class="button" href="https://sites.google.com/view/audiovisualresearch" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Residual Dense Network for Image Super-Resolution</b></p>
          <p>Yulun Zhang, <b>Yapeng Tian</b>, Yu Kong , Bineng Zhong, Yun Fu</p>
          <p><i>CVPR'18 Spotlight: IEEE/CVF Conf. on Computer Vision and Pattern Recognition.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1802.08797.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/yulunzhang/RDN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results</b></p>
          <p>Timofte et al.</p>
          <p><i>CVPRW'17: CVPR Workshops.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://people.ee.ethz.ch/~timofter/publications/Timofte-CVPRW-2017.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Consistent Coding Scheme for Single-Image Super-Resolution</b></p>
          <p>Wenming Yang, <b>Yapeng Tian</b>, Fei Zhou, Qingmin Liao, Hai Chen, Chenglin Zheng</p>
          <p><i>TMM'16: EEE Trans. Multimedia. (First student author)</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/1MpEM5qmSJi1GtY9TftNNV0cgSHqMT_KQ/view" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Anchored Neighborhood Regression based SISR from Self-examples</b></p>
          <p><b>Yapeng Tian</b>, Fei Zhou, Wenming Yang, Xuesen Shang, Qingmin Liao</p>
          <p><i>ICIP'16: IEEE International Conference on Image Processing.</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/1Ts_gYIp57llzK53Wyt7hwu4lZ9GQsbis/view" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/ICIP2016" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>SISR Using Clustering-Based Global Regression and Propagation Filtering</b></p>
          <p>Wenming Yang, <b>Yapeng Tian</b>, Fei Zhou, ..., Qingmin Liao</p>
          <p><i>ACPR'15 Oral: Asian Conference on Pattern Recognition. (First student author)</i></p>
           
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/12tqTHt0aLn7-B1jEgEJBYY64uhwfEezA/view" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
    </div> -->

    <!-- <div class="tab-pane active" id="papers-selected">
      
      
    </div> -->
  
        

    <!-- <div class="tab-pane active" id="papers-audiovisual">
      
      
        <div class="paper">
          <p class="title"><b>Towards Online Multi-Modal Social Interaction Understanding</b></p>
          <p>Xinpeng Li, Shijian Deng, Bolin Lai, Weiguo Pian, James M. Rehg, <b>Yapeng Tian</b></p>
          <p><i>Preprint'25. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2503.19851" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Modality-Inconsistent Continual Learning of Multimodal Large Language Models</b></p>
          <p>Weiguo Pian, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>Preprint'24. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.13050" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning in Audio-visual Context: A Review, Analysis, and New Perspective</b></p>
          <p>Yake Wei, Di Hu, <b>Yapeng Tian</b>, Xuelong Li</p>
          <p><i>Preprint'22. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2208.09579.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://gewu-lab.github.io/audio-visual-learning/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?</b></p>
          <p>Jia Li, Wenjie Zhao, Ziru Huang, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>AAAI'26: Annual AAAI Conference on Artificial Intelligence. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2502.00358" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AVROBUSTBENCH: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time</b></p>
          <p>Sarthak Kumar Maharana, Saksham Singh Kushwaha, Baoming Zhang, Adrian Rodriguez, Songtao Wei, <b>Yapeng Tian</b>, Yunhui Guo</p>
          <p><i>NeurIPS'25: Conference on Neural Information Processing Systems (D&B Track). </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2506.00358" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/sarthaxxxxx/AVROBUSTBENCH" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation</b></p>
          <p>Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, <b>Yapeng Tian</b></p>
          <p><i>ACM MM'25: ACM International Conference on Multimedia. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.07686" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling</b></p>
          <p>Chao Huang, Susan Liang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>IJCV'25: International Journal of Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2509.22063" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Prompt Image to Watch and Hear: Multimodal Prompting for Parameter-Efficient Audio-Visual Learning</b></p>
          <p>Kai Wang, Shentong Mo, <b>Yapeng Tian</b>, Dimitrios Hatzinakos</p>
          <p><i>BMVC'25: The British Machine Vision Conference (BMVC). </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation</b></p>
          <p>Saksham Singh Kushwaha, <b>Yapeng Tian</b></p>
          <p><i>CVPR'25: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.10768" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://sakshamsingh1.github.io/vintage/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Diff-SAGe: End-to-End Spatial Audio Generation Using Diffusion Models</b></p>
          <p>Saksham Singh Kushwaha, Jianbo Ma, Mark R. P. Thomas,  <b>Yapeng Tian</b>, and Avery Bruni</p>
          <p><i>ICASSP'25: IEEE International Conference on Acoustics, Speech, and Signal Processing. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2410.11299" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>MagicTalk: Implicit and Explicit Correlation Learning for Diffusion-based Emotional Talking Face Generation</b></p>
          <p>Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, <b>Yapeng Tian</b>, Jiashi Feng, Xiaohu Guo</p>
          <p><i>CVM:  Computational Visual Media Journal. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11145205" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://magictalk.github.io/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Joint Co-Speech Gesture and Expressive Talking Face Generation using Diffusion with Adapters</b></p>
          <p>Steven Hogue, Chenxu Zhang, <b>Yapeng Tian</b>, Xiaohu Guo</p>
          <p><i>WACV'25: IEEE/CVF Winter Conference on Applications of Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2412.14333" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Dataset Distillation</b></p>
          <p>Saksham Singh Kushwaha, Siva Sai Nagender Vasireddy, Kai Wang, <b>Yapeng Tian</b></p>
          <p><i>TMLR'24: Transactions on Machine Learning Research </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/forum?id=IJlbuSrXmk" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Continual Audio-Visual Sound Separation</b></p>
          <p>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>NeurIPS'24: The Annual Conference on Neural Information Processing Systems </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Pian_Learning_Continual_Audio-Visual_Sound_Separation_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition</b></p>
          <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
          <p><i>TMM'24: IEEE Transactions on Multimedia. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2406.02554" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://github.com/ShijianDeng/AV-ASD" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b><span style='color: #FFC107;'> <i class='material-icons' style='font-size: 24px; vertical-align: top;'>emoji_events</i></span> DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models<br><span style='color: red;'> (Best Paper Honorable Mention)</span></b></p>
          <p>Chao Huang, Susan Liang,  <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ACCV'24 Oral: Asian Conference on Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2308.00122" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Language-Guided Joint Audio-Visual Editing Via One-Shot Adaptation</b></p>
          <p>Susan Liang, Chao Huang,  <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ACCV'24: Asian Conference on Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2410.07463" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering</b></p>
          <p>Tianyu Yang, Yiyang Nan, Lisen Dai, Zhenwen Liang, <b>Yapeng Tian</b>, Xiangliang Zhang</p>
          <p><i>EMNLP'24: Empirical Methods in Natural Language Processing (Findings) </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2411.04933" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Towards Long Form Audio-visual Video Understanding</b></p>
          <p>Wenxuan Hou, Guangyao Li, <b>Yapeng Tian</b>, Di Hu</p>
          <p><i>TOMM'24: ACM Trans. on Multimedia Computing, Communications and App. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2306.09431" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://gewu-lab.github.io/LFAV/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>MIMOSA: Human-AI Co-Creation of Computational Spatial Audio Effects on Videos</b></p>
          <p>Zheng Ning, Zheng Zhang, Jerrick Ban, Kaiwen Jiang, Ruohong Gan, <b>Yapeng Tian</b>, Toby Jia-Jun Li</p>
          <p><i>C&C'24: ACM Conference on Creativity & Cognition. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://dl.acm.org/doi/pdf/10.1145/3635636.3656189" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AV-Mamba: Cross-Modality Selective State Space Models for Audio-Visual Question Answering</b></p>
          <p>Ziru Huang, Jia Li, Wenjie Zhao, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Huang_AV-Mamba_Cross-Modality_Selective_State_Space_Models_for_Audio-Visual_Question_Answering.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning Continual Audio-Visual Sound Separation Models</b></p>
          <p>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Pian_Learning_Continual_Audio-Visual_Sound_Separation_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Autism Behavior Recognition with MMLMs</b></p>
          <p>Shijian Deng, Erin Kosloski, Siddhi Patel, Zeke A Barnett, Yiyang Nan, Alexander M Kaplan, Sisira Aarukapalli, William Doan, Matthew Wang, Harsh Singh, Rollins Pamela, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Audio-Visual_Autism_Behavior_Recognition_with_Multimodal_Large_Language_Models_Audio-Visual_Autism_Behavior_Recognition_with_Multimodal_Large_Language_Models.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Dataset distillation for audio-visual datasets</b></p>
          <p>Saksham Singh Kushwaha, Siva Sai Nagender Vasireddy, Kai Wang, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'24: CVPR Signt and Sound Workshop </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2024/Kushwaha_Dataset_distillation_for_audio-visual_datasets.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures</b></p>
          <p>Steven Hogue, Chenxu Zhang, Hamza Daruger, <b>Yapeng Tian</b>, Xiaohu Guo</p>
          <p><i>CVPRW'24: CVPR HuMoGen Workshop </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2024W/HuMoGen/papers/Hogue_DiffTED_One-shot_Audio-driven_TED_Talk_Video_Generation_with_Diffusion-based_Co-speech_CVPRW_2024_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Towards Efficient Audio-Visual Learners via Empowering Pre-trained Vision Transformers with Cross-Modal Adaptation</b></p>
          <p>Kai Wang, <b>Yapeng Tian</b>, Dimitrios Hatzinakos</p>
          <p><i>CVPRW'24: CVPR Multimodal Foundation Models Workshop </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content/CVPR2024W/MMFM/papers/Wang_Towards_Efficient_Audio-Visual_Learners_via_Empowering_Pre-trained_Vision_Transformers_with_CVPRW_2024_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/kaiw7/STG-CMA" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>MA-AVT: Modality Alignment for Parameter-Efficient Audio-Visual Transformers</b></p>
          <p>Tanvir Mahmud, Shentong Mo, <b>Yapeng Tian</b>, Diana Marculescu</p>
          <p><i>CVPRW'24: CVPR Efficient Deep Learning for Computer Vision Workshop </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2406.04930" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>T-VSL: Text-Guided Visual Sound Source Localization in Mixtures</b></p>
          <p>Tanvir Mahmud, <b>Yapeng Tian</b>, Diana Marculescu</p>
          <p><i>CVPR'24: IEEE/CVF Conference on Computer Vision and Pattern Recognition </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2404.01751" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/enyac-group/T-VSL/tree/main" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers</b></p>
          <p>Zheng Ning, Brianna Wimer, Kaiwen Jiang, Keyi Chen, Jerrick Ban, <b>Yapeng Tian</b>, Yuhang Zhao, Toby Li</p>
          <p><i>CHI'24: The ACM Conference on Human Factors in Computing Systems. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2402.07300" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> LAVSS: Location-Guided Audio-Visual Spatial Audio Separation</b></p>
          <p>Yuxin Ye, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>WACV'24: Winter Conference on Applications of Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2310.20446.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://yyx666660.github.io/LAVSS/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Disentangled counterfactual learning for physical audiovisual commonsense reasoning</b></p>
          <p>Changsheng Lv, Shuai Zhang, <b>Yapeng Tian</b>, Mengshi Qi, Huadong Ma</p>
          <p><i>NeurIPS'23: The Annual Conference on Neural Information Processing Systems. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2310.19559" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Andy20178/DCL" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>NeurIPS'23: The Annual Conference on Neural Information Processing Systems. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2302.02088.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://liangsusan-git.github.io/project/avnerf/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>PEANUT: A Human-AI Collaborative Tool for Annotating Audio-Visual Data</b></p>
          <p>Zheng Zhang<sup>‚Ä°</sup>, Zheng Ning<sup>‚Ä°</sup>, Chenliang Xu <b>Yapeng Tian</b>, Toby Jia-Jun Li</p>
          <p><i>UIST'23: ACM Symposium on User Interface Software and Technology. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2307.15167.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Towards Robust Active Speaker Detection</b></p>
          <p>Siva Sai Nagender Vasireddy, Chenxu Zhang, Xiaohu Guo, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p5.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Position-Aware Audio-Visual Separation for Spatial Audio</b></p>
          <p>Yuxin Ye, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p8.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Towards Better Egocentric Action Understanding in a Multi-Input Multi-Output View</b></p>
          <p>Wenxuan Hou, Ruoxuan Feng, Yixin Xu, <b>Yapeng Tian</b>, Di Hu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p13.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p1.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Separating Invisible Sounds Toward Universal Audio-Visual Scene-Aware Sound Separation</b></p>
          <p>Yiyang Su, Ali Vosoughi, Shijian Deng, <b>Yapeng Tian</b>, Chenliang Xu</p>
          <p><i>ICCVW'23: ICCV AV4D Workshop . </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://av4d.org/papers/iccv23/p3.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Audio-Visual Class-Incremental Learning</b></p>
          <p>Weiguo Pian<sup>‚Ä°</sup>, Shentong Mo<sup>‚Ä°</sup>, Yunhui Guo, <b>Yapeng Tian</b></p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.11073.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/weiguoPian/AV-CIL_ICCV2023" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b> Class-Incremental Grouping Network for Continual Audio-Visual Learning</b></p>
          <p>Shentong Mo<sup>‚Ä°</sup>, Weiguo Pian<sup>‚Ä°</sup>, <b>Yapeng Tian</b></p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2309.05281.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/CIGN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2305.01836.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment</b></p>
          <p>Shentong Mo, Jing Shi, <b>Yapeng Tian</b></p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2305.12903" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</b></p>
          <p>Susan Liang, Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, Chenliang Xu</p>
          <p><i>CVPRW'23: CVPR Sight and Sound Workshop. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2302.02088.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            
              <a class="button" href="https://liangsusan-git.github.io/project/avnerf/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Grouping Network for Sound Localization from Mixtures</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2303.17056.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/AVGN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Egocentric Audio-Visual Object Localization</b></p>
          <p>Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, and Chenliang Xu</p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2303.13471.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/WikiChao/Ego-AV-Loc" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Towards Unified, Explainable, and Robust Multisensory Perception</b></p>
          <p>Yapeng Tian</p>
          <p><i>AAAI'23: AAAI Conference on Artificial Intelligence. (NFH program)</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="/assets/publications/2023_aaainfh/paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing</b></p>
          <p>Shentong Mo, <b>Yapeng Tian</b></p>
          <p><i>NeurIPS'22: The Annual Conference on Neural Information Processing Systems. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openreview.net/pdf?id=zfo2LqFEVY" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/stoneMo/MGN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Scene Understanding Towards Unified, Explainable, and Robust Multisensory Perception</b></p>
          <p><b>Yapeng Tian</b></p>
          <p><i>PhD Thesis </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://www.proquest.com/openview/99bef5e8207df0ebab29e6b1f2afbad8/1.pdf?pq-origsite=gscholar&cbl=18750&diss=y" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Object Localization in Egocentric Videos</b></p>
          <p>Chao Huang, <b>Yapeng Tian</b>, Anurag Kumar, and Chenliang Xu</p>
          <p><i>CVPRW'22: CVPR Workshops</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</b></p>
          <p>Guangyao Li<sup>‚Ä°</sup>, Yake Wei<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, Chenliang Xu, Ji-Rong Wen, and Di Hu</p>
          <p><i>CVPR'22 Oral: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2203.14072.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=JH3t5gwe9Xw" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/GeWu-Lab/MUSIC-AVQA" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://gewu-lab.github.io/MUSIC-AVQA/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Space-Time Memory Network for Sounding Object Localization in Videos</b></p>
          <p>Sizhe Li<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, and Chenliang Xu</p>
          <p><i>BMVC'21: The British Machine Vision Conference. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2111.05526.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/lester0866/Space-Time-Memory-Network-for-Sounding-Object-Localization-in-Videos" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://sites.google.com/view/bmvc2021stm" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Can audio-visual integration strengthen robustness under multimodal attacks?</b></p>
          <p><b>Yapeng Tian</b> and Chenliang Xu</p>
          <p><i>CVPR'21: IEEE/CVF Conference on Computer Vision and Pattern Recognition. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2104.02000.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/AV-Robustness-CVPR21" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation</b></p>
          <p><b>Yapeng Tian</b>, Di Hu, and Chenliang Xu</p>
          <p><i>CVPR'21: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2104.02026.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/CCOL-CVPR21" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing</b></p>
          <p><b>Yapeng Tian</b>, Dingzeyu Li, and Chenliang Xu</p>
          <p><i>ECCV'20 Spotlight: European Conference on Computer Vision.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2007.10558.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/AVVP-ECCV20" target="_blank">Code</a>
            

            
              <a class="button" href="https://github.com/YapengTian/AVVP-ECCV20" target="_blank">Data</a>
            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Deep Audio Prior</b></p>
          <p><b>Yapeng Tian</b>, Chenliang Xu, and Dingzeyu Li</p>
          <p><i>CVPRW'20: CVPR Workshops.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1912.10292v1.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/adobe/Deep-Audio-Prior" target="_blank">Code</a>
            

            

            
              <a class="button" href="https://opensource.adobe.com/Deep-Audio-Prior/" target="_blank">Project</a>
            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Interpretable and Controllable Audio-Visual Video Captioning</b></p>
          <p><b>Yapeng Tian</b>, Chenxiao Guan, Goodman Justin, Marc Moore, and Chenliang Xu</p>
          <p><i>CVPRW'19: CVPR Workshops.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Yapeng_Tian_Audio-Visual_Interpretable_and_Controllable_Video_Captioning_CVPRW_2019_paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Audio-Visual Event Localization in Unconstrained Videos</b></p>
          <p><b>Yapeng Tian</b>, Jing Shi, Bochen Li, Zhiyao Duan, Chenliang Xu</p>
          <p><i>ECCV'18: European Conference on Computer Vision.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=m6r6BbD5MSc" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/YapengTian/AVE-ECCV18" target="_blank">Code</a>
            

            
              <a class="button" href="https://drive.google.com/file/d/1FjKwe79e0u96vdjIVwfRQ1V6SoDHe7kK" target="_blank">Data</a>
            

            
              <a class="button" href="https://sites.google.com/view/audiovisualresearch" target="_blank">Project</a>
            

          </div>
        </div>
      
    </div> -->
<!-- 
    <div class="tab-pane active" id="papers-videorestoration">
      
      
        <div class="paper">
          <p class="title"><b>EgoVSR: Towards High-Quality Egocentric Video Super-Resolution</b></p>
          <p>Yichen Chi, Junhao Gu, Jiamiao Zhang, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>TCSVT'24: IEEE Transactions on Circuits and Systems for Video Technology. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2305.14708.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>STADNet: Spatial-Temporal Attention-Guided Dual-Path Network for cardiac cine MRI super-resolution</b></p>
          <p>Jun Lyu, Shuo Wang, <b>Yapeng Tian</b>, Jing Zou, Shunjie Dong, Chengyan Wang, Angelica I Aviles-Rivero, Jing Qin</p>
          <p><i>MIA'24: Medical Image Analysis</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841524000677" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Structured Sparsity Learning for Efficient Video Super-Resolution</b></p>
          <p>Bin Xia, Jingwen He, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Luc Van Gool</p>
          <p><i>CVPR'23: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2206.07687" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/SSL" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Stdan: deformable attention network for space-time video super-resolution</b></p>
          <p>Hai Wang, Xiaoyu Xiang, <b>Yapeng Tian</b>, Wenming Yang, Qingmin Liao</p>
          <p><i>TNNLS'23: IEEE Transactions on Neural Networks and Learning Systems.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/abstract/document/10045744" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/littlewhitesea/STDAN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Learning Spatio-Temporal Downsampling for Effective Video Upscaling</b></p>
          <p>Xiaoyu Xiang, <b>Yapeng Tian</b>, Vijay Rengarajan, Lucas Young, Bo Zhu, Rakesh Ranjan</p>
          <p><i>ECCV'22: European Conference on Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136780159.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Video Matting via Consistency-Regularized Graph Neural Networks</b></p>
          <p>Tiantian Wang, Sifei Liu, <b>Yapeng Tian</b>, Kai Li, and Ming-Hsuan Yang</p>
          <p><i>ICCV'21: IEEE/CVF International Conference on Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://faculty.ucmerced.edu/mhyang/papers/iccv2021_video_matting.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/TiantianWang/VideoMatting-CRGNN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution</b></p>
          <p>Xiaoyu Xiang<sup>‚Ä°</sup>, <b>Yapeng Tian<sup>‚Ä°</sup></b>, Yulun Zhang, Yun Fu, Jan Allebach, and Chenliang Xu</p>
          <p><i>CVPR'20: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2002.11616.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=8mgD8JxBOus" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>TDAN: Temporally Deformable Alignment Network for Video Super-Resolution</b></p>
          <p><b>Yapeng Tian</b>, Yulun Zhang, Yun Fu, and Chenliang Xu</p>
          <p><i>CVPR'20: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Tian_TDAN_Temporally-Deformable_Alignment_Network_for_Video_Super-Resolution_CVPR_2020_paper.pdf" target="_blank">Paper</a>
            

            

            

            
              <a class="button" href="https://www.youtube.com/watch?v=eZExENE50I0" target="_blank">Video</a>
            

            
              <a class="button" href="https://github.com/YapengTian/TDAN-VSR-CVPR-2020" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
    </div> -->
<!-- 
    <div class="tab-pane active" id="papers-imagerestoration">
      
      
        <div class="paper">
          <p class="title"><b>ZFusion: Efficient Deep Compositional Zero-shot Learning for Blind Image Super-Resolution with Generative Diffusion Prior</b></p>
          <p>Alireza Esmaeilzehi, Hossein Zaredar, <b>Yapeng Tian</b>, Laleh Seyyed-Kalantari</p>
          <p><i>ICCV'25: IEEE/CVF International Conference on Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/2673.png?t=1759954583.3458676" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>DiffIR: Efficient Diffusion Model for Image Restoration</b></p>
          <p>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, <b>Yapeng Tian</b>, Wenming Yang, Radu Timotfe, Luc Van Gool</p>
          <p><i>TPAMI'25: IEEE Transactions on Pattern Analysis and Machine Intelligence. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2308.13767" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/DiffIR" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>DiffIR: Efficient Diffusion Model for Image Restoration</b></p>
          <p>Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, <b>Yapeng Tian</b>, Wenming Yang, Luc Van Gool</p>
          <p><i>ICCV'23: IEEE/CVF International Conference on Computer Vision. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2303.09472" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/DiffIR" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</b></p>
          <p>Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>MICCAI'23: Medical Image Computing and Computer-Assisted Intervention. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2307.02334" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/jmzhang79/Dual-ArbNet" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Meta-Learning based Degradation Representation for Blind Super-Resolution</b></p>
          <p>Bin Xia,  <b>Yapeng Tian</b>, Yulun Zhang, Yucheng Hang, Wenming Yang, Qingmin Liao</p>
          <p><i>TIP'23: IEEE Transactions on Image Processing. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2207.13963" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/MRDA" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Knowledge Distillation based Degradation Estimation for Blind Super-Resolution</b></p>
          <p>Bin Xia, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Radu Timofte, Luc Van Gool</p>
          <p><i>ICLR'23: International Conference on Learning Representations.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/2211.16928" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/KDSR" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Basic Binary Convolution Unit for Binarized Image Restoration Network</b></p>
          <p>Bin Xia, Yulun Zhang, Yitong Wang, <b>Yapeng Tian</b>, Wenming Yang, Radu Timofte, Luc Van Gool</p>
          <p><i>ICLR'23: International Conference on Learning Representations.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2210.00405.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/BBCU" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>GDSSR: Toward Real-World Ultra-High-Resolution Image Super-Resolution</b></p>
          <p>Yichen Chi, Wenming Yang, <b>Yapeng Tian</b></p>
          <p><i>SPL'23: IEEE Signal Processing Letters.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://ieeexplore.ieee.org/abstract/document/10041757" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>DuDoCAF: Dual-Domain Cross-Attention Fusion with Recurrent Transformer for Fast Multi-contrast MR Imaging</b></p>
          <p>Jun Lyu, Bin Sui, Chengyan Wang, <b>Yapeng Tian</b>, Qi Dou, and Jing Qin</p>
          <p><i>MICCAI'22: Medical Image Computing and Computer Assisted Intervention. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="/assets/publications/2022_dudocaf/paper.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Transformer-empowered Multi-contrast MRI Super-Resolution</b></p>
          <p>Guangyuan Li, Jun Lv, <b>Yapeng Tian</b>, Qi Dou, Chengyan Wang, Chenliang Xu, Jing Qin</p>
          <p><i>CVPR'22: IEEE/CVF Conference on Computer Vision and Pattern Recognition.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2203.13963.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/XAIMI-Lab/McMRSR" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution</b></p>
          <p>Bin Xia, <b>Yapeng Tian</b>, Yucheng Hang, Wenming Yang, Qingmin Liao, Jie Zhou</p>
          <p><i>AAAI'22: The AAAI Conference on Artificial Intelligence.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2201.04358.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/AMSA" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Efficient Non-Local Contrastive Attention for Image Super-Resolution</b></p>
          <p>Bin Xia<sup>‚Ä°</sup>, Yucheng Hang<sup>‚Ä°</sup>, <b>Yapeng Tian</b>, Wenming Yang, Qingmin Liao, Jie Zhou</p>
          <p><i>AAAI'22: The AAAI Conference on Artificial Intelligence. </i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/2201.03794.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/Zj-BinXia/ENLCA" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Residual Dense Network for Image Super-Resolution</b></p>
          <p>Yulun Zhang, <b>Yapeng Tian</b>, Yu Kong , Bineng Zhong, Yun Fu</p>
          <p><i>TPAMI'20: IEEE Transactions on Pattern Analysis and Machine Intelligence.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1812.10477.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/yulunzhang/RDN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>CFSNet: Toward a Controllable Feature Space for Image Restoration</b></p>
          <p>Wei Wang<sup>‚Ä°</sup>, Ruiming Guo<sup>‚Ä°</sup>, <b>Yapeng Tian</b>, and Wenming Yang</p>
          <p><i>ICCV'19: IEEE/CVF International Conference on Computer Vision.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1904.00634.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/qibao77/CFSNet" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>LCSCNet: Linear Compressing Based Skip-Connecting Network for ISR</b></p>
          <p>Wenming Yang, Xuechen Zhang, <b>Yapeng Tian</b>, Wei Wang, Jing-Hao Xue, Qingmin Liao</p>
          <p><i>TIP'19: IEEE Trans. Image Processing.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/abs/1909.03573" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Deep Learning for Single Image Super-Resolution: A Brief Review</b></p>
          <p>Wenming Yang, Xuechen Zhang, <b>Yapeng Tian</b>, Wei Wang, JingHao Xue, Qingmin Liao</p>
          <p><i>TMM'19: IEEE Trans. Multimedia.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1808.03344.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Residual Dense Network for Image Super-Resolution</b></p>
          <p>Yulun Zhang, <b>Yapeng Tian</b>, Yu Kong , Bineng Zhong, Yun Fu</p>
          <p><i>CVPR'18 Spotlight: IEEE/CVF Conf. on Computer Vision and Pattern Recognition.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://arxiv.org/pdf/1802.08797.pdf" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/yulunzhang/RDN" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results</b></p>
          <p>Timofte et al.</p>
          <p><i>CVPRW'17: CVPR Workshops.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://people.ee.ethz.ch/~timofter/publications/Timofte-CVPRW-2017.pdf" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Consistent Coding Scheme for Single-Image Super-Resolution</b></p>
          <p>Wenming Yang, <b>Yapeng Tian</b>, Fei Zhou, Qingmin Liao, Hai Chen, Chenglin Zheng</p>
          <p><i>TMM'16: EEE Trans. Multimedia. (First student author)</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/1MpEM5qmSJi1GtY9TftNNV0cgSHqMT_KQ/view" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>Anchored Neighborhood Regression based SISR from Self-examples</b></p>
          <p><b>Yapeng Tian</b>, Fei Zhou, Wenming Yang, Xuesen Shang, Qingmin Liao</p>
          <p><i>ICIP'16: IEEE International Conference on Image Processing.</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/1Ts_gYIp57llzK53Wyt7hwu4lZ9GQsbis/view" target="_blank">Paper</a>
            

            

            

            

            
              <a class="button" href="https://github.com/YapengTian/ICIP2016" target="_blank">Code</a>
            

            

            

          </div>
        </div>
      
        <div class="paper">
          <p class="title"><b>SISR Using Clustering-Based Global Regression and Propagation Filtering</b></p>
          <p>Wenming Yang, <b>Yapeng Tian</b>, Fei Zhou, ..., Qingmin Liao</p>
          <p><i>ACPR'15 Oral: Asian Conference on Pattern Recognition. (First student author)</i></p>
           <div class="paper-buttons">
            
              <a class="button" href="https://drive.google.com/file/d/12tqTHt0aLn7-B1jEgEJBYY64uhwfEezA/view" target="_blank">Paper</a>
            

            

            

            

            

            

            

          </div>
        </div>
      
    </div> -->

    

    

    


   

  </div>
</div>
</div>

<!-- ========== PROJECTS ========== 
<div class="docs-section" id="projects">
  <h4>Projects</h4>

  <ul class="tab-nav">
    <li><div class="button active" data-ref="#projects-selected">Selected</div></li>
    <li><div class="button" data-ref="#projects-all">All</div></li>
  </ul>

  <div class="tab-content">
    <div class="tab-pane active" id="projects-selected">
      
      
        
        
          <div class="row">
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2016_network-ab-testing.html">
                    <img src="assets/projects/2016_network-ab/thumbnail.jpg" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Detecting Network Effects</b> <br/>
                  Randomizing Over Randomized Experiments
                </div>

            </div>
          </div>

        
      
        
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2016_human-atlas.html">
                    <img src="assets/projects/2016_human-atlas/thumbnail.png" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Human Atlas</b> <br/>
                  Tool for Mapping Social Networks
                </div>

            </div>
          </div>

        
      
        
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2015_jun.html">
                    <img src="assets/projects/2015_jun/thumbnail.png" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Responsive Communities</b> <br/>
                  Pilot project in Jun, Spain
                </div>

            </div>
          </div>

        
          </div>
        
      
    </div>

    <div class="tab-pane" id="projects-all">
      
        
        
          <div class="row">
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2016_network-ab-testing.html">
                    <img src="assets/projects/2016_network-ab/thumbnail.jpg" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Detecting Network Effects</b> <br/>
                  Randomizing Over Randomized Experiments
                </div>

            </div>
          </div>

        
      
        
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2016_human-atlas.html">
                    <img src="assets/projects/2016_human-atlas/thumbnail.png" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Human Atlas</b> <br/>
                  Tool for Mapping Social Networks
                </div>

            </div>
          </div>

        
      
        
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2015_jun.html">
                    <img src="assets/projects/2015_jun/thumbnail.png" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Responsive Communities</b> <br/>
                  Pilot project in Jun, Spain
                </div>

            </div>
          </div>

        
          </div>
        
      
        
        
          <div class="row">
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2014_item-cold-start.html">
                    <img src="assets/projects/2014_item-cold-start/thumbnail.jpg" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Cold-Start Recommendations</b> <br/>
                  Learning Local Collective Embeddings
                </div>

            </div>
          </div>

        
      
        
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2013_iterative-hybrid-algorithm.html">
                    <img src="assets/projects/2013_iterative-hybrid-algorithm/thumbnail.jpg" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Semi-supervised Learning</b> <br/>
                  Iterative Hybrid Algorithm
                </div>

            </div>
          </div>

        
      
        
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2011_twitter-sentiment-analysis.html">
                    <img src="assets/projects/2011_twitter-sentiment-analysis/thumbnail.png" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Twitter Sentiment Analysis</b> <br/>
                  Analyzing Financial Tweets
                </div>

            </div>
          </div>

        
          </div>
        
      
        
        
          <div class="row">
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2010_wordnet-contruction.html">
                    <img src="assets/projects/2010_wordnet-construction/thumbnail.jpg" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Wordnet Construction</b> <br/>
                  Automatic Wordnet Construction using Language Models
                </div>

            </div>
          </div>

        
      
        
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2010_fingerprint-verification.html">
                    <img src="assets/projects/2010_fingerprint-verification/thumbnail.png" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Fingerprint Verification</b> <br/>
                  Image processing class project
                </div>

            </div>
          </div>

        
      
        
        

          <div class="four columns">
            <div class="project-container">

                <div class="project-image-container">
                  <a href="projects/2009_connect-four.html">
                    <img src="assets/projects/2009_connect-four/thumbnail.png" class="u-max-full-width" />
                  </a>
                </div>

                <div class="project-caption">
                  <b>Connect Four AI Agent</b> <br/>
                  Iterative deepening and alpha-beta pruning
                </div>

            </div>
          </div>

        
          </div>
        
      
    </div>
  </div>

</div>
-->

<!-- ========== PUBLICATIONS ========== -->
<span class="anchor" id="teaching"></span>
<div class="docs-section">
<div class="docs-section">
  <h4>Teaching</h4>

  <ul class="STYLE238">
    <li> Fall 2025 - CS 4391: <a href=/t/4391F25/ target="_blank">Introduction to Computer Vision</a>  </li>
    <li> Spring 2025 - CS 6384: <a href=/t/6384S25/ target="_blank">Computer Vision</a>  </li>
    <li> Fall 2024 - CS 4391: <a href=/t/4391F24/ target="_blank">Introduction to Computer Vision</a>  </li>
    <li> Spring 2024 - CS 6384: <a href=/t/6384S24/ target="_blank">Computer Vision</a>  </li>
    <li> Fall 2023 - CS 4391: <a href=/t/4391F23/ target="_blank">Introduction to Computer Vision</a>  </li>
    <li> Spring 2023 - CS 6384: <a href=/t/6384S23/ target="_blank">Computer Vision</a>  </li>
    <li> Fall 2022 - CS 6334: <a href=/t/6334F22/ target="_blank">Virtual Reality</a> </li>
    
  </ul>

  <!-- <ul class="STYLE238">
    <li> Spring 2019 - <a href="https://www.cs.rochester.edu/~cxu22/t/249S19/">CSC 249/449: Machine Vision</a> (University of Rochester) </li>
    <li> Fall 2018 - <a href="https://www.cs.rochester.edu/~cxu22/t/577F18/">CSC 577: Advanced Topics in Computer Vision,</a> (University of Rochester) </li>
    <li> Spring 2018 - <a href="https://www.cs.rochester.edu/~cxu22/t/249S18/">CSC 249/449: Machine Vision</a> (University of Rochester) </li>
    <li> Fall 2016 - Advanced Image Processing and Its Applications (Tsinghua University) </li>
    <li> Spring 2016 -‚ÄÄDigital Image Processing (Tsinghua University) </li>

  </ul>

    <p style="text-align:left"><b>Guest Lecturer:</b></p>
    <ul class="STYLE238">
      <li> Spring 2021 - <a href="https://www.cs.rochester.edu/~cxu22/t/577S21/">CSC 577: Advanced Topics in Computer Vision,</a> (University of Rochester) </li>
      <li> Fall 2020 - <a href="https://www.cs.rochester.edu/~cxu22/t/249F20/">CSC 249/449: Machine Vision</a> (University of Rochester) </li>
    </ul> -->





</div>
</div>

<!-- ========== Professional Activities ========== -->
<span class="anchor" id="activity"></span>
<div class="docs-section">
<div class="docs-section">
  <h4>Service</h4>

  <p>
    <b>Organizer:</b>
    </p>
    <ul class="STYLE238">
      <li> <a href="https://cv4a11y.github.io/ICCV2025/index.html">CV4A11y: Workshop on Vision Foundation Models and Generative AI for Accessibility</a>, ICCV 2025</li>
      <li> <a href="https://knowledgemr-workshop.github.io/">KnowledgeMR: Workshop on Knowledge-Intensive Multimodal Reasoning</a>, ICCV 2025</li>
      <li> <a href="https://mclworkshop25.github.io/mcl-iccv2025/ICCV2025/index.html">MCL: Workshop on Multimodal Continual Learning</a>, ICCV 2025</li>
      <li> <a href="https://www.audio-imagination.com/">Audio Imagination: AI-Driven Speech, Music, and Sound Generation Workshop</a>, NeurIPS 2024</li>
      <li> <a href="https://sites.google.com/view/elvm/home/">ELVM: Efficient Large Vision Models Workshop</a>, CVPR 2024</li>
      <li> <a href="https://cmrxrecon.github.io/">Cardiac MRI Reconstruction Challenge</a>, MICCAI 2023</li>
      <li>Tutorial on <a href="https://audio-visual-scene-understanding.github.io/">Audio-Visual Scene Understanding</a>, CVPR 2021 </li>
      <li>Tutorial on <a href="https://echo0409.github.io/Audio-Visual-Scene-Understanding/">Audio-Visual Scene Understanding</a>, WACV 2021 </li>
    </ul>

  
  <p style="text-align:left"><b>Area Chair or Senior Program Committee:</b></p>
  <ul class="STYLE238">
    <li> ACL ARR: ACL Rolling Review, 2025 </li>
    <li> NeurIPS: Conference on Neural Information Processing Systems, 2025 </li>
    <li>CVPR: IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025, 2026 </li>
    <li>ICLR: International Conference on Learning Representations, 2025, 2026</li>
    <li>AAAI: AAAI Conference on Artificial Intelligence, 2023, 2024, 2025, 2026</li>

  </ul>

    
  <p style="text-align:left"><b>Session Chair:</b></p>
  <ul class="STYLE238">
    <li>AAAI 2023 (Multimodal Learning, Low-Level & Physics-based Vision)</li>
  </ul>

  <p style="text-align:left"><b>Conference Program Committee/Reviewer:</b></p>
  <ul class="STYLE238">
    <li>CVPR: IEEE/CVF Conference on Computer Vision and Pattern Recognition </li>
    <li>ICCV: IEEE/CVF International Conference on Computer Vision</li>
    <li>ECCV: European Conference on Computer Vision </li>
    <li>NeurIPS: Conference on Neural Information Processing Systems  </li>
    <li>ICLR: International Conference on Learning Representations </li>
    <li>AAAI: AAAI Conference on Artificial Intelligence</li>
    <li>ICML: International Conference on Machine Learning </li>
    <li>WACV: Winter Conference on Applications of Computer Vision</li>
    <li>ACCV: Asian Conference on Computer Vision</li>
    <li>MICCAI: International Conference On Medical Image Computing & Computer Assisted Intervention</li>
  </ul>

  <p style="text-align:left"><b>Journal Reviewer:</b></p>
  <ul class="STYLE238">
    <li>TPAMI: IEEE Transactions on Pattern Analysis and Machine Intelligence  </li>
    <li> TMLR: The Transactions on Machine Learning Research  </li>
    <li>TIP: IEEE Transactions on Image Processing  </li>
    <li>TNNLS: IEEE Transactions on Neural Networks and Learning Systems  </li>
    <li> TMM: IEEE Transactions on Multimedia </li>
    <li>TCSVT: IEEE Transcations on Circuits and Systems for Video Technology  </li>
    <li>TASLP: IEEE/ACM Transactions on Audio, Speech and Language Processing </li>
    <li>Scientific Reports‚ÄìNature  </li>
    <li>CGF: Computer Graphics Forum </li>
    <li>CVIU: Computer Vision and Image Understanding </li>
    <li>SPIC: Signal Processing: Image Communication </li>
    <li> IEEE Access</li>

  </ul>

  <p style="text-align:left"><b>Talks and Seminars:</b></p>
  <ul class="STYLE238">
        <li> Audio-visual Scene Perception and Generation<br>  &nbsp  &nbsp &nbspGuest lecture@ Texas A&M, April 2025 </li>
    <li> Audio-visual Scene Perception and Generation<br>  &nbsp  &nbsp &nbspAIM Seminar @ UNT AI Seminar, Jan. 2025 </li>
    <li>Enhancing Image Quality with Deep Learning-Based Super-Resolution: From Natural Scenes to Medical Imaging<br>  &nbsp  &nbsp &nbspAIM Seminar @ UTSW, Oct. 2024 </li>
    <li>Learning Semantic-aware Grouping for Weakly-Supervised Audio-Visual Scene Understanding <br>  &nbsp  &nbsp &nbsp<a href="https://sightsound.org/">Sight and Sound Workshop</a> @ CVPR, June 2023 </li>
    <li>Human-Multisensory AI Collaboration: Opportunities and Challenges<br>  &nbsp  &nbsp &nbsp <a href="https://av4d.org/">AV4D Workshop</a> @ ECCV, Oct. 2022 </li>
    <li>UTD CS Mixer, Oct. 2022 </li>
    <li> Audio-Visual Scene Understanding Towards Unified, Explainable, and Robust Multisensory Perception 
      <p style = "text-indent:0.5cm;"> KTH Dive-Deep Seminar, Dec. 2021<br>
        &nbsp;&nbsp;&nbsp;&nbsp; RIT PhD Colloquium Series, Oct. 2021</p>
    </li>
    <li>Audio-Visual Video Understanding, IIAI Seminar, Sep. 2021 </li>
    <li>The Future of Audio-Visual Research Panel Discussion, VALSE Webinar, Nov. 2021 </li>

    
  </ul>
  

</div>
</div>

<!-- ========== Awards ========== -->
<span class="anchor" id="award"></span>
<div class="docs-section">
<div class="docs-section">
  <h4>Awards</h4>
    <p>
      ACCV Best Paper Honorable Mention Award, 2024<br>
      IEEE ISMAR IDEATExR workshop Best Paper Award, 2024 <br>
      UIST Belonging & Inclusion Best Paper Award, 2024<br>
      Amazon Research Award, 2024<br>
      Undergraduate Research Apprenticeship Program (URAP) award, 2023 and 2024<br>
      Cisco Faculty Research Award, 2023<br>
      AAAI New Faculty Highlights, 2023<br>
      CVPR Doctoral Consortium, 2022<br>
      Top 10% of High-Scoring Reviewers for NeurIPS, 2020<br>
      Outstanding Graduate of Tsinghua University, 2017  <br>
      Outstanding Master Thesis Award, Tsinghua University, 2017  <br>
      National Scholarship, Tsinghua University, 2016 <br>
    </p>


</div>
</div>


<!-- ========== RESUME ========== -->
<span class="anchor" id="resume"></span>
<div class="docs-section">
<div class="docs-section">
  <h4>Vit√¶</h4>

  <p>Full CV in <a href=/assets/cv/cv_web.pdf target="_blank">PDF</a>.</p>

  <!-- The Timeline -->
  <ul class="timeline">
    
    <li>
      
      <div class="direction-l">
      
        <div class="flag-wrapper">
          <span class="flag">University of Texas at Dallas</span>
          <span class="time-wrapper"><span class="time">2022 - now</span></span>
        </div>
        <div class="desc"><b>Assistant Professor</b> <br/> Department of Computer Science</div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-r">
      
        <div class="flag-wrapper">
          <span class="flag">University of Rochester</span>
          <span class="time-wrapper"><span class="time">2017 - 2022</span></span>
        </div>
        <div class="desc"><b>Ph.D. Student</b> <br/> Department of Computer Science</div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-l">
      
        <div class="flag-wrapper">
          <span class="flag">Meta</span>
          <span class="time-wrapper"><span class="time">Sep. 2021 - Jan. 2022</span></span>
        </div>
        <div class="desc"><b>Research Intern</b> <br/> Reality Labs</div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-l">
      
        <div class="flag-wrapper">
          <span class="flag">Adobe</span>
          <span class="time-wrapper"><span class="time">Summer 2021</span></span>
        </div>
        <div class="desc"><b>Research Intern</b> <br/> Creative Intelligence Lab</div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-l">
      
        <div class="flag-wrapper">
          <span class="flag">Adobe</span>
          <span class="time-wrapper"><span class="time">Summer 2019</span></span>
        </div>
        <div class="desc"><b>Research Intern</b> <br/> Creative Intelligence Lab</div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-r">
      
        <div class="flag-wrapper">
          <span class="flag">Tsinghua University</span>
          <span class="time-wrapper"><span class="time">2014-2017</span></span>
        </div>
        <div class="desc"><b>M.E. Student</b> <br/> Department of Electronic Engineering</div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-l">
      
        <div class="flag-wrapper">
          <span class="flag">Chinese Academy of Sciences</span>
          <span class="time-wrapper"><span class="time">Nov. 2016- May 2017</span></span>
        </div>
        <div class="desc"><b>Visiting Student</b> <br/> Shenzhen Institutes of Advanced Technology</div>
      </div>
    </li>
    
    <li>
      
      <div class="direction-r">
      
        <div class="flag-wrapper">
          <span class="flag">Xidian University</span>
          <span class="time-wrapper"><span class="time">2009 - 2013</span></span>
        </div>
        <div class="desc"><b>B.E. Student</b> <br/> School of Electronic Engineering</div>
      </div>
    </li>
    
  </ul>
</div>
</div>

<p> This website was built with <a href="https://jekyllrb.com">jekyll</a> based on a template from <a href="http://web.media.mit.edu/~msaveski/">Martin Saveski</a>.
</p>
<!-- <a href="https://clustrmaps.com/site/19nf5" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=NHvsgfF3ADBf-LMNCTgzvH2p8lgbFk4ZF2bRwOuJgH0&cl=ffffff"></a> -->



    <div class="footer">
      <div class="row">
        <div class="four columns">
          Yapeng Tian
        </div>
        <div class="four columns">
          yapeng.tian@utdallas.edu
        </div>
        <div class="four columns">
          <span onclick="window.open('https://twitter.com/YapengTian')" style="cursor: pointer">
            <i class="fa fa-twitter" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://www.linkedin.com/in/yapeng-tian-780795141/')" style="cursor: pointer">
            <i class="fa fa-linkedin-square" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://github.com/YapengTian')" style="cursor: pointer">
            <i class="fa fa-github" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://scholar.google.com/citations?user=lxCqdpoAAAAJ&hl=en')" style="cursor: pointer">
            <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://dblp.dagstuhl.de/pid/176/4020.html')" style="cursor: pointer">
            <i class="ai ai-dblp ai-lg" aria-hidden="true"></i>
          </span>
        </div>
      </div>
    </div>

  </div>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-7BR8HB30JT', 'auto');
  ga('send', 'pageview');

</script>

  <!-- do not remove -->
  <span id="62cd7b7da1aff3196fdc26b60e396df9"></span>

<!-- End Document
  ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì -->
</body>
</html>
